<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[午后随感]]></title>
      <url>/2017/07/26/essay_1/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p><img src="http://orwbuystz.bkt.clouddn.com/随笔/20170726212755_GyZWqy_午后随想.jpeg" width="60%"></p>
<p><br></p>
<p>趴在实验室的桌子上睡的迷迷糊糊，窗外的蚕鸣断断续续的传到耳朵里，有那么几个瞬间我以为自己回到了小时候，那时候一家人还在小县城郊区的小院里居住。同样的盛夏午后，同样的蝉鸣不绝于耳，我迷迷糊糊的醒来，院子里我妈在晒被子，三十多度的高温炙烤着大地，老旧的录音机播放着邓丽君的“恰似你的温柔”，我扑在被子上，放肆的闻着上面独特又好闻的“太阳味”。有人说这味道源自于“螨虫”，后被辟谣说其实是棉花的味道，但这已经不重要了。后来的岁月里无数次我晒被子，无数次的在太阳落山时把头埋在软绵绵的尚有余温的被子中，尽情闻着上面的味道，却都不如当年那个下午那么好闻。后来我才明白，味道并没有变，只是我们的日子再也不似当年那么闲暇而缓慢了。</p>
<p><br></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>回忆起以前的事，总是觉得时间在某个片段过的很快，又会在某些片段定格，以至于有时候我误以为自己的记忆是不连续的。大部分的往事被埋藏在脑海深处，却总有那么几个场景被反复唤起，无意间就会闯入你的眼前，让你忘记自己在做什么，随之而来的是一阵恍惚和发呆。</p>
<p><br></p>
<p>最近喜欢听一些老歌，有时候觉得老歌怎么听都比现在的歌有味道，或许正是因为这些老歌本身就寄托着某些回忆吧。喜欢邓丽君可能是因为我的父母，她的歌声在我还不记事的时候就开始陪伴我了。记不清那是几岁的时候，大概在小学时吧，那时候一家人刚离开小县城来到市区，全家挤在一栋破旧的楼里不足三十平米的小房间里，同样迷迷糊糊的醒来，我妈在不远处炒菜，背影有些模糊，白色的烟顺着天花板飘到窗外，录音机刚好放到“小城故事”。那个傍晚，那一幕，伴随着这首歌，长久的定格在我的记忆中，无论如何都挥之不去。</p>
<p><br></p>
<p>每一个小城都有着千千万万的人，每个人又都有着自己的故事。这首歌总会让我联想到某个江南小镇，青瓦石阶，烟雨迷蒙，一把油纸伞掉落在地上，水滴顺着屋檐留下，发出滴滴答答的声音。也或许是在遥远的天边，某个深山中的小城。这些我都不得而知，从小我在鲁西南广袤的内陆平原地区一个普通的小县城长大，没有经历过江南的雨季，没有见过小桥流水人家，没有广袤的大海，没有连绵的山脉，甚至连个丘陵都看不到。只能从音乐中，从文字中，从电视中捕捉到一丝半点，然后交给想象。直到我比较大的时候才第一次在连云港见到大海，在济南见到山。</p>
<p><br></p>
<p>后来我走过大疆南北很多地方，爬过很多山，看过很多次海，也去过很多有名的江南小镇，却很难找回当年在拥挤的小屋子里听邓丽君的“小城故事”感受的那种意境。我想是因为自己越成长，想象力变的越局限了吧。当年和小伙伴人手一把木剑，能编出一整套武侠电视连续剧，我们把自己编进我们构造的庞大而宏伟的世界中，纵横驰骋，策马扬鞭。</p>
<p><br></p>
<p>我们这代人上小学那会不比如今，大多都是高中才有属于自己的手机，而且大多还是从诺基亚开始。那会没有王者荣耀，只有小霸王游戏机，那会没有微信，大家还生活在用空间写日志的生活中。日子过得很慢很慢，初中毕业时大家还分发回忆录一张张的写，QQ昵称和简介还是火星文。我的第一篇日志写于2007年，如今十年过去了，谁能想到北京奥运会已经是十年前的事了。</p>
<p><br></p>
<p>后来我们家搬家，原来住了好多年的楼房被推平，变成一片废墟，或许这个假期我再回去已经盖起别的楼房了。小时候住过的小院前早已不是大片农田，再也看不到麦子和玉米，到处都是水泥和钢筋的痕迹。现代化渐渐把90年代的一切痕迹抹去，连同90后这个群体也被历史的车轮拉扯着裹挟着长大。每个年代都有着属于自己的记忆，记得很小的时候还看过黑白电视，记得小时候大人还在用BB机，记得我的第一款诺基亚手机，记得我们毕业时在每一个回忆录上认真写着一路顺风。</p>
<p><br></p>
<p>我们的手机早已可以保存任意多条短信，却很少再有发短信的人。这个世界在日新月异的变化，只是在闲暇的时候，我依然还是会想起那些闪闪发光的日子，那些时常闯入梦境的回忆，是使我坚持走下去的动力。</p>
<p><br></p>
<p>——2017.07.26</p>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 随笔 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习基础：全连接层]]></title>
      <url>/2017/07/26/DeepLearningFc/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>最近补了一下深度学习的基础知识，对全连接层中数据的前向传播和梯度的反向传播有了更深的认识，同时自己用Python实现了一个三层的全连接网络，并将结果可视化出来。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><p>全连接的神经网络也叫<strong>多层感知机(MLP)</strong>，前后两层的任意两个神经元都相连，同层的神经元之间没有连接关系，通过如下线性组合后送入一个激活函数进行非线性运算。<br>$$<br>Zi = \sum_j W{i,~ j} x_j + b_i<br>$$<br>常见的激活函数有<code>Sigmoid</code>、<code>ReLU</code>、双曲正切等，本次试验要实现一个二分类的神经网络，因此采用<code>Sigmoid</code>函数。<br>$$<br>Sigmoid=\frac {1}{1+e^{-z}}<br>$$<br><code>Sigmoid</code>函数图像如下所示，它会把任意范围的数映射到0和1之间，我们可以把它的输出当作二分类的概率，当最后一层的输出大于0.5时我们认为它类别为1，小于0.5时我们认为类别为0。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726123606_MsxRaI_全连接层.jpeg" alt="Sigmoid"></center>

<p>本试验采用的<strong>损失函数(Loss Function)</strong>为最简单的二次损失函数，即：<br>$$<br>Loss(y,t)=\frac{1}{2}(y-t)^2<br>$$<br>其中t表示标签数据，y表示神经网络的输出。</p>
<p><br></p>
<h1 id="实验数据"><a href="#实验数据" class="headerlink" title="实验数据"></a>实验数据</h1><p>本次试验构造一个三层全连接神经网络，第一层为输入层，维度为2；最后一层为输出层，维度为1；隐藏层神经元为超参数，可自由设定。</p>
<p>训练样本共有96个，特征数据为浮点数，标签数据为0或者1，分别表示两类。</p>
<p>训练数据如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">-0.1097</th>
<th style="text-align:center">0.5517</th>
<th style="text-align:center">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.1097</td>
<td style="text-align:center">-0.5517</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">-0.2392</td>
<td style="text-align:center">0.5774</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">0.2392</td>
<td style="text-align:center">-0.5774</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<p>可视化如图：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726130640_GLPFuK_train.jpeg" alt="Train Data" width="500"></center>

<p>测试数据和其相似，只是在坐标上有轻微变化。</p>
<h1 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h1><p>我简单的画了下前向传播和反向传播的流程，为方便起见这里令隐藏层神经元数目为4个，这个值对后续的代码没有影响。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726134924_1EqSco_WechatIMG202.jpeg" alt=""></p>
<p>此处x的每一行都表示一个样本，总的行数表示样本的个数。</p>
<p><br></p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>我们首先来定义全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FC</span>:</span></div><div class="line">    <span class="string">"""Define a fully connected layer"""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, output_dim, lr)</span>:</span></div><div class="line">        self._input_dim = input_dim</div><div class="line">        self._output_dim = output_dim</div><div class="line">        self.lr = lr</div><div class="line">        self.w = np.random.randn(input_dim, output_dim)</div><div class="line">        self.b = np.zeros(output_dim)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(self, z)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        self.y = self._sigmoid(np.dot(x, self.w) + self.b)</div><div class="line">        self.x = x</div><div class="line">        <span class="keyword">return</span> self.y</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, gradient)</span>:</span></div><div class="line">        grad_z = gradient * self.y * (<span class="number">1</span> - self.y)</div><div class="line">        grad_w = np.dot(self.x.T, grad_z)</div><div class="line">        grad_b = grad_z.sum(<span class="number">0</span>)</div><div class="line">        grad_x = np.dot(grad_z, self.w.T)</div><div class="line">        self.w -= grad_w * self.lr</div><div class="line">        self.b -= grad_b * self.lr</div><div class="line">        <span class="keyword">return</span> grad_x</div></pre></td></tr></table></figure>
<p>注意这里反向传播时，对于参数b的偏导<code>grad_b</code>，我们令它等于z的偏导<code>grad_z</code>在第一维度上的求和。在每次前向传播时，我们都把x的值保留下来，用于反向传播。所有的梯度传播完成之后，再进行参数的更新。</p>
<p><br></p>
<p>然后我们定义损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SquareLoss</span>:</span></div><div class="line">    <span class="string">"""Define the loss function"""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, output, label)</span>:</span></div><div class="line">        self.loss = output - label</div><div class="line">        <span class="keyword">return</span> np.sum(self.loss * self.loss) / self.loss.shape[<span class="number">0</span>] / <span class="number">2</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.loss</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, output, label)</span>:</span></div><div class="line">        <span class="keyword">return</span> (np.around(output) == label).sum() / len(label)</div></pre></td></tr></table></figure>
<p><br></p>
<p>接下来我们就可以开始搭建神经网络了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>:</span></div><div class="line">    <span class="string">"""Train the model"""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, output_dim, lr)</span>:</span></div><div class="line">        self.fc1 = FC(input_dim, hidden_dim, lr)</div><div class="line">        self.fc2 = FC(hidden_dim, output_dim, lr)</div><div class="line">        self.loss = SquareLoss()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, train_label, iter)</span>:</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(iter):</div><div class="line">            <span class="comment"># forward step</span></div><div class="line">            out_fc1 = self.fc1.forward(train_data)</div><div class="line">            out_fc2 = self.fc2.forward(out_fc1)</div><div class="line">            out_loss = self.loss.forward(out_fc2, train_label)</div><div class="line">            <span class="comment"># backward step</span></div><div class="line">            loss_grad = self.loss.backward()</div><div class="line">            loss_fc2 = self.fc2.backward(loss_grad)</div><div class="line">            loss_fc1 = self.fc1.backward(loss_fc2)</div><div class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</div><div class="line">                train_accuracy = self.loss.accuracy(out_fc2, train_label)</div><div class="line">                print(<span class="string">"Iter: &#123;0&#125;   Train accuracy: &#123;1&#125;"</span>.format(</div><div class="line">                    i, train_accuracy))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, test_data, test_label)</span>:</span></div><div class="line">        out_fc1 = self.fc1.forward(test_data)</div><div class="line">        out_fc2 = self.fc2.forward(out_fc1)</div><div class="line">        out_loss = self.loss.forward(out_fc2, test_label)</div><div class="line">        accuracy = self.loss.accuracy(out_fc2, test_label)</div><div class="line">        <span class="keyword">return</span> accuracy</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, predict_data)</span>:</span></div><div class="line">        out_fc1 = self.fc1.forward(predict_data)</div><div class="line">        out_fc2 = self.fc2.forward(out_fc1)</div><div class="line">        out_result = np.around(out_fc2)</div><div class="line">        <span class="keyword">return</span> out_result</div></pre></td></tr></table></figure>
<p>我们让每10次训练打印一次训练准确率，最终训练完成后用测试数据进行测试，并打印出测试准确率。</p>
<p><br></p>
<p>下面我们把训练数据和测试数据输入进去开始训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    train_set=np.loadtxt(TRAIN_DATA)</div><div class="line">    test_set=np.loadtxt(TEST_DATA)</div><div class="line">    train_data = train_set[:, :<span class="number">2</span>]</div><div class="line">    train_label = train_set[:, <span class="number">2</span>].reshape((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">    test_data = test_set[:, :<span class="number">2</span>]</div><div class="line">    test_label = test_set[:, <span class="number">2</span>].reshape((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line"></div><div class="line">    net = Net(<span class="number">2</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">0.1</span>)</div><div class="line">    net.train(train_data, train_label, <span class="number">5000</span>)</div><div class="line">    accuracy = net.test(test_data, test_label)</div><div class="line">    print(<span class="string">'Test accuracy: &#123;0&#125;'</span>.format(accuracy))</div><div class="line">    </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<p><br></p>
<p>我们首先定义隐藏层神经元为10个，训练迭代次数为5000次，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">Iter: 0   Train accuracy: 0.6041666666666666</div><div class="line">Iter: 10   Train accuracy: 0.6458333333333334</div><div class="line">Iter: 20   Train accuracy: 0.6458333333333334</div><div class="line">...</div><div class="line">Iter: 650   Train accuracy: 0.6875</div><div class="line">Iter: 660   Train accuracy: 0.6979166666666666</div><div class="line">Iter: 670   Train accuracy: 0.6979166666666666</div><div class="line">...</div><div class="line">Iter: 1530   Train accuracy: 0.9270833333333334</div><div class="line">Iter: 1540   Train accuracy: 0.9270833333333334</div><div class="line">Iter: 1550   Train accuracy: 0.9375</div><div class="line">...</div><div class="line">Iter: 1790   Train accuracy: 0.9791666666666666</div><div class="line">Iter: 1800   Train accuracy: 0.9895833333333334</div><div class="line">Iter: 1810   Train accuracy: 1.0</div><div class="line">Iter: 1820   Train accuracy: 1.0</div><div class="line">...</div><div class="line">Iter: 4980   Train accuracy: 1.0</div><div class="line">Iter: 4990   Train accuracy: 1.0</div><div class="line">Test accuracy: 1.0</div></pre></td></tr></table></figure>
<p>可以看出从1800次迭代之后就已经达到了1的训练准确率，最终的测试数据准确率同样为1。</p>
<p><br></p>
<p>我们添加一个类<code>Draw</code>来画出整个模型的收敛结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Draw</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.x = np.linspace(<span class="number">-5</span>,<span class="number">5</span>,<span class="number">100</span>)</div><div class="line">        self.y = np.linspace(<span class="number">-5</span>,<span class="number">5</span>,<span class="number">100</span>)</div><div class="line">        self.X, self.Y = np.meshgrid(self.x,self.y)</div><div class="line">        self.X_f = self.X.flatten()</div><div class="line">        self.Y_f = self.Y.flatten()</div><div class="line">        self.p = zip(self.X_f, self.Y_f)</div><div class="line">        self.data = list()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.p:</div><div class="line">            self.data.append(list(i))</div><div class="line">        self.data = np.array(self.data)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw2D</span><span class="params">(self, Z)</span>:</span></div><div class="line">        plt.figure()</div><div class="line">        plt.scatter(self.X_f,self.Y_f,c=Z)</div><div class="line">        plt.show()</div><div class="line">        </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    ...</div><div class="line">    draw = Draw()</div><div class="line">    out = net.predict(draw.data)</div><div class="line">    draw.draw2D(out)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<p>输出如下：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726140239_JEbLon_10.jpeg" width="500"></center>

<p>我们把隐藏层神经元改为30，得出的结果：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726140243_R5cuNf_30.jpeg" width="500"></center>

<p>可以看出隐藏层神经元在30的情况下边界相对更加平滑，而且收敛速度也更快。</p>
<p>完整代码和训练数据可在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/FC" target="_blank" rel="external">GitHub</a>下载。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/21525237" target="_blank" rel="external">神经网络-全连接层（1） - 知乎专栏</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21535703" target="_blank" rel="external">神经网络-全连接层（2） - 知乎专栏</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21572419" target="_blank" rel="external">神经网络-全连接层（3） - 知乎专栏</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（十五）：tf.contrib.learn日志和监控]]></title>
      <url>/2017/07/22/TensorFlow_15/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文介绍了<code>tf.contrib.learn</code>中的<strong>日志(Logging)</strong>和<strong>监控(Monitoring)</strong>的基础知识。当我们训练一个模型时，实时的追踪和评估训练进度往往是很有用的。在本教程中，我们将学会如何使用<code>TensorFLow</code>的日志功能和监控器<code>Monitor</code>API来监控一个神经网络分类器的训练过程，该分类器对鸾尾花(Iris)数据集进行分类。</p>
<p>该笔记的代码基于我之前的一篇笔记<a href="http://zangbo.me/2017/07/09/TensorFlow_13/">《TensorFlow 笔记（十三）：tf.contrib.learn入门》</a>改进而来，如果大家还没看那篇笔记，可以先去看完再来看这篇。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/get_started/monitors" target="_blank" rel="external">https://www.tensorflow.org/get_started/monitors</a></p>
<p><br></p>
<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><p>如下代码来源于笔记<a href="http://zangbo.me/2017/07/09/TensorFlow_13/">《TensorFlow 笔记（十三）：tf.contrib.learn入门》</a>，我们将在该代码的基础上改进。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># Data sets</span></div><div class="line">IRIS_TRAINING = os.path.join(os.path.dirname(__file__), <span class="string">"iris_training.csv"</span>)</div><div class="line">IRIS_TEST = os.path.join(os.path.dirname(__file__), <span class="string">"iris_test.csv"</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(unused_argv)</span>:</span></div><div class="line">    <span class="comment"># Load datasets.</span></div><div class="line">    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">        filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)</div><div class="line">    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">        filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)</div><div class="line"></div><div class="line">    <span class="comment"># Specify that all features have real-value data</span></div><div class="line">    feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</div><div class="line"></div><div class="line">    <span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></div><div class="line">    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</div><div class="line">                                                hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">                                                n_classes=<span class="number">3</span>,</div><div class="line">                                                model_dir=<span class="string">"/tmp/iris_model"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Fit model.</span></div><div class="line">    classifier.fit(x=training_set.data,</div><div class="line">                   y=training_set.target,</div><div class="line">                   steps=<span class="number">2000</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Evaluate accuracy.</span></div><div class="line">    accuracy_score = classifier.evaluate(x=test_set.data,</div><div class="line">                                         y=test_set.target)[<span class="string">"accuracy"</span>]</div><div class="line">    print(<span class="string">'Accuracy: &#123;0:f&#125;'</span>.format(accuracy_score))</div><div class="line"></div><div class="line">    <span class="comment"># Classify two new flower samples.</span></div><div class="line">    new_samples = np.array(</div><div class="line">        [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>], [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=float)</div><div class="line">    y = list(classifier.predict(new_samples, as_iterable=<span class="keyword">True</span>))</div><div class="line">    print(<span class="string">'Predictions: &#123;&#125;'</span>.format(str(y)))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    tf.app.run()</div></pre></td></tr></table></figure>
<p>下面，我们将在这个代码基础上，一点点更新，最终使得它具有日志和监控的功能。</p>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>上述代码仅仅是实现了一个神经网络分类器的功能，把鸾尾花样本正确分类。但是该代码没有打印任何记录模型训练过程的日志，仅仅展示出<code>print</code>语句中的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Accuracy: 0.933333</div><div class="line">Predictions: [1 2]</div></pre></td></tr></table></figure>
<p>没有任何日志记录，模型的训练就让人感觉像是一个黑箱子，在TensorFlow每次执行梯度下降算法时，我们都不知道里面发生了什么，也不了解模型是否正确收敛，或者确定是否应该提前<strong>停止训练(early stopping)</strong>。</p>
<p>其中一个解决方法是将模型训练分成多个<code>fit</code>来调用，以更小的步数来逐步评估模型的准确性。然而，这种方法在实践中并不推荐，因为这会使得训练过程变得很漫长。幸运的是，<code>tf.contrib.learn</code>提供了另外一个解决方法——<code>Monitor API</code>，我们可以用它在训练过程中打印出训练日志同时评估我们的模型。</p>
<p>在接下来的小节我们将学习如何在TensorFlow中打印日志，如何建立一个<strong>验证监控器(ValidationMonitor)</strong>来做评估，同时在TensorBoard中进行可视化。</p>
<p><br></p>
<h1 id="打印日志"><a href="#打印日志" class="headerlink" title="打印日志"></a>打印日志</h1><p>TensorFlow定义了五种不同层次的日志信息。以升序排列分别为<code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>FATAL</code>。当我们配置了其中一个层次的日志，TensorFlow不仅会输出该层次的日志，同时会输出比它高层次的日志记录。例如，当我们配置日志层次为<code>ERROR</code>，我们将会得到包含<code>ERROR</code>和<code>FATAL</code>的日志信息；当我们设置日志层次为<code>DEBUG</code>，我们将得到全部五种层次的日志信息。</p>
<p>TensorFlow默认的配置层次为<code>WARN</code>，但是当我们追踪模型的训练时，我们最好把它调整为<code>INFO</code>，这会给我们提供训练时<code>fit</code>节点的信息。</p>
<p>在代码的开始阶段(import 之后)添加以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.logging.set_verbosity(tf.logging.INFO)</div></pre></td></tr></table></figure>
<p>现在当我们执行代码，我们会看到如下额外的输出信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:loss = 1.18812, step = 1</div><div class="line">INFO:tensorflow:loss = 0.210323, step = 101</div><div class="line">INFO:tensorflow:loss = 0.109025, step = 201</div></pre></td></tr></table></figure>
<p>当我们用<code>INFO</code>层次的日志时，<code>tf.contirb.learn</code>会默认每100步迭代输出一次训练损失。</p>
<p><br></p>
<h1 id="配置验证监控器进行评估"><a href="#配置验证监控器进行评估" class="headerlink" title="配置验证监控器进行评估"></a>配置验证监控器进行评估</h1><p>打印出来训练模型可以帮助我们了解模型是否收敛，但是如果我们想要进一步的了解训练过程中发生了什么，<code>tf.contrib.learn</code>提供了一些高水平的<code>Monitor</code>，我们可以把它们运用到<code>fit</code>操作中来追踪训练过程或者调试一些低水平的TensorFlow操作。<code>Monitor</code>主要有以下几种：</p>
<table>
<thead>
<tr>
<th>Monitor</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CaptureVariable</code></td>
<td>每N次迭代把一个指定变量值保存到一个收集器中。</td>
</tr>
<tr>
<td><code>PrintTensor</code></td>
<td>每N次迭代打印出来指定<code>tensor</code>的值。</td>
</tr>
<tr>
<td><code>SummarySaver</code></td>
<td>对于一个指定<code>tensor</code>每N次迭代保存一次 <code>tf.Summary</code> <code>protocol buffers</code>，通过使用<a href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter" target="_blank" rel="external"><code>tf.summary.FileWriter</code></a> 指令。</td>
</tr>
<tr>
<td><code>ValidationMonitor</code></td>
<td>每N次迭代打印一次评估结果，还可以选择什么情况下提前停止训练(early stopping)。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="每N次迭代做一次评估"><a href="#每N次迭代做一次评估" class="headerlink" title="每N次迭代做一次评估"></a>每N次迭代做一次评估</h2><p>对于鸾尾花神经网络分类器，当我们打印训练损失时，我们可能还想要同步的在测试数据集上进行评估，进而得知模型的泛化能力。我们可以通过配置<code>ValidationMonitor</code>来实现这个功能，同时还可以设置每多少次迭代评估一次，<code>every_n_steps</code>默认值是100，这里我们设置<code>every_n_steps</code>为50，即让它每50次迭代来对测试集进行一次评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</div><div class="line">    test_set.data,</div><div class="line">    test_set.target,</div><div class="line">    every_n_steps=<span class="number">50</span>)</div></pre></td></tr></table></figure>
<p>把这行代码放在初始化<code>classifier</code>之前。</p>
<p><code>ValidationMonitor</code>依赖于保存的<em>checkpoint</em>文件来进行评估操作，因此我们需要在实例化<code>classifier</code>时添加<code>tf.contirb.learn.RunConfig</code>项，它包含了<code>save_checkpoints_secs</code>值来定义我们每多少秒保存一次<em>checkpoint</em>文件。因为鸾尾花数据集十分小，并且训练的特别快，我们可以把<code>save_checkpoints_secs</code>设置为1，也就是每秒钟保存一次<em>checkpoint</em>文件，这样我们可以确保有足够数量的<em>checkpoint</em>文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">classifier = tf.contrib.learn.DNNClassifier(</div><div class="line">    feature_columns=feature_columns,</div><div class="line">    hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">    n_classes=<span class="number">3</span>,</div><div class="line">    model_dir=<span class="string">"/tmp/iris_model"</span>,</div><div class="line">    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p><strong>注意：</strong><code>model_dir</code>参数定义了一个绝对地址来保存模型数据。每次运行代码时，该路径下的所有现有数据将被加载，并且模型训练将从上次停止的地方继续运行（例如，若训练期间每次<code>fit</code>操作迭代2000次，则连续执行脚本两次将迭代4000次 ）。</p>
<p>最后，我们来把之前定义好的监控器<code>validation_monitor</code>添加进<code>fit</code>操作里，把它赋值给<code>monitor</code>参数，该参数会接受一个包含了所有监控器的列表，并在训练过程中执行它们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">classifier.fit(x=training_set.data,</div><div class="line">               y=training_set.target,</div><div class="line">               steps=<span class="number">2000</span>,</div><div class="line">               monitors=[validation_monitor])</div></pre></td></tr></table></figure>
<p>现在，当我们运行该代码，我们可以看到打印出来的评估结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Validation (step 50): loss = 1.71139, global_step = 0, accuracy = 0.266667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 300): loss = 0.0714158, global_step = 268, accuracy = 0.966667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 1750): loss = 0.0574449, global_step = 1729, accuracy = 0.966667</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="用MetricSpec改进评估指标"><a href="#用MetricSpec改进评估指标" class="headerlink" title="用MetricSpec改进评估指标"></a>用MetricSpec改进评估指标</h2><p>如果我们没有定义评估指标，在验证阶段<code>ValidationMonitor</code>将会默认同时打印出损失值和准确率。但是我们可以更改指标列表，定义我们想要的评估指标，我们可以向<code>ValidationMonitor</code>函数添加<code>metircs</code>参数。 <code>metircs</code>采用字典的键/值对来定义，其中每个键都是我们想要记录的指标的名字，相应的值是<code>MetricSpec</code>对象。</p>
<p><code>MetricSpec</code>对象使用函数<code>tf.contirb.learn.MetricSpec()</code>来定义，它接受四个参数：</p>
<p><br></p>
<ul>
<li><code>metric_fn</code>：函数计算并且返回一组指标的值，可以是<code>tf.contrib.metrics</code>模块中已经预定义好的函数，例如<code>tf.contrib.metrics.streaming_precision</code>或<code>tf.contrib.metrics.streaming_recall</code>。同时我们可以定义我们自己需要的指标函数，这需要把<code>pridictions</code>和<code>labels</code>张量作为参数(<code>weights</code>参数是可选的)。该函数必须返回两种形式的指标值<ul>
<li>一个<code>tensor</code></li>
<li>一组<code>ops(value_op, update_op)</code>，其中<code>value_op</code>返回指标的值，<code>update_op</code>执行一个相关的操作来更新内部模型状态。</li>
</ul>
</li>
</ul>
<p><br></p>
<ul>
<li><code>prediction_key</code>：该<code>tensor</code>包含了模型返回的预测值。如果模型返回单个张量或具有单个条目的字典，则可以省略此参数。对于<code>DNNClassifier</code>模型，类预测将使用关键字<code>tf.contrib.learn.PredictionKey.CLASSES</code>在张量中返回。</li>
</ul>
<p><br></p>
<ul>
<li><code>label_key</code>：该<code>tensor</code>包含了模型返回的标签，它被模型的<code>input_fn</code>函数定义。与<code>prediction_key</code>一样，如果<code>input_fn</code>返回单个张量或具有单个条目的字典，则可以省略此参数。在本教程中，<code>DNNClassifier</code>没有<code>input_fn</code>（x，y数据直接传递给fit），因此不需要提供<code>label_key</code>。</li>
</ul>
<p><br></p>
<ul>
<li><code>weights_key</code>：可选项。<code>tensor</code>（由<code>input_fn</code>返回）包含<code>metric_fn</code>的权重输入。</li>
</ul>
<p><br></p>
<p>下面的代码创建一个<code>validation_metrics</code>字典，定义了三个在模型评估过程中需要打印的指标。</p>
<ul>
<li><code>&quot;accuracy&quot;</code>，使用<code>tf.contrib.metrics.streaming_accuracy</code>作为<code>metric_fn</code></li>
<li><code>&quot;precision&quot;</code>，使用<code>tf.contrib.metrics.streaming_precision</code>作为<code>metric_fn</code></li>
<li><code>&quot;recall&quot;</code>使用<code>tf.contrib.metrics.streaming_recall</code>作为<code>metric_fn</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">validation_metrics = &#123;</div><div class="line">    <span class="string">"accuracy"</span>:</div><div class="line">        tf.contrib.learn.MetricSpec(</div><div class="line">            metric_fn=tf.contrib.metrics.streaming_accuracy,</div><div class="line">            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),</div><div class="line">    <span class="string">"precision"</span>:</div><div class="line">        tf.contrib.learn.MetricSpec(</div><div class="line">            metric_fn=tf.contrib.metrics.streaming_precision,</div><div class="line">            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),</div><div class="line">    <span class="string">"recall"</span>:</div><div class="line">        tf.contrib.learn.MetricSpec(</div><div class="line">            metric_fn=tf.contrib.metrics.streaming_recall,</div><div class="line">            prediction_key=tf.contrib.learn.PredictionKey.CLASSES)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>把上述代码添加到<code>ValidationMonitor</code>的定义之前，然后修改关于<code>ValidationMonitor</code>的定义，添加一个<code>metrics</code>参数来打印准确率，预测值和召回率指标，这些都是之前在<code>validation_metrics</code>中定义过的。（损失值是始终都会被打印出来的，我们不需要明确去定义它）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</div><div class="line">    test_set.data,</div><div class="line">    test_set.target,</div><div class="line">    every_n_steps=<span class="number">50</span>,</div><div class="line">    metrics=validation_metrics)</div></pre></td></tr></table></figure>
<p>执行该代码，我们可以看到预测值和召回率包含在输出中了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Validation (step 50): recall = 0.0, loss = 1.20626, global_step = 1, precision = 0.0, accuracy = 0.266667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 600): recall = 1.0, loss = 0.0530696, global_step = 571, precision = 1.0, accuracy = 0.966667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 1500): recall = 1.0, loss = 0.0617403, global_step = 1452, precision = 1.0, accuracy = 0.966667</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="用验证监控器实现Early-Stopping"><a href="#用验证监控器实现Early-Stopping" class="headerlink" title="用验证监控器实现Early Stopping"></a>用验证监控器实现Early Stopping</h2><p>注意在上述打印的日志中，到第600次迭代时，模型在测试集上已经到达接近于1的准确率和召回率，这种情况下我们可以使用<code>early stopping</code>。</p>
<p>除了打印评估指标，<code>ValidationMonitor</code>实现<code>early stopping</code>也十分的简单。我们可以通过定义以下三个参数来实现：</p>
<table>
<thead>
<tr>
<th>Param</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>early_stopping_metric</code></td>
<td>引发early stopping的指标 (例如loss或accuracy)，触发条件定义在<code>early_stopping_rounds</code>和 <code>early_stopping_metric_minimize</code>中，默认的值是 <code>&quot;loss&quot;</code>。</td>
</tr>
<tr>
<td><code>early_stopping_metric_minimize</code></td>
<td>如果模型是想要最小化<code>early_stopping_metric</code>则为<code>True</code>; 如果模型是想要最大化<code>early_stopping_metric</code>则为<code>False</code>。默认为<code>True</code>。</td>
</tr>
<tr>
<td><code>early_stopping_rounds</code></td>
<td>设置一定的迭代次数，如果在这些次数内<code>early_stopping_metric</code>没有减少(<code>early_stopping_metric_minimize</code>值为<code>True</code>) 或者没有增加(<code>early_stopping_metric_minimize</code>值为<code>False</code>)，训练将会停止。默认值为<code>None</code>，这意味着<code>early stopping</code>不会被触发。</td>
</tr>
</tbody>
</table>
<p>我们对<code>ValidationMonitor</code>的定义做如下修改，当我们的损失持续了200次迭代都没有减小，模型的训练将会停止，将不会执行完<code>fit</code>中定义的2000次迭代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</div><div class="line">    test_set.data,</div><div class="line">    test_set.target,</div><div class="line">    every_n_steps=<span class="number">50</span>,</div><div class="line">    metrics=validation_metrics,</div><div class="line">    early_stopping_metric=<span class="string">"loss"</span>,</div><div class="line">    early_stopping_metric_minimize=<span class="keyword">True</span>,</div><div class="line">    early_stopping_rounds=<span class="number">200</span>)</div></pre></td></tr></table></figure>
<p>再次执行该代码，我们会看到模型提前停止了训练：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 1150): recall = 1.0, loss = 0.056436, global_step = 1119, precision = 1.0, accuracy = 0.966667</div><div class="line">INFO:tensorflow:Stopping. Best step: 800 with loss = 0.048313818872.</div></pre></td></tr></table></figure>
<p>训练在第1150次迭代停止，这表明在之前的200次训练迭代中，损失都没有下降，总体来说，800次迭代就已经在测试集达到了最小的损失值。这表明通过减少训练次数可以进一步改进模型，同时改善超参数。</p>
<p><br></p>
<h2 id="TensorBoard可视化日志数据"><a href="#TensorBoard可视化日志数据" class="headerlink" title="TensorBoard可视化日志数据"></a>TensorBoard可视化日志数据</h2><p><code>ValidationMonitor</code>在训练期间生成了大量关于模型性能的原始数据，我们可以可视化它们来进一步了解训练情况——例如准确率是如何随着训练次数改变的。我们可以使用TensorBoard来实现这个功能，地址是我们保存模型数据的地址，打开命令行输入以下代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ tensorboard --logdir=/tmp/iris_model/</div></pre></td></tr></table></figure>
<p>然后打开浏览器在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板。具体的使用方法可以参考我的笔记<a href="http://zangbo.me/2017/06/27/TensorFlow_4/">《TensorFlow 笔记（四）：TensorBoard可视化》</a></p>
<p><br></p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><p>完整代码可以在TensorFlow官方GitHub下载：</p>
<p><a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/monitors/iris_monitors.py" target="_blank" rel="external">iris_monitors.py</a></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/monitors" target="_blank" rel="external">Logging and Monitoring Basics with tf.contrib.learn  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（十四）：tf.contrib.learn输入函数]]></title>
      <url>/2017/07/18/TensorFlow_14/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本教程介绍了<code>tf.contrib.learn</code>中的输入函数，我们将首先学习如何建立一个<code>input_fn</code>来预处理数据并且把它们送入模型中训练、评估或测试。接着我们将运用<code>input_fn</code>构造一个神经网络回归器用来预测房价。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/get_started/input_fn" target="_blank" rel="external">https://www.tensorflow.org/get_started/input_fn</a></p>
<p><br></p>
<h1 id="使用input-fn的输入方法"><a href="#使用input-fn的输入方法" class="headerlink" title="使用input_fn的输入方法"></a>使用input_fn的输入方法</h1><p>当我们用<code>tf.contrib.learn</code>来训练一个神经网络时，我们可以直接把特征数据和标签数据送入<code>fit</code>、<code>evaluate</code>或<code>predict</code>操作中。如下例子是我们上篇教程中用到的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)</div><div class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)</div><div class="line">...</div><div class="line"></div><div class="line">classifier.fit(x=training_set.data,</div><div class="line">               y=training_set.target,</div><div class="line">               steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>当我们对源数据操作较小时，这种方法很好。但如果我们需要大量的特征工程，我们可以使用输入函数<code>input_fn</code>来进行数据预处理的操作。</p>
<p><br></p>
<h2 id="input-fn结构"><a href="#input-fn结构" class="headerlink" title="input_fn结构"></a>input_fn结构</h2><p>如下代码显示了一个输入函数的基本框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 在这里预处理你的数据...</span></div><div class="line">    <span class="comment"># return:1) 特征列与相应特征数据的Tensors的映射 </span></div><div class="line">    <span class="comment">#        2) 包含标签的Tensor</span></div><div class="line">    <span class="keyword">return</span> feature_cols, labels</div></pre></td></tr></table></figure>
<p>函数主体包含了对输入数据的预处理操作，例如清除异常点等。输入函数必须返回如下两个值：</p>
<ul>
<li><code>feature_cols</code>：一个包含了键值对的字典，把特征名映射到对应的<code>Tensor</code>上，该<code>Tensor</code>包含了特征数据。</li>
<li><code>labels</code>：一个包含了标签数据的<code>Tensor</code></li>
</ul>
<p><br></p>
<h2 id="把特征数据转化为Tensors"><a href="#把特征数据转化为Tensors" class="headerlink" title="把特征数据转化为Tensors"></a>把特征数据转化为Tensors</h2><p>如果在<code>input_fn</code>函数中，我们的特征数据或标签数据保存在<code>pandas dataframes</code>或者<code>numpy arrays</code>里面，我们需要把它们转化为<code>Tensor</code>返回。</p>
<p>对于连续数据，我们可以用<code>tf.constant</code>创建一个<code>Tensor</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feature_column_data = [<span class="number">1</span>, <span class="number">2.4</span>, <span class="number">0</span>, <span class="number">9.9</span>, <span class="number">3</span>, <span class="number">120</span>]</div><div class="line">feature_tensor = tf.constant(feature_column_data)</div></pre></td></tr></table></figure>
<p>对于稀疏矩阵数据（大部分值为0），我们可以使用<code>SparseTensor</code>，一般来说我们用三个参数来实例化一个<code>SparseTensor</code>：</p>
<ul>
<li><code>dense_shape</code>：<code>tensor</code>的形状，用一个列表来表示每个维度的元素的个数，例如<code>dense_shape=[3,5]</code>表示一个二维的<code>tensor</code>，有3行5列；而<code>dense_shape=[9]</code>表示一个一维的<code>tensor</code>，有9个元素。</li>
<li><code>indices</code>：表示<code>tensor</code>中非0值的位置。用一个嵌套列表来表示，每个嵌套列表代表一个非0值所在的位置。例如<code>indices=[[0,1], [2,4]]</code>表示在<code>tensor</code>中第0行第1个元素和第2行第4个元素为非零值。</li>
<li><code>values</code>：一维<code>tensor</code>，<code>values</code>中的第i个值位于在<code>indices</code>中第i个值表示的位置上。例如下面的例子，表示在第0行第1个元素为6，第2行第4个元素为0.5。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sparse_tensor = tf.SparseTensor(indices=[[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">4</span>]],</div><div class="line">                                values=[<span class="number">6</span>, <span class="number">0.5</span>],</div><div class="line">                                dense_shape=[<span class="number">3</span>, <span class="number">5</span>])</div></pre></td></tr></table></figure>
<p>对应的<code>tensor</code>为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[[<span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.5</span>]]</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="把input-fn送入模型中"><a href="#把input-fn送入模型中" class="headerlink" title="把input_fn送入模型中"></a>把input_fn送入模型中</h2><p>为了把数据送入模型进行训练，我们只需要把创建的输入函数赋值给<code>fit</code>的<code>input_fn</code>参数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=my_input_fn, steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>注意<code>input_fn</code>会把特征数据和标签数据一起送入模型中，它代替了<code>fit</code>中的<code>x</code>和<code>y</code>参数，如果我们同时保留了<code>input_fn</code>和<code>x</code>或者<code>y</code>，程序会报错。</p>
<p>同时需要注意的是<code>input_fn</code>参数必须接受一个函数名(<code>input_fn=my_input_fn</code>)而不是接受一个函数调用(<code>input_fn=my_input_fn()</code>)。这意味着如果我们尝试在<code>fit</code>函数的参数中使用一个函数调用，如下代码，将会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=my_input_fn(training_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>不过如果我们想让我们的输入函数具有输入参数，第一个方法是用另一个无参数函数包含它，然后用该函数名作为<code>input_fn</code>的值，这样我们就可以在内层函数送入我们需要的参数。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_function_training_set</span><span class="params">()</span>:</span></div><div class="line">	<span class="keyword">return</span> my_input_function(training_set)</div><div class="line"></div><div class="line">classifier.fit(input_fn=my_input_fn_training_set, steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>第二个方法是使用python内置的偏函数<code>functools.partial</code>来实现上述功能，它会建立一个新的函数同时把所有的参数固定住：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=functools.partial(my_input_function,</div><div class="line">                                          data_set=training_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>第三个方法我们可以使用<code>lambda</code>操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=<span class="keyword">lambda</span>: my_input_fn(training_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>上述三个方法最大的优点是我们可以把同一个<code>input_fn</code>送到<code>evaluate</code>和<code>predict</code>操作上，而只需要修改内层函数的输入参数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.evaluate(input_fn=<span class="keyword">lambda</span>: my_input_fn(test_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>这些方法增强了代码的可操作性，我们不需要对每个操作都分别获取<code>x</code>和<code>y</code>的值并用两个变量表示出来，如 <code>x_train</code>, <code>x_test</code>, <code>y_train</code>, <code>y_test</code>。</p>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>关于<code>functools.partial()</code>：</p>
<p>如果某一个函数的其中一个输入参数始终固定，而我们每次调用该函数都要输入一次就显得很没有必要，于是我们就可以对该函数进行封装，使得输入参数减少。用到的函数就是<code>functools.partial()</code>。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input1</span><span class="params">(key1,key2)</span>:</span></div><div class="line">    <span class="keyword">return</span> key1+<span class="string">'_'</span>+key2</div></pre></td></tr></table></figure>
<p>如果我们的<code>key1</code>在使用过程中始终是固定的，而只有<code>key2</code>是变化的，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = input1(<span class="string">'China'</span>,<span class="string">'liming'</span>)</div><div class="line">b = input1(<span class="string">'China'</span>,<span class="string">'zhangsan'</span>)</div><div class="line">c = input1(<span class="string">'China'</span>,<span class="string">'wangwei'</span>)</div><div class="line">...</div></pre></td></tr></table></figure>
<p>这时候我们就可以固定<code>key1</code>的值，对<code>input1()</code>进行封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> functools</div><div class="line">input2 = functools.partial(input1, <span class="string">'China'</span>)</div></pre></td></tr></table></figure>
<p>这样我们就可以很方便的使用该函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = input2(<span class="string">'liming'</span>)</div><div class="line">b = input2(<span class="string">'zhangsan'</span>)</div><div class="line">c = input2(<span class="string">'wangwei'</span>)</div></pre></td></tr></table></figure>
<p>输出<code>a,b,c</code>的值得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">China_liming </div><div class="line">China_zhangsan </div><div class="line">China_wangwei</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="波士顿房价神经网络模型"><a href="#波士顿房价神经网络模型" class="headerlink" title="波士顿房价神经网络模型"></a>波士顿房价神经网络模型</h1><p>接下来我们将携一个输入函数来预处理波士顿房价数据，然后我们把处理后的数据送入神经网络回归器中进行训练，进而预测房价中位数。</p>
<p>我们要用到的数据集的CSV文件包含以下特征：</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>CRIM</td>
<td>Crime rate per capita</td>
</tr>
<tr>
<td>ZN</td>
<td>Fraction of residential land zoned to permit 25,000+ sq ft lots</td>
</tr>
<tr>
<td>INDUS</td>
<td>Fraction of land that is non-retail business</td>
</tr>
<tr>
<td>NOX</td>
<td>Concentration of nitric oxides in parts per 10 million</td>
</tr>
<tr>
<td>RM</td>
<td>Average Rooms per dwelling</td>
</tr>
<tr>
<td>AGE</td>
<td>Fraction of owner-occupied residences built before 1940</td>
</tr>
<tr>
<td>DIS</td>
<td>Distance to Boston-area employment centers</td>
</tr>
<tr>
<td>TAX</td>
<td>Property tax rate per $10,000</td>
</tr>
<tr>
<td>PTRATIO</td>
<td>Student-teacher ratio</td>
</tr>
</tbody>
</table>
<p>模型预测的标签是MEDV，表示房价的中位数，以<strong>千美元</strong>为计数单位。</p>
<p>通过以下链接下载数据集：</p>
<ul>
<li>训练数据：<a href="http://download.tensorflow.org/data/boston_train.csv" target="_blank" rel="external">boston_train.csv</a></li>
<li>测试数据：<a href="http://download.tensorflow.org/data/boston_test.csv" target="_blank" rel="external">boston_test.csv</a></li>
<li>预测数据：<a href="http://download.tensorflow.org/data/boston_predict.csv" target="_blank" rel="external">boston_predict.csv</a></li>
</ul>
<p>接下来我们将依次介绍如何创建一个输入函数，把它们送入神经网络回归器中，训练并评估模型，进行房价预测。</p>
<p><br></p>
<h2 id="引入房价数据"><a href="#引入房价数据" class="headerlink" title="引入房价数据"></a>引入房价数据</h2><p>首先我们需要引入必要的库，包括<code>pandas</code>和<code>tensorflow</code>，同时我们利用<code>set_verbosity</code>来获得更多的输出信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">tf.logging.set_verbosity(tf.logging.INFO)</div></pre></td></tr></table></figure>
<p>关于<code>logging</code>我们将在下个笔记中介绍，简单来说我们在代码开始(<code>import</code>之后)加上这行代码，训练过程将打印出详细的<code>loss</code>信息和<code>step</code>信息，可以更好的监控整个训练过程。</p>
<p>因为我们是使用<code>pandas</code>来倒入CSV数据，所以在此之前先定义好<code>column</code>的名字，以及特征名和标签名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">COLUMNS = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>, <span class="string">"age"</span>,</div><div class="line">           <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>, <span class="string">"medv"</span>]</div><div class="line">FEATURES = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>,</div><div class="line">            <span class="string">"age"</span>, <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>]</div><div class="line">LABEL = <span class="string">"medv"</span></div><div class="line"></div><div class="line">training_set = pd.read_csv(<span class="string">"boston_train.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</div><div class="line">                           skiprows=<span class="number">1</span>, names=COLUMNS)</div><div class="line">test_set = pd.read_csv(<span class="string">"boston_test.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</div><div class="line">                       skiprows=<span class="number">1</span>, names=COLUMNS)</div><div class="line">prediction_set = pd.read_csv(<span class="string">"boston_predict.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</div><div class="line">                             skiprows=<span class="number">1</span>, names=COLUMNS)</div></pre></td></tr></table></figure>
<p>因为第一行是关于数据的介绍，所以我们在这里跳过第一行，用<code>skiprows=1</code>。</p>
<p><br></p>
<h2 id="定义FeatureColumns并创建回归器"><a href="#定义FeatureColumns并创建回归器" class="headerlink" title="定义FeatureColumns并创建回归器"></a>定义FeatureColumns并创建回归器</h2><p>接下来我们创建一系列<code>FeatureColumn</code>来组成特征数据。每个<code>FeatureColumn</code>表示一个特征，根据数据类型的不同用相应的函数创建，更详细内容可以看以下两个参考链接：</p>
<ul>
<li>所有函数类型：<a href="https://www.tensorflow.org/api_guides/python/contrib.layers#Feature_columns" target="_blank" rel="external">Layers (contrib)  |  TensorFlow</a>，</li>
<li>特征是分类数据(categorical data)的例子：<a href="https://www.tensorflow.org/tutorials/wide#base_categorical_feature_columns" target="_blank" rel="external">TensorFlow Linear Model Tutorial  |  TensorFlow</a>。</li>
</ul>
<p>因为本数据集所有的特征都是连续值，所以我们可以用<code>tf.contrib.layers.real_valued_column()</code>函数来创建，函数输入是特征的名字，这和下面的<code>input_fn()</code>函数相对应。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feature_cols = [tf.contrib.layers.real_valued_column(k)</div><div class="line">                  <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES]</div></pre></td></tr></table></figure>
<p>接下来，我们将实例化一个<code>DNNRegressor</code>，我们需要提供两个参数，一个<code>hidden_units</code>，它是用来定义隐藏层神经元个数的超参数，这里我们用两层各10个神经元；另一个是<code>feature_columns</code>，即我们刚刚定义的<code>FeatureColumns</code>的列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,</div><div class="line">                                          hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</div><div class="line">                                          model_dir=<span class="string">"/tmp/boston_model"</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="建立input-fn"><a href="#建立input-fn" class="headerlink" title="建立input_fn"></a>建立input_fn</h2><p>为了把输入数据送入<code>regressor</code>中，我们要创建一个输入函数，它接受一个<code>pandas</code>的<code>Datafram</code>同时返回包含特征信息和标签信息的两个<code>Tensor</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(data_set)</span>:</span></div><div class="line">  feature_cols = &#123;k: tf.constant(data_set[k].values)</div><div class="line">                  <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES&#125;</div><div class="line">  labels = tf.constant(data_set[LABEL].values)</div><div class="line">  <span class="keyword">return</span> feature_cols, labels</div></pre></td></tr></table></figure>
<p>注意<code>data_set[k].values</code>将返回一个<code>numpy array</code>，<code>feature_cols</code>是一个字典。<code>input_fn()</code>函数将对<code>data_set</code>进行处理，这意味着我们可以把任何我们需要的<code>DataFrame</code>送入该函数中，包括<code>training_set</code>、<code>test_set</code>和 <code>prediction_set</code>。</p>
<p><br></p>
<h2 id="训练回归器"><a href="#训练回归器" class="headerlink" title="训练回归器"></a>训练回归器</h2><p>训练神经网络回归器，我们只需要执行<code>fit</code>方法，然后把<code>training_set</code>送入<code>input_fn</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">regressor.fit(input_fn=<span class="keyword">lambda</span>: input_fn(training_set), steps=<span class="number">5000</span>)</div></pre></td></tr></table></figure>
<p>我们将会看到打印出的训练信息，每100步迭代打印一次训练损失：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Step 1: loss = 483.179</div><div class="line">INFO:tensorflow:Step 101: loss = 81.2072</div><div class="line">INFO:tensorflow:Step 201: loss = 72.4354</div><div class="line">...</div><div class="line">INFO:tensorflow:Step 1801: loss = 33.4454</div><div class="line">INFO:tensorflow:Step 1901: loss = 32.3397</div><div class="line">INFO:tensorflow:Step 2001: loss = 32.0053</div><div class="line">INFO:tensorflow:Step 4801: loss = 27.2791</div><div class="line">INFO:tensorflow:Step 4901: loss = 27.2251</div><div class="line">INFO:tensorflow:Saving checkpoints for 5000 into /tmp/boston_model/model.ckpt.</div><div class="line">INFO:tensorflow:Loss for final step: 27.1674.</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>借下来我们在测试集上评估训练好的模型，执行<code>evaluate</code>指令，这次把<code>test_set</code>送入<code>input_fn</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ev = regressor.evaluate(input_fn=<span class="keyword">lambda</span>: input_fn(test_set), steps=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>打印出损失的确切值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">loss_score = ev[<span class="string">"loss"</span>]</div><div class="line">print(<span class="string">"Loss: &#123;0:f&#125;"</span>.format(loss_score))</div></pre></td></tr></table></figure>
<p>可以看到以下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Eval steps [0,1) for training step 5000.</div><div class="line">INFO:tensorflow:Saving evaluation summary for 5000 step: loss = 11.9221</div><div class="line">Loss: 11.922098</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="进行预测"><a href="#进行预测" class="headerlink" title="进行预测"></a>进行预测</h2><p>最后，我们可以用训练好的模型来进行房价的预测，这里我们使用的数据集是<code>prediction_set</code>，共有6个样本，包含了特征数据但是没有包含标签数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">y = regressor.predict(input_fn=<span class="keyword">lambda</span>: input_fn(prediction_set))</div><div class="line"><span class="comment"># .predict() returns an iterator; convert to a list and print predictions</span></div><div class="line">predictions = list(itertools.islice(y, <span class="number">6</span>))</div><div class="line"><span class="keyword">print</span> (<span class="string">"Predictions: &#123;&#125;"</span>.format(str(predictions)))</div></pre></td></tr></table></figure>
<p>结果输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Predictions: [ 33.30348587  17.04452896  22.56370163  34.74345398  14.55953979  19.58005714]</div></pre></td></tr></table></figure>
<p>结果给出了六个样本的预测房价。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/input_fn" target="_blank" rel="external">Building Input Functions with tf.contrib.learn  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/handsomekang/article/details/9712125" target="_blank" rel="external">飘逸的python - 偏函数functools.partial - mattkang - CSDN博客</a></li>
<li><a href="https://www.tensorflow.org/get_started/monitors#enabling_logging_with_tensorflow" target="_blank" rel="external">Logging and Monitoring Basics with tf.contrib.learn  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（十三）：tf.contrib.learn入门]]></title>
      <url>/2017/07/09/TensorFlow_13/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>TensorFlow有很多高级的机器学习API，<code>tf.contrib.learn</code>就是其中之一。<code>tf.contrib.learn</code>的出现让机器学习模型的建立、训练和评估变得十分的简单。本文介绍了用<code>tf.contrib.learn</code>搭建神经网络分类器并对Iris数据集进行分类，我们的代码分为以下五步：</p>
<ol>
<li>载入包含Iris训练数据和测试数据的CSV文件并保存为<code>Dataset</code>格式</li>
<li>构建神经网络分类器</li>
<li>用训练数据训练模型</li>
<li>评估模型的准确率</li>
<li>对新的样本进行分类</li>
</ol>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/get_started/tflearn" target="_blank" rel="external">https://www.tensorflow.org/get_started/tflearn</a></p>
<p><br></p>
<h1 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h1><p>Iris数据集是一个关于鸢尾花的数据集，样式如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Sepal Length</th>
<th style="text-align:center">Sepal Width</th>
<th style="text-align:center">Petal Length</th>
<th style="text-align:center">Petal Width</th>
<th style="text-align:center">Species</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">5.1</td>
<td style="text-align:center">3.5</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">4.9</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">4.7</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">1.3</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">7.0</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">4.7</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">6.4</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">4.5</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">6.9</td>
<td style="text-align:center">3.1</td>
<td style="text-align:center">4.9</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">6.5</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">5.2</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">6.2</td>
<td style="text-align:center">3.4</td>
<td style="text-align:center">5.4</td>
<td style="text-align:center">2.3</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">5.9</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">5.1</td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">2</td>
</tr>
</tbody>
</table>
<p>数据集包含了150行数据，代表着150个样本，共有3个类别，每个类别有50个样本，具体类别如下（分别用0、1、2表示）：</p>
<ul>
<li>setosa</li>
<li>versicolor</li>
<li>virginica</li>
</ul>
<p>数据集共有四个特征（浮点数）：</p>
<ul>
<li>Sepal length</li>
<li>Sepal width</li>
<li>Petal length</li>
<li>Petal width</li>
</ul>
<p>我们要利用花萼的长度和宽度，花瓣的长度和宽度这四个特征来预测花的类别。</p>
<p><br></p>
<h1 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h1><p>对于本教程，Iris数据集已经被随机排序并且分割成了两个CSV文件：</p>
<ul>
<li>训练集包含120个样本，文件名<code>iris_training.csv</code></li>
<li>测试集包含30个样本，文件名<code>iris_test.csv</code></li>
</ul>
<p>我们只需要从TensorFlow官网下载这两个文件即可，其中<code>iris_training.csv</code>文件如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">120</th>
<th style="text-align:center">4</th>
<th style="text-align:center">setosa</th>
<th style="text-align:center">versicolor</th>
<th style="text-align:center">virginica</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">6.4</td>
<td style="text-align:center">2.8</td>
<td style="text-align:center">5.6</td>
<td style="text-align:center">2.2</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">5.0</td>
<td style="text-align:center">2.3</td>
<td style="text-align:center">3.3</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<p>第一行为头信息，分别描述了训练数据的样本数、特征数以及种类名，接下来的120行为训练数据。前四列为四种特征，最后一列为所属类别。<code>iris_test.csv</code>与此类似。</p>
<p>代码开始阶段，我们首先要导入必要的模块，并且定义我们下载并存储数据集的地址：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> urllib</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">IRIS_TRAINING = <span class="string">"iris_training.csv"</span></div><div class="line">IRIS_TRAINING_URL = <span class="string">"http://download.tensorflow.org/data/iris_training.csv"</span></div><div class="line"></div><div class="line">IRIS_TEST = <span class="string">"iris_test.csv"</span></div><div class="line">IRIS_TEST_URL = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></div></pre></td></tr></table></figure>
<p>接下来判断目标地址有没有该数据集，如果没有则需要下载数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TRAINING):</div><div class="line">  raw = urllib.urlopen(IRIS_TRAINING_URL).read()</div><div class="line">  <span class="keyword">with</span> open(IRIS_TRAINING,<span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(raw)</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TEST):</div><div class="line">  raw = urllib.urlopen(IRIS_TEST_URL).read()</div><div class="line">  <span class="keyword">with</span> open(IRIS_TEST,<span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(raw)</div></pre></td></tr></table></figure>
<p>最后使用<code>load_csv_with_header()</code>函数载入训练和测试数据到<code>Dataset</code>中，该函数在<code>tf.contrib.learn.datasets.base</code>类中，有三个必须的参数：</p>
<ul>
<li><code>filename</code>：表示CSV文件的路径。</li>
<li><code>target_dtype</code>：表示数据集label的<code>numpy</code>类型。</li>
<li><code>features_dtype</code>：表示数据集特征的<code>numpy</code>类型。</li>
</ul>
<p>在这里，我们的label共有三类，用0-2表示，因此它的<code>numpy</code>类型为<code>np.int</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load datasets.</span></div><div class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TRAINING,</div><div class="line">    target_dtype=np.int,</div><div class="line">    features_dtype=np.float32)</div><div class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TEST,</div><div class="line">    target_dtype=np.int,</div><div class="line">    features_dtype=np.float32)</div></pre></td></tr></table></figure>
<p>在<code>tf.contrib.learn</code>中<code>Dataset</code>是命名元组(named tuples)，我们可以通过<code>training_set.data</code>和 <code>training_set.target</code>命令非常方便的获得训练集的特征数据和目标标签数据。同样的，<code>test_set.data</code>和<code>test_set.target</code>分别包含了测试集的特征数据和目标标签数据。 </p>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>1、关于<code>load_csv_with_header()</code>函数的定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">Dataset = collections.namedtuple(<span class="string">'Dataset'</span>, [<span class="string">'data'</span>, <span class="string">'target'</span>])</div><div class="line"><span class="comment">#...</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_csv_with_header</span><span class="params">(filename,</span></span></div><div class="line">                         target_dtype,</div><div class="line">                         features_dtype,</div><div class="line">                         target_column=<span class="number">-1</span>):</div><div class="line">	<span class="string">"""Load dataset from CSV file with a header row."""</span></div><div class="line">	<span class="keyword">with</span> gfile.Open(filename) <span class="keyword">as</span> csv_file:</div><div class="line">		data_file = csv.reader(csv_file)</div><div class="line">    	header = next(data_file) <span class="comment"># 返回第一行数据</span></div><div class="line">        <span class="comment"># 此处data_file可以理解为一个迭代器</span></div><div class="line">        <span class="comment"># 该行代码可以替换为: header = data_file.__next__()</span></div><div class="line">    	n_samples = int(header[<span class="number">0</span>]) <span class="comment"># 返回样本数目</span></div><div class="line">    	n_features = int(header[<span class="number">1</span>]) <span class="comment"># 返回特征数目</span></div><div class="line">    	data = np.zeros((n_samples, n_features), dtype=features_dtype)</div><div class="line">    	target = np.zeros((n_samples,), dtype=target_dtype)</div><div class="line">    	<span class="keyword">for</span> i, row <span class="keyword">in</span> enumerate(data_file): </div><div class="line">    	<span class="comment"># enumerate()函数可以同时遍历索引和元素</span></div><div class="line">        <span class="comment"># 因为data_file为迭代器，上面用过一次next，此处则从第二行开始</span></div><div class="line">      		target[i] = np.asarray(row.pop(target_column), dtype=target_dtype)</div><div class="line">      		data[i] = np.asarray(row, dtype=features_dtype)</div><div class="line"></div><div class="line">	<span class="keyword">return</span> Dataset(data=data, target=target)</div></pre></td></tr></table></figure>
<p>关于生成器的相关概念，可以看下面这个教程：</p>
<ul>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/" target="_blank" rel="external">Python yield 使用浅析</a></li>
</ul>
<p><br></p>
<p>2、关于命名元组：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">collections.namedtuple(typename, </div><div class="line">                       field_names, </div><div class="line">                       verbose=<span class="keyword">False</span>, </div><div class="line">                       rename=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<blockquote>
<p>返回一个名为typenname的元组。参数field_names是一个字符串表示的元素名称，每个字段之间可以通过空格、逗号方式来分隔，比如’x y’，’x, y’。另外也可以采用列表的方式，比如[‘x’, ‘y’]。在字段名称命名上需要注意的是每个字段必须是有效的Python标识符，同时不能是python关键字，另外不要以下划线或数字开头。</p>
<p>如果参数rename为True就会自动地把不合法名称转换为相应合法的名称，比如：<code>[&#39;abc&#39;, &#39;def&#39;, &#39;ghi&#39;, &#39;abc&#39;]</code>转换为<code>[&#39;abc&#39;, &#39;_1&#39;, &#39;ghi&#39;, &#39;_3&#39;]</code>，在这里把def转换<code>_1</code>，同时把重复的abc转换<code>_3</code>。</p>
</blockquote>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#python 3.4</span></div><div class="line"><span class="keyword">import</span> collections</div><div class="line">Point = collections.namedtuple(<span class="string">'Point'</span>, <span class="string">'x, y, z'</span>)</div><div class="line">p1 = Point(<span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>)</div><div class="line">print(p1)</div><div class="line"><span class="comment"># Point(x=30, y=40, z=50)</span></div><div class="line"></div><div class="line">print(p1[<span class="number">0</span>] + p1[<span class="number">1</span>] + p1[<span class="number">2</span>])</div><div class="line"><span class="comment"># 120</span></div><div class="line"></div><div class="line">x, y, z = p1</div><div class="line"></div><div class="line">print(x, y, z)</div><div class="line"><span class="comment"># 30 40 50</span></div><div class="line"></div><div class="line">print(p1.x, p1.y, p1.z)</div><div class="line"><span class="comment"># 30 40 50</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="构建深度神经网络分类器"><a href="#构建深度神经网络分类器" class="headerlink" title="构建深度神经网络分类器"></a>构建深度神经网络分类器</h1><p><code>tf.contrib.learn</code>提供了多种预定义好的模型，我们叫它<code>Estimators</code>，我们可以把它当作黑箱子来训练和评估我们的数据。在这里，我们将搭建一个深度神经网络分类器来训练我们的Iris数据。在使用<code>tf.contrib.learn</code>之前，我们首先要用简短的代码实例化<code>tf.contrib.learn.DNNClassifier</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Specify that all features have real-value data</span></div><div class="line">feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</div><div class="line"></div><div class="line"><span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></div><div class="line">classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</div><div class="line">                                            hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">                                            n_classes=<span class="number">3</span>,</div><div class="line">                                            model_dir=<span class="string">"/tmp/iris_model"</span>)</div></pre></td></tr></table></figure>
<p>上面代码的第一部分定义了特征列(feature columns)，它定义了数据集的数据类型。因为所有的特征数据是连续的，因此我们用<code>tf.contrib.layers.real_valued_column()</code>函数来构建特征列。在数据集中有四种特征(sepal width, sepal height, petal width, and petal height)，因此我们设置<code>dimension=4</code>。</p>
<p>然后，代码用以下参数创建了一个<code>DNNClassifier</code>模型：</p>
<ul>
<li><code>feature_columns=feature_columns</code>，上面提到的特征列的设置。</li>
<li><code>hidden_units=[10, 20, 10]</code>，三个隐藏层，分别包含10、20和10个神经元</li>
<li><code>n_classes=3</code>，目标种类数，共三个类别</li>
<li><code>model_dir=/tmp/iris_model</code>，在训练过程中TensorFlow保存<em>checkpoint</em>数据的路径。</li>
</ul>
<p><br></p>
<h1 id="输入训练数据"><a href="#输入训练数据" class="headerlink" title="输入训练数据"></a>输入训练数据</h1><p><code>tf.contrib.learn</code>的API使用的是输入函数，它们会创建TensorFlow的<code>ops</code>然后生成数据。在本例中，因为数据量足够小因此我们直接把它们存在<code>TensorFlow constants</code>中。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define the training inputs</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_inputs</span><span class="params">()</span>:</span></div><div class="line">  x = tf.constant(training_set.data)</div><div class="line">  y = tf.constant(training_set.target)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> x, y</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>我们已经构建好模型名为<code>classifier</code>，现在我们可以用<code>fit</code>方法来训练Iris数据，通过调用<code>get_train_inputs</code>函数给参数<code>input_fn</code>来传入训练数据，同时设置迭代次数。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Fit model.</span></div><div class="line">classifier.fit(input_fn=get_train_inputs, steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>模型的状态将会保存在<code>classifier</code>中，这意味着我们可以反复的训练。上面的代码等同于如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">classifier.fit(x=training_set.data, y=training_set.target, steps=<span class="number">1000</span>)</div><div class="line">classifier.fit(x=training_set.data, y=training_set.target, steps=<span class="number">1000</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>我们已经在Iris数据集上训练好了一个<code>DNNClassifier</code>模型<code>classifier</code>，现在我们要在Iris测试集上评估模型的准确率。用到的函数为<code>evaluate</code>。诸如<code>fit</code>和<code>evaluate</code>这种函数需要传入一个输入函数来建立输入管道。<code>evaluate</code>函数会返回一个包含着评估结果的<code>dict</code>字典。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define the test inputs</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_inputs</span><span class="params">()</span>:</span></div><div class="line">  x = tf.constant(test_set.data)</div><div class="line">  y = tf.constant(test_set.target)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> x, y</div><div class="line"></div><div class="line"><span class="comment"># Evaluate accuracy.</span></div><div class="line">accuracy_score = classifier.evaluate(input_fn=get_test_inputs,</div><div class="line">                                     steps=<span class="number">1</span>)[<span class="string">"accuracy"</span>]</div><div class="line"></div><div class="line">print(<span class="string">"\nTest Accuracy: &#123;0:f&#125;\n"</span>.format(accuracy_score))</div></pre></td></tr></table></figure>
<p>我们会得到以下输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Test Accuracy: 0.966667</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h2><p>1、<code>evaluate</code>函数输出的为一个<code>dict</code>字典，字典如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'accuracy'</span>: <span class="number">0.93333334</span>, <span class="string">'global_step'</span>: <span class="number">2000</span>, <span class="string">'loss'</span>: <span class="number">0.068770193</span>&#125;</div></pre></td></tr></table></figure>
<p>其中<code>global_step</code>为训练迭代的次数。代码中我们直接令<code>accuracy_score</code>等于其中的<code>accuracy</code>的值。</p>
<p><br></p>
<p>2、python格式化输出<code>print(&quot;\nTest Accuracy: {0:f}\n&quot;.format(accuracy_score))</code></p>
<p>其中的0表示后面第0个数，f表示数据类型为<code>float</code>型。</p>
<p><br></p>
<h1 id="分类新样本"><a href="#分类新样本" class="headerlink" title="分类新样本"></a>分类新样本</h1><p>使用<code>classifier</code>的<code>predict()</code>函数可以对新的样本进行分类。例如我们有如下两个新的鸾尾花的样本：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Sepal Length</th>
<th style="text-align:center">Sepal Width</th>
<th style="text-align:center">Petal Length</th>
<th style="text-align:center">Petal Width</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">6.4</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">4.5</td>
<td style="text-align:center">1.5</td>
</tr>
<tr>
<td style="text-align:center">5.8</td>
<td style="text-align:center">3.1</td>
<td style="text-align:center">5.0</td>
<td style="text-align:center">1.7</td>
</tr>
</tbody>
</table>
<p>我们可以用<code>predict()</code>函数来预测它们的种类。<code>predict</code>会返回一个生成器，我们可以把它很方便的转化为列表(list)。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Classify two new flower samples.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_samples</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">return</span> np.array(</div><div class="line">    [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>],</div><div class="line">     [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=np.float32)</div><div class="line"></div><div class="line">predictions = list(classifier.predict(input_fn=new_samples))</div><div class="line"></div><div class="line">print(</div><div class="line">    <span class="string">"New Samples, Class Predictions:    &#123;&#125;\n"</span></div><div class="line">    .format(predictions))</div></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">New Samples, Class Predictions:    [1 2]</div></pre></td></tr></table></figure>
<p> 结果表明第一个样本种类为<em>Iris versicolor</em>，第二个样本种类为<em>Iris virginica</em>。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/tflearn" target="_blank" rel="external">tf.contrib.learn Quickstart  |  TensorFlow</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/learn/python/learn/datasets/base.py" target="_blank" rel="external">load_csv_with_header()</a></li>
<li><a href="http://blog.csdn.net/caimouse/article/details/50493926" target="_blank" rel="external">5.3.5 namedtuple() 创建命名字段的元组结构 - 大坡3D软件开发 - CSDN博客</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier" target="_blank" rel="external">tf.contrib.learn.DNNClassifier  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（十二）：CNN示例代码CIFAR-10分析（下）]]></title>
      <url>/2017/07/07/TensorFlow_12/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文接上文，继续学习TensorFlow在CIFAR-10上的教程，该代码主要由以下五部分组成：</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py" target="_blank" rel="external"><code>cifar10_input.py</code></a></td>
<td>读取原始的 CIFAR-10 二进制格式文件</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py" target="_blank" rel="external"><code>cifar10.py</code></a></td>
<td>建立 CIFAR-10 网络模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py" target="_blank" rel="external"><code>cifar10_train.py</code></a></td>
<td>在单块CPU或者GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py" target="_blank" rel="external"><code>cifar10_multi_gpu_train.py</code></a></td>
<td>在多块GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py" target="_blank" rel="external"><code>cifar10_eval.py</code></a></td>
<td>在测试集上评估 CIFAR-10 模型的表现</td>
</tr>
</tbody>
</table>
<p>本次主要学习<code>cifar10_train.py</code>和<code>cifar10_eval.py</code>两个文件，内容分别为训练模型和评估模型，并最终给出实验过程。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">https://www.tensorflow.org/tutorials/deep_cnn</a></p>
<p>代码地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>
<p><br></p>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>这部分代码在<code>cifar10_train.py</code>文件中，实现了用单块GPU训练模型，具体训练过程设计为：</p>
<ul>
<li>共计100万次迭代（自己实验时改成了10万次）</li>
<li>batch_size为128</li>
<li>每10次迭代打印一次训练数据（损失、样本/秒、秒/batch）</li>
<li>每600s保存一次<em>checkpoint</em>文件</li>
<li>每300s对最新的<em>checkpoint</em>文件执行一次评估</li>
<li>每100次迭代保存一次summary</li>
</ul>
<p><br></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""用单块GPU训练CIFAR-10"""</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10</div><div class="line"></div><div class="line"><span class="comment">#作用类似于argparse，通过命令行传参改变训练参数</span></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'train_dir'</span>, <span class="string">'/tmp/cifar10_train'</span>,</div><div class="line">                           <span class="string">"""Directory where to write event logs """</span></div><div class="line">                           <span class="string">"""and checkpoint."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'max_steps'</span>, <span class="number">1000000</span>,</div><div class="line">                            <span class="string">"""Number of batches to run."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'log_device_placement'</span>, <span class="keyword">False</span>,</div><div class="line">                            <span class="string">"""Whether to log device placement."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'log_frequency'</span>, <span class="number">10</span>,</div><div class="line">                            <span class="string">"""How often to log results to the console."""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""训练 CIFAR-10 数据集."""</span></div><div class="line">  <span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">    <span class="comment"># 返回或创建全局迭代张量（是一个不会被训练的变量）</span></div><div class="line">    global_step = tf.contrib.framework.get_or_create_global_step()</div><div class="line"></div><div class="line">    <span class="comment"># 获得CIFAR-10的训练数据和标签</span></div><div class="line">    <span class="comment"># 强迫输入管道在 CPU:0 上操作避免有时候操作在GPU上会停止并导致运行变慢</span></div><div class="line">    <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</div><div class="line">      images, labels = cifar10.distorted_inputs()</div><div class="line"></div><div class="line">    <span class="comment"># 用模型的接口函数inference()建立Graph并且计算logits</span></div><div class="line">    logits = cifar10.inference(images)</div><div class="line"></div><div class="line">    <span class="comment"># 计算损失</span></div><div class="line">    loss = cifar10.loss(logits, labels)</div><div class="line"></div><div class="line">    <span class="comment"># 建立 Graph 并用一个batch的数据来训练模型并更新参数</span></div><div class="line">    train_op = cifar10.train(loss, global_step)</div><div class="line"></div><div class="line">    <span class="class"><span class="keyword">class</span> <span class="title">_LoggerHook</span><span class="params">(tf.train.SessionRunHook)</span>:</span></div><div class="line">      <span class="string">"""打印损失和运行时间"""</span></div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">begin</span><span class="params">(self)</span>:</span></div><div class="line">        self._step = <span class="number">-1</span></div><div class="line">        self._start_time = time.time()</div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">before_run</span><span class="params">(self, run_context)</span>:</span></div><div class="line">        self._step += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> tf.train.SessionRunArgs(loss)  <span class="comment"># 计算损失</span></div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">after_run</span><span class="params">(self, run_context, run_values)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._step % FLAGS.log_frequency == <span class="number">0</span>:</div><div class="line">          current_time = time.time()</div><div class="line">          duration = current_time - self._start_time</div><div class="line">          self._start_time = current_time</div><div class="line"></div><div class="line">          loss_value = run_values.results</div><div class="line">          <span class="comment"># 计算每秒钟训练了多少个样本</span></div><div class="line">          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration </div><div class="line">          <span class="comment"># 计算每次迭代用了多长时间</span></div><div class="line">          sec_per_batch = float(duration / FLAGS.log_frequency) </div><div class="line"></div><div class="line">          format_str = (<span class="string">'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '</span></div><div class="line">                        <span class="string">'sec/batch)'</span>)</div><div class="line">          <span class="keyword">print</span> (format_str % (datetime.now(), self._step, loss_value,</div><div class="line">                               examples_per_sec, sec_per_batch))</div><div class="line"></div><div class="line">    <span class="comment"># 开启一个会话执行训练过程</span></div><div class="line">    <span class="comment"># tf.train.NanTensorHook(loss)：监控loss，如果loss为NaN则停止训练</span></div><div class="line">    <span class="keyword">with</span> tf.train.MonitoredTrainingSession(</div><div class="line">        checkpoint_dir=FLAGS.train_dir,</div><div class="line">        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),</div><div class="line">               tf.train.NanTensorHook(loss),</div><div class="line">               _LoggerHook()],</div><div class="line">        config=tf.ConfigProto(</div><div class="line">            log_device_placement=FLAGS.log_device_placement)) <span class="keyword">as</span> mon_sess:</div><div class="line">      <span class="keyword">while</span> <span class="keyword">not</span> mon_sess.should_stop(): <span class="comment"># 如果没有到最大迭代次数</span></div><div class="line">        mon_sess.run(train_op) <span class="comment"># 执行训练过程</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span> </div><div class="line">  cifar10.maybe_download_and_extract() <span class="comment"># 下载数据并解压缩</span></div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(FLAGS.train_dir):</div><div class="line">    tf.gfile.DeleteRecursively(FLAGS.train_dir)</div><div class="line">  tf.gfile.MakeDirs(FLAGS.train_dir)</div><div class="line">  train()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  tf.app.run()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">tf.train.MonitoredTrainingSession(</div><div class="line">    master=<span class="string">''</span>,</div><div class="line">    is_chief=<span class="keyword">True</span>,</div><div class="line">    checkpoint_dir=<span class="keyword">None</span>,</div><div class="line">    scaffold=<span class="keyword">None</span>,</div><div class="line">    hooks=<span class="keyword">None</span>,</div><div class="line">    chief_only_hooks=<span class="keyword">None</span>,</div><div class="line">    save_checkpoint_secs=<span class="number">600</span>,</div><div class="line">    save_summaries_steps=<span class="number">100</span>,</div><div class="line">    save_summaries_secs=<span class="keyword">None</span>,</div><div class="line">    config=<span class="keyword">None</span>,</div><div class="line">    stop_grace_period_secs=<span class="number">120</span>,</div><div class="line">    log_step_count_steps=<span class="number">100</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>每600s保存一次<em>checkpoint</em>，每100s保存一次<em>summary</em>。</p>
<p><br></p>
<h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>这部分代码在<code>cifar10_eval.py</code>中，默认每300s执行一次评估，具体流程：</p>
<ul>
<li><code>evaluate()</code>负责创建和维护整个评估过程： </li>
</ul>
<ol>
<li>获得测试数据</li>
<li>搭建神经网络模型（和训练过程一样） </li>
<li>创建<code>saver</code> ，<code>saver</code>负责恢复<code>shadow variable</code>的值并赋给<code>variable</code></li>
<li>每隔固定的间隔(300s)，运行一次<code>eval_once()</code></li>
</ol>
<ul>
<li><code>eval_once()</code>负责完成一次评估，步骤是： </li>
</ul>
<ol>
<li>从checkpoint中取出最新模型 </li>
<li>运行<code>saver.restore</code>从<code>checkpoint</code>中恢复<code>shadow variable</code>的值并赋给<code>variable</code>。</li>
<li>运行神经网络，对测试集的数据按批次进行预测</li>
<li>计算整个测试集的预测精度</li>
</ol>
<p><br></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""Evaluation for CIFAR-10"""</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'eval_dir'</span>, <span class="string">'/tmp/cifar10_eval'</span>,</div><div class="line">                           <span class="string">"""Directory where to write event logs."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'eval_data'</span>, <span class="string">'test'</span>,</div><div class="line">                           <span class="string">"""Either 'test' or 'train_eval'."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'checkpoint_dir'</span>, <span class="string">'/tmp/cifar10_train'</span>,</div><div class="line">                           <span class="string">"""Directory where to read model checkpoints."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'eval_interval_secs'</span>, <span class="number">60</span> * <span class="number">5</span>,</div><div class="line">                            <span class="string">"""How often to run the eval."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'num_examples'</span>, <span class="number">10000</span>,</div><div class="line">                            <span class="string">"""Number of examples to run."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'run_once'</span>, <span class="keyword">False</span>,</div><div class="line">                         <span class="string">"""Whether to run eval only once."""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_once</span><span class="params">(saver, summary_writer, top_k_op, summary_op)</span>:</span></div><div class="line">  <span class="string">"""运行一次评估</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    saver: Saver.</div><div class="line">    summary_writer: Summary writer.</div><div class="line">    top_k_op: Top K op.</div><div class="line">    summary_op: Summary op.</div><div class="line">  """</div><div class="line">  <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</div><div class="line">      <span class="comment"># 从checkpoint恢复变量的值</span></div><div class="line">      saver.restore(sess, ckpt.model_checkpoint_path)</div><div class="line">      <span class="comment"># model_checkpoint_path提取最新的checkpoint文件名，看起来如下:</span></div><div class="line">      <span class="comment"># /my-favorite-path/cifar10_train/model.ckpt-0</span></div><div class="line">      <span class="comment"># 从中提取出global_step </span></div><div class="line">      global_step = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      print(<span class="string">'No checkpoint file found'</span>)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    <span class="comment"># 开始队列</span></div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">      threads = []</div><div class="line">      <span class="keyword">for</span> qr <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):</div><div class="line">        threads.extend(qr.create_threads(sess, coord=coord, daemon=<span class="keyword">True</span>,</div><div class="line">                                         start=<span class="keyword">True</span>))</div><div class="line"></div><div class="line">      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size)) <span class="comment"># 总的迭代数目</span></div><div class="line">      true_count = <span class="number">0</span>  <span class="comment"># 统计预测正确的数目</span></div><div class="line">      total_sample_count = num_iter * FLAGS.batch_size <span class="comment"># 总的样本数目</span></div><div class="line">      step = <span class="number">0</span></div><div class="line">      <span class="keyword">while</span> step &lt; num_iter <span class="keyword">and</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">        predictions = sess.run([top_k_op])</div><div class="line">        true_count += np.sum(predictions)</div><div class="line">        step += <span class="number">1</span></div><div class="line"></div><div class="line">      <span class="comment"># 计算准确率 @ 1.</span></div><div class="line">      precision = true_count / total_sample_count</div><div class="line">      print(<span class="string">'%s: precision @ 1 = %.3f'</span> % (datetime.now(), precision))</div><div class="line"></div><div class="line">      summary = tf.Summary()</div><div class="line">      summary.ParseFromString(sess.run(summary_op))</div><div class="line">      summary.value.add(tag=<span class="string">'Precision @ 1'</span>, simple_value=precision)</div><div class="line">      summary_writer.add_summary(summary, global_step)</div><div class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:  </div><div class="line">      coord.request_stop(e)</div><div class="line"></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads, stop_grace_period_secs=<span class="number">10</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""Eval CIFAR-10 for a number of steps."""</span></div><div class="line">  <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">    <span class="comment"># 从CIFAR-10中获取图像数据和标签数据</span></div><div class="line">    eval_data = FLAGS.eval_data == <span class="string">'test'</span></div><div class="line">    images, labels = cifar10.inputs(eval_data=eval_data)</div><div class="line"></div><div class="line">    <span class="comment"># 建立一个Graph来计算logits</span></div><div class="line">    logits = cifar10.inference(images)</div><div class="line"></div><div class="line">    <span class="comment"># 计算预测值，输出一个batch_size大小的bool数组</span></div><div class="line">    top_k_op = tf.nn.in_top_k(logits, labels, <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 恢复训练变量的滑动平均值来评估模型</span></div><div class="line">    variable_averages = tf.train.ExponentialMovingAverage(</div><div class="line">        cifar10.MOVING_AVERAGE_DECAY)</div><div class="line">    variables_to_restore = variable_averages.variables_to_restore()</div><div class="line">    saver = tf.train.Saver(variables_to_restore)</div><div class="line"></div><div class="line">    summary_op = tf.summary.merge_all()</div><div class="line"></div><div class="line">    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)</div><div class="line"></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">      eval_once(saver, summary_writer, top_k_op, summary_op)</div><div class="line">      <span class="keyword">if</span> FLAGS.run_once:</div><div class="line">        <span class="keyword">break</span></div><div class="line">      time.sleep(FLAGS.eval_interval_secs)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span>  </div><div class="line">  cifar10.maybe_download_and_extract()</div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(FLAGS.eval_dir):</div><div class="line">    tf.gfile.DeleteRecursively(FLAGS.eval_dir)</div><div class="line">  tf.gfile.MakeDirs(FLAGS.eval_dir)</div><div class="line">  evaluate()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  tf.app.run()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tf.nn.in_top_k(</div><div class="line">    predictions,</div><div class="line">    targets,</div><div class="line">    k,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>判断<code>targets</code>是否在top k的预测之中。输出<code>batch_size</code>大小的bool数组，如果对目标累的预测在所有预测的top k中，则<code>out[i]=True</code>。</p>
<p><br></p>
<h1 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h1><p>作者在单块Tesla K40中训练了10万次用了8小时（350 - 600 images/sec），我在单块Quadro M5000上只用了46分钟（4800～5000 images/sec），下面是训练过程：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">2017-07-07 15:30:08.459355: step 0, loss = 4.68 (317.5 examples/sec; 0.403 sec/batch)</div><div class="line">2017-07-07 15:30:08.794469: step 10, loss = 4.62 (3819.4 examples/sec; 0.034 sec/batch)</div><div class="line">2017-07-07 15:30:09.067413: step 20, loss = 4.49 (4689.6 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:09.343335: step 30, loss = 4.45 (4638.9 examples/sec; 0.028 sec/batch)</div><div class="line">2017-07-07 15:30:09.618720: step 40, loss = 4.31 (4648.0 examples/sec; 0.028 sec/batch)</div><div class="line">2017-07-07 15:30:09.889763: step 50, loss = 4.32 (4722.5 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.162925: step 60, loss = 4.26 (4685.9 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.436191: step 70, loss = 4.07 (4684.1 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.702081: step 80, loss = 4.20 (4814.0 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.963494: step 90, loss = 4.26 (4896.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 15:30:11.442152: step 100, loss = 4.08 (2674.1 examples/sec; 0.048 sec/batch)</div><div class="line">...</div><div class="line">2017-07-07 16:16:08.694992: step 99900, loss = 0.67 (3468.2 examples/sec; 0.037 sec/batch)</div><div class="line">2017-07-07 16:16:08.952094: step 99910, loss = 0.71 (4978.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.211538: step 99920, loss = 0.65 (4933.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.472046: step 99930, loss = 0.76 (4913.5 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.728118: step 99940, loss = 0.81 (4998.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.986376: step 99950, loss = 0.77 (4956.3 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:10.241033: step 99960, loss = 0.56 (5026.4 examples/sec; 0.025 sec/batch)</div><div class="line">2017-07-07 16:16:10.496853: step 99970, loss = 0.71 (5003.5 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:10.760321: step 99980, loss = 0.64 (4858.3 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:11.018312: step 99990, loss = 0.76 (4961.4 examples/sec; 0.026 sec/batch)</div></pre></td></tr></table></figure>
<p>训练和评估过程是放在两个程序分开进行的，具体的实现方法是，在训练过程中，为每个训练变量添加指数滑动平均变量，然后每600s就将模型训练到的变量值保存在<em>checkpoint</em>中，评估过程运行时，从最新存储的<em>checkpoint</em>中取出模型的<code>shadow variable</code>，赋值给对应的变量，然后进行评估。</p>
<p>我们需要同时运行两个程序才能实时的对训练过程进行评估，否则得到的永远只是最新的<em>checkpoint</em>文件中的评估结果。具体可以先运行<code>python cifar_train.py</code> ，再打开另一个窗口运行<code>python cifar_eval.py</code> 。</p>
<p>官方给的代码最大迭代次数是100万，我运行的时候改成了10万。</p>
<p>因为我的迭代速度太快了，到600s时第一次保存<em>checkpoint</em>就已经是两万多次迭代了，可以通过修改<code>tf.train.MonitoredTrainingSession()</code>函数的<code>save_checkpoint_secs</code>参数来修改保存<em>checkpoint</em>的时间间隔，默认600s。</p>
<p>最终10万次迭代后的评估准确率是86.2%，和官方给出的数据还是吻合的。</p>
<p>最后来张TensorBoard的图：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170707164106_Jfe0c9_total_loss.jpeg" alt="Total Loss"></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks  |  TensorFlow</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10 and CIFAR-100 datasets</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/image" target="_blank" rel="external">Images  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/strided_slice" target="_blank" rel="external">tf.strided_slice  |  TensorFlow</a></li>
<li><a href="https://segmentfault.com/a/1190000008793389" target="_blank" rel="external">TensorFlow学习笔记（11）：数据操作指南 - 数据实验室 - SegmentFault</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="external">tf.nn.local_response_normalization  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession" target="_blank" rel="external">tf.train.MonitoredTrainingSession  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（十一）：CNN示例代码CIFAR-10分析（上）]]></title>
      <url>/2017/07/06/TensorFlow_11/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>TensorFlow官方网站关于卷积神经网络的教程有具体实例，该实例在CIFAR-10数据集上实现，我对这部分代码进行了学习，该代码主要由以下五部分组成：</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cifar10_input.py</code></td>
<td>读取原始的 CIFAR-10 二进制格式文件</td>
</tr>
<tr>
<td><code>cifar10.py</code></td>
<td>建立 CIFAR-10 网络模型</td>
</tr>
<tr>
<td><code>cifar10_train.py</code></td>
<td>在单块CPU或者GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><code>cifar10_multi_gpu_train.py</code></td>
<td>在多块GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><code>cifar10_eval.py</code></td>
<td>在测试集上评估 CIFAR-10 模型的表现</td>
</tr>
</tbody>
</table>
<p>本次只对单GPU情况进行学习，对多GPU不做学习。本次学习分上下两部分，本文首先介绍<code>cifar10_input.py</code>、<code>cifar10.py</code>两个函数，内容分别为数据的获取和模型的建立，同时我们还介绍了本次教程的重点和CIFAR-10数据集。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">https://www.tensorflow.org/tutorials/deep_cnn</a></p>
<p>代码地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>
<p><br></p>
<h1 id="教程重点"><a href="#教程重点" class="headerlink" title="教程重点"></a>教程重点</h1><p>该教程主要实现了以下几个用TensorFlow设计大型且复杂网络模型时的重要构造：</p>
<ul>
<li>核心的运算包括卷积(convolution)、relu激活(rectified linear activations)、最大池化(max poolnig)和局部响应归一化(LRN)。</li>
<li>网络训练过程中的可视化操作</li>
<li>对学习到的参数计算滑动平均值(moving average)，并且在评估模型表现时使用它们。</li>
<li>实现学习率衰减策略来训练，采用指数衰减(exponential_decay)方式。</li>
<li>使用队列(queues)操作获取输入数据。</li>
</ul>
<p><br></p>
<h1 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h1><p>CIFAR-10 数据集分类是机器学习领域很经典的任务，该任务旨在把32x32的RGB图像分成十类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.</div></pre></td></tr></table></figure>
<p>我们这里用到的是二进制数据，该数据共有六个文件，其中五个训练数据文件，文件名为：<code>data_batch_1.bin</code>,…, <code>data_batch_5.bin</code>，一个测试数据文件，名为<code>test_batch.bin</code>。每个文件里包含10000个样本，共计50000个训练样本，10000个测试样本。</p>
<p>文件中数据结构如下（文件中并没具体的划分行）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;1 x label&gt;&lt;3072 x pixel&gt;</div><div class="line">...</div><div class="line">&lt;1 x label&gt;&lt;3072 x pixel&gt;</div></pre></td></tr></table></figure>
<p>第一个字节代表着标签，范围0-9分别代表十类。接下来的3072个字节代表着图像像素值，前1024个字节是red通道的值，接着1024个字节是green通道值，最后1024个字节是blue通道值。字节排列是以行为主的顺序，也就是说前32个字节代表red通道下第一行的图像数据。每个文件由10000个3073字节组成，理论上来说每个文件包含30730000字节长的数据。</p>
<p><br></p>
<h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><p>这部分的代码在<code>cifar10_input.py</code>文件中，该代码主要由四个函数组成：</p>
<ul>
<li><code>read_cifar10()</code>：从文件名队列读取二进制数据并提取出单张图片数据。</li>
<li><code>_generate_image_and_label_batch()</code>：利用单张图片数据生成<code>batch</code>数据。</li>
<li><code>distorted_inputs()</code>：构建训练数据并进行预处理。</li>
<li><code>inputs()</code>：构建测试数据并进行预处理（也可以用在训练集上）。</li>
</ul>
<p>给外部调用的主要是后两个函数，分别生成训练数据和测试数据。</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 如果是python2的代码，会有不兼容，这里把python3的特性引入，使得python2也可以运行该代码。</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import <span class="comment"># 绝对引用</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division <span class="comment"># 精确除法</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function <span class="comment"># print函数</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="comment"># xrange返回一个生成器，range返回一个列表，xrange在生成大范围数据的时候更节省内存。</span></div><div class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">IMAGE_SIZE = <span class="number">24</span> <span class="comment"># 图像尺寸</span></div><div class="line"></div><div class="line"><span class="comment"># 描述 CIFAR-10 数据的全局常量。</span></div><div class="line">NUM_CLASSES = <span class="number">10</span> <span class="comment"># 类别数目</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = <span class="number">50000</span> <span class="comment"># 训练样本数目</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = <span class="number">10000</span> <span class="comment"># 测试样本数目</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_cifar10</span><span class="params">(filename_queue)</span>:</span></div><div class="line">    </div><div class="line">  <span class="string">"""从文件名队列读取二进制数据并提取出单张图像数据</span></div><div class="line"></div><div class="line">  建议: </div><div class="line">    如果想N个线程同步读取, 调用这个函数N次即可。</div><div class="line">    这将会产生N个独立的Readers从不同文件不同位置读取数据，进而产生更好的样本混合效果。</div><div class="line">    </div><div class="line">  输入参数:</div><div class="line">    filename_queue: 一个包含文件名列表的字符串队列</div><div class="line"></div><div class="line">  返回一个类，包含了单张图像的各种数据。</div><div class="line">  """</div><div class="line"></div><div class="line">  <span class="class"><span class="keyword">class</span> <span class="title">CIFAR10Record</span><span class="params">(object)</span>:</span> <span class="comment"># 定义一个类</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line">  result = CIFAR10Record() <span class="comment"># 建立类的实体对象</span></div><div class="line"></div><div class="line">  </div><div class="line">  <span class="comment"># 输入数据格式</span></div><div class="line">  label_bytes = <span class="number">1</span>  </div><div class="line">  result.height = <span class="number">32</span></div><div class="line">  result.width = <span class="number">32</span></div><div class="line">  result.depth = <span class="number">3</span></div><div class="line">  image_bytes = result.height * result.width * result.depth</div><div class="line">  </div><div class="line">  record_bytes = label_bytes + image_bytes</div><div class="line"></div><div class="line">  <span class="comment"># 定义固定长度阅读器读取长度为record_bytes的数据，从文件名队列中获取文件并读出单张图像数据。</span></div><div class="line">  <span class="comment"># CIFAR-10格式数据没有头数据和尾数据，所以我们令 header_bytes 和 footer_bytes 保持默认值0。</span></div><div class="line">  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)</div><div class="line">  result.key, value = reader.read(filename_queue)</div><div class="line"></div><div class="line">  <span class="comment"># 使用解码器把字符串类型转化为uint8类型的数据</span></div><div class="line">  record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line"></div><div class="line">  <span class="comment"># 第一个字节代表着标签数据，所以我们把它的格式从uint8转化为int32。</span></div><div class="line">  <span class="comment"># tf.strided_slice(input_, begin, end, strides)</span></div><div class="line">  result.label = tf.cast(</div><div class="line">      tf.strided_slice(record_bytes, [<span class="number">0</span>], [label_bytes]), tf.int32)</div><div class="line"></div><div class="line">  <span class="comment"># 剩下的字节表示图像数据，我们首先根据CIFAR-10的数据排列[depth * height * width] </span></div><div class="line">  <span class="comment"># 把它转化为 [depth, height, width] 形状的张量。</span></div><div class="line">  depth_major = tf.reshape(</div><div class="line">      tf.strided_slice(record_bytes, [label_bytes],</div><div class="line">                       [label_bytes + image_bytes]),</div><div class="line">      [result.depth, result.height, result.width])</div><div class="line">  <span class="comment"># 把 [depth, height, width] 转化为 [height, width, depth] 形状的张量，这是TensorFlow处理图像的格式。</span></div><div class="line">  result.uint8image = tf.transpose(depth_major, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</div><div class="line"></div><div class="line">  <span class="keyword">return</span> result</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_generate_image_and_label_batch</span><span class="params">(image, label, min_queue_examples, batch_size, shuffle)</span>:</span></div><div class="line">  <span class="string">"""生成图像和标签的batch数据</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    image: 3-D Tensor of [height, width, 3] of type.float32.</div><div class="line">    label: 1-D Tensor of type.int32</div><div class="line">    min_queue_examples: int32, 每次出队后队伍中剩下的样本数量的最小值。</div><div class="line">    batch_size: 每个batch的图像数量。</div><div class="line">    shuffle: boolean值表明是否对图像队列随机排序。</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, height, width, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  <span class="comment"># 创造一个样本队列，根据需求决定是否对样本随机排序，每次从队列中出队batch_size个图像数据和标签数据</span></div><div class="line">  num_preprocess_threads = <span class="number">16</span> </div><div class="line">  <span class="comment">#16个Reader平行读取，每个Reader读不同的文件或者位置，可以充分的混合样本数据</span></div><div class="line">  <span class="keyword">if</span> shuffle: <span class="comment"># 对样本队列随机排序</span></div><div class="line">    images, label_batch = tf.train.shuffle_batch(</div><div class="line">        [image, label],</div><div class="line">        batch_size=batch_size,</div><div class="line">        num_threads=num_preprocess_threads,</div><div class="line">        capacity=min_queue_examples + <span class="number">3</span> * batch_size,</div><div class="line">        min_after_dequeue=min_queue_examples)</div><div class="line">  <span class="keyword">else</span>: <span class="comment"># 不对样本队列随机排序</span></div><div class="line">    images, label_batch = tf.train.batch(</div><div class="line">        [image, label],</div><div class="line">        batch_size=batch_size,</div><div class="line">        num_threads=num_preprocess_threads,</div><div class="line">        capacity=min_queue_examples + <span class="number">3</span> * batch_size)</div><div class="line"></div><div class="line">  <span class="comment"># 添加summary节点以便在TensorBoard中对图像信息进行可视化</span></div><div class="line">  tf.summary.image(<span class="string">'images'</span>, images)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> images, tf.reshape(label_batch, [batch_size])</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">distorted_inputs</span><span class="params">(data_dir, batch_size)</span>:</span></div><div class="line">  <span class="string">""" 构造训练数据并对其进行失真处理。</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    data_dir: CIFAR-10数据的存放路径.</div><div class="line">    batch_size: 每个batch中图像的数目.</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  filenames = [os.path.join(data_dir, <span class="string">'data_batch_%d.bin'</span> % i)</div><div class="line">               <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">6</span>)]</div><div class="line">  <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line"></div><div class="line">  <span class="comment"># 创建一个文件名队列</span></div><div class="line">  filename_queue = tf.train.string_input_producer(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># 调用read_cifar10函数从文件名队列中读取文件，并得到单张图像信息</span></div><div class="line">  read_input = read_cifar10(filename_queue)</div><div class="line">  reshaped_image = tf.cast(read_input.uint8image, tf.float32)</div><div class="line"></div><div class="line">  height = IMAGE_SIZE <span class="comment"># 24</span></div><div class="line">  width = IMAGE_SIZE <span class="comment"># 24</span></div><div class="line"></div><div class="line">  <span class="comment"># 对训练数据图像预处理，包括多个随机失真操作。</span></div><div class="line"></div><div class="line">  <span class="comment"># 把原始的32*32的图像随机裁剪为24*24</span></div><div class="line">  distorted_image = tf.random_crop(reshaped_image, [height, width, <span class="number">3</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 随机左右翻转</span></div><div class="line">  distorted_image = tf.image.random_flip_left_right(distorted_image)</div><div class="line"></div><div class="line">  <span class="comment"># 随机改变图像亮度和对比度</span></div><div class="line">  distorted_image = tf.image.random_brightness(distorted_image,</div><div class="line">                                               max_delta=<span class="number">63</span>)</div><div class="line">  distorted_image = tf.image.random_contrast(distorted_image,</div><div class="line">                                             lower=<span class="number">0.2</span>, upper=<span class="number">1.8</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 把图像进行归一化处理，变为0均值和1方差。</span></div><div class="line">  float_image = tf.image.per_image_standardization(distorted_image)</div><div class="line"></div><div class="line">  <span class="comment"># 有时候graph没法推断出tensors的形状，我们可以手动保存tensors的形状信息</span></div><div class="line">  float_image.set_shape([height, width, <span class="number">3</span>]) </div><div class="line">  read_input.label.set_shape([<span class="number">1</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 当采用随机生成batch操作时设定min_after_dequeue的值为50000*0.4=20000，保证足够的随机性。</span></div><div class="line">  min_fraction_of_examples_in_queue = <span class="number">0.4</span></div><div class="line">  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *</div><div class="line">                           min_fraction_of_examples_in_queue)</div><div class="line">  <span class="keyword">print</span> (<span class="string">'Filling queue with %d CIFAR images before starting to train. '</span></div><div class="line">         <span class="string">'This will take a few minutes.'</span> % min_queue_examples)</div><div class="line"></div><div class="line">  <span class="comment"># 通过建立样本队列来生成batch图像数据和batch标签数据</span></div><div class="line">  <span class="keyword">return</span> _generate_image_and_label_batch(float_image, read_input.label, </div><div class="line">                                         min_queue_examples, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(eval_data, data_dir, batch_size)</span>:</span></div><div class="line">  <span class="string">"""构造测试数据.</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    eval_data: bool型，表明使用训练数据还是测试数据，True为测试集</div><div class="line">    data_dir: CIFAR-10数据集的存放路径</div><div class="line">    batch_size: 每个batch的图片数量</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> eval_data: <span class="comment"># 训练数据</span></div><div class="line">    filenames = [os.path.join(data_dir, <span class="string">'data_batch_%d.bin'</span> % i)</div><div class="line">                 <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">6</span>)]</div><div class="line">    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN</div><div class="line">  <span class="keyword">else</span>: <span class="comment"># 测试数据</span></div><div class="line">    filenames = [os.path.join(data_dir, <span class="string">'test_batch.bin'</span>)]</div><div class="line">    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL</div><div class="line"></div><div class="line">  <span class="keyword">for</span> f <span class="keyword">in</span> filenames: <span class="comment"># 检查文件是否存在</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line"></div><div class="line">  <span class="comment"># 创建一个文件名队列</span></div><div class="line">  filename_queue = tf.train.string_input_producer(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># 从文件名队列中的文件中读取单个样本</span></div><div class="line">  read_input = read_cifar10(filename_queue)</div><div class="line">  reshaped_image = tf.cast(read_input.uint8image, tf.float32)</div><div class="line"></div><div class="line">  height = IMAGE_SIZE</div><div class="line">  width = IMAGE_SIZE</div><div class="line"></div><div class="line">  <span class="comment"># 测试时的图像预处理</span></div><div class="line">  <span class="comment"># 沿中心裁剪28*28大小的图像</span></div><div class="line">  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, height, width)</div><div class="line"></div><div class="line">  <span class="comment"># 图像归一化处理</span></div><div class="line">  float_image = tf.image.per_image_standardization(resized_image)</div><div class="line"></div><div class="line">  <span class="comment"># 设置张量的形状</span></div><div class="line">  float_image.set_shape([height, width, <span class="number">3</span>])</div><div class="line">  read_input.label.set_shape([<span class="number">1</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 设置min_after_dequeue参数为20000(训练数据)，4000(测试数据)，保证足够随机性。</span></div><div class="line">  min_fraction_of_examples_in_queue = <span class="number">0.4</span></div><div class="line">  min_queue_examples = int(num_examples_per_epoch * min_fraction_of_examples_in_queue)</div><div class="line"></div><div class="line">  <span class="comment"># 通过建立一个样本队列生成batch图像数据和batch标签数据</span></div><div class="line">  <span class="keyword">return</span> _generate_image_and_label_batch(float_image, read_input.label, </div><div class="line">                                         min_queue_examples, batch_size, shuffle=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>1、<code>tf.strided_slice(input_, begin, end, strides)</code></p>
<p>用法示例(注意和<code>tf.slice()</code>的区分)：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 'input' is [[[1, 1, 1], [2, 2, 2]],</div><div class="line">#             [[3, 3, 3], [4, 4, 4]],</div><div class="line">#             [[5, 5, 5], [6, 6, 6]]]</div><div class="line">tf.strided_slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3]]]</div><div class="line">tf.strided_slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3],</div><div class="line">                                                               [4, 4, 4]]]</div><div class="line">tf.strided_slice(input, [1, -1, 0], [2, -3, 3], [1, -1, 1]) ==&gt;[[[4, 4, 4],</div><div class="line">                                                              [3, 3, 3]]]</div></pre></td></tr></table></figure>
<p><br></p>
<p>2、TensorFlow提供两种类型的拼接：</p>
<ul>
<li><code>tf.concat(values, axis, name=&#39;concat&#39;)</code>：按照指定的<strong>已经存在</strong>的轴进行拼接</li>
</ul>
<ul>
<li><code>tf.stack(values, axis=0, name=&#39;stack&#39;)</code>：按照指定的<strong>新建</strong>的轴进行拼接</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">t1 = [[1, 2, 3], [4, 5, 6]]</div><div class="line">t2 = [[7, 8, 9], [10, 11, 12]]</div><div class="line">tf.concat([t1, t2], 0) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</div><div class="line">tf.concat([t1, t2], 1) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</div><div class="line">tf.stack([t1, t2], 0)  ==&gt; [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]</div><div class="line">tf.stack([t1, t2], 1)  ==&gt; [[[1, 2, 3], [7, 8, 9]], [[4, 5, 6], [10, 11, 12]]]</div><div class="line">tf.stack([t1, t2], 2)  ==&gt; [[[1, 7], [2, 8], [3, 9]], [[4, 10], [5, 11], [6, 12]]]</div></pre></td></tr></table></figure>
<p>上面的结果读起来不太直观，我们从shape角度看一下就很容易明白了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</div><div class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</div><div class="line">tf.concat([t1, t2], <span class="number">0</span>)  <span class="comment"># [2,3] + [2,3] ==&gt; [4, 3]</span></div><div class="line">tf.concat([t1, t2], <span class="number">1</span>)  <span class="comment"># [2,3] + [2,3] ==&gt; [2, 6]</span></div><div class="line">tf.stack([t1, t2], <span class="number">0</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2*,2,3]</span></div><div class="line">tf.stack([t1, t2], <span class="number">1</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2,2*,3]</span></div><div class="line">tf.stack([t1, t2], <span class="number">2</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2,3,2*]</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><p>这部分代码在<code>cifar10.py</code>文件中，该代码主要由以下函数组成：</p>
<ul>
<li><code>_activation_summary()</code>：为激活函数创造可视化summary</li>
<li><code>_variable_on_cpu()</code>：新建一个变量存储在CPU上</li>
<li><code>_variable_with_weight_decay()</code>：新建一个已经初始化的变量并计算正则化损失</li>
<li><code>distorted_inputs()</code>：获得训练数据</li>
<li><code>inputs()</code>：获得测试数据</li>
<li><code>inference()</code>：搭建神经网络模型，输出Logits</li>
<li><code>loss()</code>：计算总体损失=交叉熵+正则化</li>
<li><code>_add_loss_summaries()</code>：为损失添加可视化summary</li>
<li><code>train()</code>：创建一个optimizer并给所有训练变量添加滑动平均</li>
<li><code>maybe_download_and_extract()</code>：下载并解压训练数据</li>
</ul>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""搭建 CIFAR-10 网络"""</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> tarfile <span class="comment"># 实现文件的压缩与解压缩</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10_input</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line"><span class="comment"># 基本的模型参数，默认batch_size128</span></div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'batch_size'</span>, <span class="number">128</span>,</div><div class="line">                            <span class="string">"""Number of images to process in a batch."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'data_dir'</span>, <span class="string">'/tmp/cifar10_data'</span>,</div><div class="line">                           <span class="string">"""Path to the CIFAR-10 data directory."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'use_fp16'</span>, <span class="keyword">False</span>,</div><div class="line">                            <span class="string">"""Train the model using fp16."""</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设置描述CIFAR-10数据的全局常量</span></div><div class="line">IMAGE_SIZE = cifar10_input.IMAGE_SIZE <span class="comment"># 28</span></div><div class="line">NUM_CLASSES = cifar10_input.NUM_CLASSES <span class="comment"># 10</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN <span class="comment"># 50000</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL <span class="comment"># 10000</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 训练过程中用到的常量</span></div><div class="line">MOVING_AVERAGE_DECAY = <span class="number">0.9999</span>     <span class="comment"># 计算滑动平均(moving average)时的衰减(decay)</span></div><div class="line">NUM_EPOCHS_PER_DECAY = <span class="number">350.0</span>      <span class="comment"># 学习率衰减的epochs</span></div><div class="line">LEARNING_RATE_DECAY_FACTOR = <span class="number">0.1</span>  <span class="comment"># 学习率衰减因子</span></div><div class="line">INITIAL_LEARNING_RATE = <span class="number">0.1</span>       <span class="comment"># 初始学习率</span></div><div class="line"></div><div class="line"><span class="comment"># 如果一个模型在多GPU上训练, 在Op名字上加上前缀tower_name来区分操作</span></div><div class="line"><span class="comment"># 注意当我们可视化一个模型的时候该前缀会被移去。</span></div><div class="line">TOWER_NAME = <span class="string">'tower'</span></div><div class="line"></div><div class="line">DATA_URL = <span class="string">'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_activation_summary</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="string">"""为激活函数创建summary</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    x: Tensor</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    nothing</div><div class="line">  """</div><div class="line">  <span class="comment"># 如果是一个多GPU训练，就把'tower_[0-9]/'移除，这将会帮助我们更清晰的在TensorBoard上展示。</span></div><div class="line">  tensor_name = re.sub(<span class="string">'%s_[0-9]*/'</span> % TOWER_NAME, <span class="string">''</span>, x.op.name)</div><div class="line">  tf.summary.histogram(tensor_name + <span class="string">'/activations'</span>, x)</div><div class="line">  tf.summary.scalar(tensor_name + <span class="string">'/sparsity'</span>,</div><div class="line">                                       tf.nn.zero_fraction(x)) <span class="comment"># 统计0的比例反映稀疏度</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_variable_on_cpu</span><span class="params">(name, shape, initializer)</span>:</span></div><div class="line">  <span class="string">"""帮助新建一个存储在CPU上的变量</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    name: 变量名</div><div class="line">    shape: 变量形状</div><div class="line">    initializer: 变量的初始化器</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    变量Tensor</div><div class="line">  """</div><div class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</div><div class="line">    dtype = tf.float16 <span class="keyword">if</span> FLAGS.use_fp16 <span class="keyword">else</span> tf.float32</div><div class="line">    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)</div><div class="line">  <span class="keyword">return</span> var</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_variable_with_weight_decay</span><span class="params">(name, shape, stddev, wd)</span>:</span></div><div class="line">  <span class="string">"""帮助新建一个已经初始化的变量并且计算正则化损失</span></div><div class="line"></div><div class="line">  变量初始化采用的 truncated normal 分布</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    name: 变量名</div><div class="line">    shape: 变量形状</div><div class="line">    stddev: truncated Gaussian 分布的标准差</div><div class="line">    wd: 正则化系数，添加正则化损失乘以该系数，如果是None则不添加。</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    Variable Tensor</div><div class="line">  """</div><div class="line">  dtype = tf.float16 <span class="keyword">if</span> FLAGS.use_fp16 <span class="keyword">else</span> tf.float32</div><div class="line">  var = _variable_on_cpu(</div><div class="line">      name,</div><div class="line">      shape,</div><div class="line">      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))</div><div class="line">  <span class="keyword">if</span> wd <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=<span class="string">'weight_loss'</span>)</div><div class="line">    tf.add_to_collection(<span class="string">'losses'</span>, weight_decay) <span class="comment"># 把正则化损失存储为全局变量</span></div><div class="line">  <span class="keyword">return</span> var</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">distorted_inputs</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""获得训练数据，对cifar10_input.py文件里distorted_inputs()进行封装</span></div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line"></div><div class="line">  引发:</div><div class="line">    ValueError: 如果没有data_dir</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.data_dir:</div><div class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Please supply a data_dir'</span>)</div><div class="line">  data_dir = os.path.join(FLAGS.data_dir, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,</div><div class="line">                                                  batch_size=FLAGS.batch_size)</div><div class="line">  <span class="keyword">if</span> FLAGS.use_fp16:</div><div class="line">    images = tf.cast(images, tf.float16)</div><div class="line">    labels = tf.cast(labels, tf.float16)</div><div class="line">  <span class="keyword">return</span> images, labels</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(eval_data)</span>:</span></div><div class="line">  <span class="string">"""获得测试数据，对cifar10_input.py文件里inputs()进行封装</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    eval_data: bool, 指出是否使用测试数据还是训练数据（一般是True）</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line"></div><div class="line">  引发:</div><div class="line">    ValueError: 如果没有data_dir</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.data_dir:</div><div class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Please supply a data_dir'</span>)</div><div class="line">  data_dir = os.path.join(FLAGS.data_dir, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  images, labels = cifar10_input.inputs(eval_data=eval_data,</div><div class="line">                                        data_dir=data_dir,</div><div class="line">                                        batch_size=FLAGS.batch_size)</div><div class="line">  <span class="keyword">if</span> FLAGS.use_fp16:</div><div class="line">    images = tf.cast(images, tf.float16)</div><div class="line">    labels = tf.cast(labels, tf.float16)</div><div class="line">  <span class="keyword">return</span> images, labels</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(images)</span>:</span></div><div class="line">  <span class="string">"""搭建 CIFAR-10 模型.</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    images: 从 distorted_inputs() 或 inputs() 中返回的图像数据</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    Logits</div><div class="line">  """</div><div class="line">  <span class="comment"># 我们是用 tf.get_variable() 而不是 tf.Variable() 来新建变量以便在多GPU训练中共享变量</span></div><div class="line">  <span class="comment"># 如果我们只是在单块GPU上训练，可以简化 tf.get_variable() 变成tf.Variable()</span></div><div class="line"></div><div class="line">  <span class="comment"># conv1</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv1'</span>) <span class="keyword">as</span> scope:</div><div class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</div><div class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>],</div><div class="line">                                         stddev=<span class="number">5e-2</span>,</div><div class="line">                                         wd=<span class="number">0.0</span>)</div><div class="line">    conv = tf.nn.conv2d(images, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    pre_activation = tf.nn.bias_add(conv, biases) <span class="comment"># tf.add()的特例，该函数中biases只能是1-D维度的</span></div><div class="line">    conv1 = tf.nn.relu(pre_activation, name=scope.name)</div><div class="line">    _activation_summary(conv1)</div><div class="line"></div><div class="line">  <span class="comment"># pool1</span></div><div class="line">  pool1 = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                         padding=<span class="string">'SAME'</span>, name=<span class="string">'pool1'</span>)</div><div class="line">  <span class="comment"># norm1</span></div><div class="line">  norm1 = tf.nn.lrn(pool1, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>,</div><div class="line">                    name=<span class="string">'norm1'</span>)</div><div class="line"></div><div class="line">  <span class="comment"># conv2</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv2'</span>) <span class="keyword">as</span> scope:</div><div class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</div><div class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>],</div><div class="line">                                         stddev=<span class="number">5e-2</span>,</div><div class="line">                                         wd=<span class="number">0.0</span>)</div><div class="line">    conv = tf.nn.conv2d(norm1, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    pre_activation = tf.nn.bias_add(conv, biases)</div><div class="line">    conv2 = tf.nn.relu(pre_activation, name=scope.name)</div><div class="line">    _activation_summary(conv2)</div><div class="line"></div><div class="line">  <span class="comment"># norm2</span></div><div class="line">  norm2 = tf.nn.lrn(conv2, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>,</div><div class="line">                    name=<span class="string">'norm2'</span>)</div><div class="line">  <span class="comment"># pool2</span></div><div class="line">  pool2 = tf.nn.max_pool(norm2, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</div><div class="line">                         strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=<span class="string">'pool2'</span>)</div><div class="line"></div><div class="line">  <span class="comment"># local3</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local3'</span>) <span class="keyword">as</span> scope:</div><div class="line">    <span class="comment"># Move everything into depth so we can perform a single matrix multiply.</span></div><div class="line">    reshape = tf.reshape(pool2, [FLAGS.batch_size, <span class="number">-1</span>])</div><div class="line">    dim = reshape.get_shape()[<span class="number">1</span>].value</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[dim, <span class="number">384</span>],</div><div class="line">                                          stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">384</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)</div><div class="line">    _activation_summary(local3)</div><div class="line"></div><div class="line">  <span class="comment"># local4</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local4'</span>) <span class="keyword">as</span> scope:</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[<span class="number">384</span>, <span class="number">192</span>],</div><div class="line">                                          stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">192</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)</div><div class="line">    _activation_summary(local4)</div><div class="line"></div><div class="line">  <span class="comment"># 线性层(WX + b)</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax_linear'</span>) <span class="keyword">as</span> scope:</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, [<span class="number">192</span>, NUM_CLASSES],</div><div class="line">                                          stddev=<span class="number">1</span>/<span class="number">192.0</span>, wd=<span class="number">0.0</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [NUM_CLASSES],</div><div class="line">                              tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)</div><div class="line">    _activation_summary(softmax_linear)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> softmax_linear</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(logits, labels)</span>:</span></div><div class="line">  <span class="string">"""添加L2正则化损失到所有的训练变量中</span></div><div class="line"></div><div class="line">  为 "Loss" 和 "Loss/avg" 添加可视化summary</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    logits: 从 inference() 得到的logits</div><div class="line">    labels: 从 distorted_inputs() 或 inputs() 得到的标签数据. 1-D tensor 形状为 [batch_size]</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    float类型的损失tensor.</div><div class="line">  """</div><div class="line">  <span class="comment"># 计算batch的平均交叉熵损失</span></div><div class="line">  <span class="comment"># 如果是label是one-hot码则用tf.nn.softmax_cross_entropy_with_logits()</span></div><div class="line">  <span class="comment"># 这里label是从0-9的数字来表示，则用tf.nn.sparse_softmax_cross_entropy_with_logits()</span></div><div class="line">  labels = tf.cast(labels, tf.int64)</div><div class="line">  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</div><div class="line">      labels=labels, logits=logits, name=<span class="string">'cross_entropy_per_example'</span>)</div><div class="line">  cross_entropy_mean = tf.reduce_mean(cross_entropy, name=<span class="string">'cross_entropy'</span>)</div><div class="line">  tf.add_to_collection(<span class="string">'losses'</span>, cross_entropy_mean)</div><div class="line"></div><div class="line">  <span class="comment"># 总的损失是交叉熵损失加上所有变量的L2正则化损失</span></div><div class="line">  <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">'losses'</span>), name=<span class="string">'total_loss'</span>) </div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_loss_summaries</span><span class="params">(total_loss)</span>:</span></div><div class="line">  <span class="string">"""为损失添加可视化summary</span></div><div class="line"></div><div class="line">  为所有的损失生成滑动平均并且关联summaries来可视化网络表现。</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    total_loss: 从loss()函数得到的总损失.</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    loss_averages_op: 生成损失滑动平均的op</div><div class="line">  """</div><div class="line">  <span class="comment"># 为所有的独立损失和总的损失计算滑动平均</span></div><div class="line">  loss_averages = tf.train.ExponentialMovingAverage(<span class="number">0.9</span>, name=<span class="string">'avg'</span>)</div><div class="line">  losses = tf.get_collection(<span class="string">'losses'</span>) <span class="comment"># 得到包含独立loss的列表</span></div><div class="line">  <span class="comment"># 把每个独立loss和总的loss合并成一个列表，并计算滑动平均</span></div><div class="line">  loss_averages_op = loss_averages.apply(losses + [total_loss])</div><div class="line"></div><div class="line">  <span class="comment"># 添加一个scalar summary给所有的独立损失和总体损失以及它们的滑动平均</span></div><div class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> losses + [total_loss]:</div><div class="line">    <span class="comment"># 给每个损失命名'(raw)'同时给滑动平均损失原始的命名</span></div><div class="line">    tf.summary.scalar(l.op.name + <span class="string">' (raw)'</span>, l)</div><div class="line">    tf.summary.scalar(l.op.name, loss_averages.average(l))</div><div class="line"></div><div class="line">  <span class="keyword">return</span> loss_averages_op</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(total_loss, global_step)</span>:</span></div><div class="line">  <span class="string">"""训练 CIFAR-10 模型.</span></div><div class="line"></div><div class="line">  新建一个优化器并且应用到所有的训练变量上，给所有的训练变量添加滑动平均</div><div class="line">  用tf.control_dependencies控制着执行的先后顺序</div><div class="line">  执行顺序如下：</div><div class="line">  计算损失的滑动平均——&gt;计算学习率——&gt;计算梯度——&gt;更新参数——&gt;计算训练变量的滑动平均——&gt;train_op(返回的值)</div><div class="line">  通过执行train_op来依次执行之前的所有步骤</div><div class="line">  </div><div class="line">  输入参数:</div><div class="line">    total_loss: 从loss()得到的总损失.</div><div class="line">    global_step: 整型Variable来计算迭代次数</div><div class="line"></div><div class="line">  输出数据:</div><div class="line">    train_op: 训练的op.</div><div class="line">  """</div><div class="line">  <span class="comment"># 影响学习率的变量.</span></div><div class="line">  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size</div><div class="line">  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)</div><div class="line"></div><div class="line">  <span class="comment"># 学习率随着迭代次数指数衰减</span></div><div class="line">  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,</div><div class="line">                                  global_step,</div><div class="line">                                  decay_steps,</div><div class="line">                                  LEARNING_RATE_DECAY_FACTOR,</div><div class="line">                                  staircase=<span class="keyword">True</span>)</div><div class="line">  tf.summary.scalar(<span class="string">'learning_rate'</span>, lr)</div><div class="line"></div><div class="line">  <span class="comment"># 对所有损失生成滑动平均并且关联可视化summaries.</span></div><div class="line">  loss_averages_op = _add_loss_summaries(total_loss)</div><div class="line"></div><div class="line">  <span class="comment"># 计算梯度（先执行生成滑动损失的操作loss_averages_op，再计算梯度）。</span></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([loss_averages_op]):</div><div class="line">    opt = tf.train.GradientDescentOptimizer(lr)</div><div class="line">    grads = opt.compute_gradients(total_loss)</div><div class="line"></div><div class="line">  <span class="comment"># 应用梯度更新参数</span></div><div class="line">  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有训练变量添加可视化histograms</span></div><div class="line">  <span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</div><div class="line">    tf.summary.histogram(var.op.name, var)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有梯度添加可视化histograms</span></div><div class="line">  <span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</div><div class="line">    <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      tf.summary.histogram(var.op.name + <span class="string">'/gradients'</span>, grad)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有训练变量添加滑动平均</span></div><div class="line">  variable_averages = tf.train.ExponentialMovingAverage(</div><div class="line">      MOVING_AVERAGE_DECAY, global_step)</div><div class="line">  variables_averages_op = variable_averages.apply(tf.trainable_variables())</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([apply_gradient_op, variables_averages_op]):</div><div class="line">    train_op = tf.no_op(name=<span class="string">'train'</span>) <span class="comment"># 什么也不做，只是为了确保上面两个op的执行</span></div><div class="line"></div><div class="line">  <span class="keyword">return</span> train_op</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download_and_extract</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""下载并解压缩数据"""</span></div><div class="line">  dest_directory = FLAGS.data_dir</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dest_directory):</div><div class="line">    os.makedirs(dest_directory)</div><div class="line">  filename = DATA_URL.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</div><div class="line">  filepath = os.path.join(dest_directory, filename)</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filepath):</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_progress</span><span class="params">(count, block_size, total_size)</span>:</span></div><div class="line">      sys.stdout.write(<span class="string">'\r&gt;&gt; Downloading %s %.1f%%'</span> % (filename,</div><div class="line">          float(count * block_size) / float(total_size) * <span class="number">100.0</span>))</div><div class="line">      sys.stdout.flush()</div><div class="line">    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)</div><div class="line">    print()</div><div class="line">    statinfo = os.stat(filepath)</div><div class="line">    print(<span class="string">'Successfully downloaded'</span>, filename, statinfo.st_size, <span class="string">'bytes.'</span>)</div><div class="line">  extracted_dir_path = os.path.join(dest_directory, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(extracted_dir_path):</div><div class="line">    tarfile.open(filepath, <span class="string">'r:gz'</span>).extractall(dest_directory)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>这里介绍下LRN层，也就是<code>local response normalization</code>，局部响应归一化。最早是由Krizhevsky和Hinton在论文《ImageNet Classification with Deep Convolutional Neural Networks》里面使用的一种数据标准化方法。这种方法是受到神经科学的启发，激活的神经元会抑制其邻近神经元的活动（侧抑制现象），至于为什么使用这种正则手段，以及它为什么有效，查阅了很多文献似乎也没有详细的解释。加上后来BN层太火，LRN用的就比较少了。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks  |  TensorFlow</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10 and CIFAR-100 datasets</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/image" target="_blank" rel="external">Images  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/strided_slice" target="_blank" rel="external">tf.strided_slice  |  TensorFlow</a></li>
<li><a href="https://segmentfault.com/a/1190000008793389" target="_blank" rel="external">TensorFlow学习笔记（11）：数据操作指南 - 数据实验室 - SegmentFault</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="external">tf.nn.local_response_normalization  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession" target="_blank" rel="external">tf.train.MonitoredTrainingSession  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（十）：张量的阶、形状和数据类型]]></title>
      <url>/2017/07/05/TensorFlow_10/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文整理了TensorFlow中<strong>张量(Tensor)</strong>相关的一些概念：</p>
<ul>
<li>阶(Rank)</li>
<li>形状(Shape)</li>
<li>数据类型(Type)</li>
</ul>
<p>TensorFlow用张量这种数据结构来表示所有的数据，我们可以把一个张量想象成一个n维的数组或列表。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="阶-Rank"><a href="#阶-Rank" class="headerlink" title="阶(Rank)"></a>阶(Rank)</h1><p>在TensorFlow系统中，张量的维数来被描述为阶(Rank)。张量的阶和矩阵的阶并不是同一个概念，张量的阶是张量维数的一个数量描述。比如下面的张量（使用Python中list定义的）就是2阶。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">t = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</div></pre></td></tr></table></figure>
<p>可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量。对于一个二阶张量我们可以用语句<code>t[i, j]</code>来访问其中的任何元素。而对于三阶张量你可以用<code>t[i, j, k]</code>来访问其中的任何元素。</p>
<p>维数越靠后，位置越靠里。比如上面的二阶张量，<code>t[1, 3] = 3</code>。维数越靠前，位置越靠外，比如第一维度的数据，是最外层中括号的数据。</p>
<table>
<thead>
<tr>
<th>阶</th>
<th>数学实例</th>
<th>Python 例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>纯量 (只有大小)</td>
<td><code>s = 483</code></td>
</tr>
<tr>
<td>1</td>
<td>向量(大小和方向)</td>
<td><code>v = [1.1, 2.2, 3.3]</code></td>
</tr>
<tr>
<td>2</td>
<td>矩阵(数据表)</td>
<td><code>m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</code></td>
</tr>
<tr>
<td>3</td>
<td>3阶张量 (数据立体)</td>
<td><code>t = [[[2], [4], [6]], [[8], [10], [12]], [[14], [16], [18]]]</code></td>
</tr>
<tr>
<td>n</td>
<td>n阶 (自己想想看)</td>
<td><code>....</code></td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="形状-Shape"><a href="#形状-Shape" class="headerlink" title="形状(Shape)"></a>形状(Shape)</h1><p>TensorFlow中使用了三种记号来方便地描述张量的维度：阶，形状以及维数。下表展示了它们之间的关系：</p>
<table>
<thead>
<tr>
<th>阶</th>
<th>形状</th>
<th>维数</th>
<th>实例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[ ]</td>
<td>0-D</td>
<td>一个0维张量，一个纯量。</td>
</tr>
<tr>
<td>1</td>
<td>[D0]</td>
<td>1-D</td>
<td>一个1维张量的形式[5]</td>
</tr>
<tr>
<td>2</td>
<td>[D0, D1]</td>
<td>2-D</td>
<td>一个2维张量的形式[3, 4]</td>
</tr>
<tr>
<td>3</td>
<td>[D0, D1, D2]</td>
<td>3-D</td>
<td>一个3维张量的形式 [1, 4, 3]</td>
</tr>
<tr>
<td>n</td>
<td>[D0, D1, … Dn]</td>
<td>n-D</td>
<td>一个n维张量的形式 [D0, D1, … Dn]</td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="数据类型-Type"><a href="#数据类型-Type" class="headerlink" title="数据类型(Type)"></a>数据类型(Type)</h1><p>除了维度，Tensors有一个数据类型属性.你可以为一个张量指定下列数据类型中的任意一个类型：</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>Python 类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DT_FLOAT</code></td>
<td><code>tf.float32</code></td>
<td>32 位浮点数.</td>
</tr>
<tr>
<td><code>DT_DOUBLE</code></td>
<td><code>tf.float64</code></td>
<td>64 位浮点数.</td>
</tr>
<tr>
<td><code>DT_INT64</code></td>
<td><code>tf.int64</code></td>
<td>64 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT32</code></td>
<td><code>tf.int32</code></td>
<td>32 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT16</code></td>
<td><code>tf.int16</code></td>
<td>16 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT8</code></td>
<td><code>tf.int8</code></td>
<td>8 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_UINT8</code></td>
<td><code>tf.uint8</code></td>
<td>8 位无符号整型.</td>
</tr>
<tr>
<td><code>DT_STRING</code></td>
<td><code>tf.string</code></td>
<td>可变长度的字节数组，每一个张量元素都是一个字节数组。</td>
</tr>
<tr>
<td><code>DT_BOOL</code></td>
<td><code>tf.bool</code></td>
<td>布尔型</td>
</tr>
<tr>
<td><code>DT_COMPLEX64</code></td>
<td><code>tf.complex64</code></td>
<td>由两个32位浮点数组成的复数：实数和虚数。</td>
</tr>
<tr>
<td><code>DT_QINT32</code></td>
<td><code>tf.qint32</code></td>
<td>用于量化Ops的32位有符号整型</td>
</tr>
<tr>
<td><code>DT_QINT8</code></td>
<td><code>tf.qint8</code></td>
<td>用于量化Ops的8位有符号整型</td>
</tr>
<tr>
<td><code>DT_QUINT8</code></td>
<td><code>tf.quint8</code></td>
<td>用于量化Ops的8位无符号整型</td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/dims_types" target="_blank" rel="external">Tensor Ranks, Shapes, and Types  |  TensorFlow</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/resources/dims_types.html" target="_blank" rel="external">张量的阶、形状、数据类型 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（九）：数据读取]]></title>
      <url>/2017/07/05/TensorFlow_9/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文整理了TensorFlow中的数据读取方法，在TensorFlow中主要有三种方法读取数据：</p>
<ol>
<li>Feeding：由Python提供数据。</li>
<li>Preloaded data：预加载数据。</li>
<li>Reading from files：从文件读取。</li>
</ol>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Feeding"><a href="#Feeding" class="headerlink" title="Feeding"></a>Feeding</h1><p>我们一般用<code>tf.placeholder</code>节点来<code>feed</code>数据，该节点不需要初始化也不包含任何数据，我们在执行<code>run()</code>或者<code>eval()</code>指令时通过<code>feed_dict</code>参数把数据传入<code>graph</code>中来计算。如果在运行过程中没有对<code>tf.placeholder</code>节点传入数据，程序会报错。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 设计Graph</span></div><div class="line">x1 = tf.placeholder(tf.int16)</div><div class="line">x2 = tf.placeholder(tf.int16)</div><div class="line">y = tf.add(x1, x2)</div><div class="line"><span class="comment"># 用Python产生数据</span></div><div class="line">li1 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</div><div class="line">li2 = [<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>]</div><div class="line"><span class="comment"># 打开一个session --&gt; 喂数据 --&gt; 计算y</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="keyword">print</span> sess.run(y, feed_dict=&#123;x1: li1, x2: li2&#125;)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Preloaded-data"><a href="#Preloaded-data" class="headerlink" title="Preloaded data"></a>Preloaded data</h1><p>预加载数据方法仅限于用在可以完全加载到内存中的小数据集上，主要有两种方法：</p>
<ol>
<li>把数据存在常量（constant）中。</li>
<li>把数据存在变量（variable）中，我们初始化并且永不改变它的值。</li>
</ol>
<p>用常量更简单些，但会占用更多的内存，因为常量存储在<code>graph</code>数据结构内部。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">training_data = ...</div><div class="line">training_labels = ...</div><div class="line"><span class="keyword">with</span> tf.Session():</div><div class="line">  input_data = tf.constant(training_data)</div><div class="line">  input_labels = tf.constant(training_labels)</div><div class="line">  ...</div></pre></td></tr></table></figure>
<p>如果用变量的话，我们需要在<code>graph</code>构建好之后初始化该变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">training_data = ...</div><div class="line">training_labels = ...</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  data_initializer = tf.placeholder(dtype=training_data.dtype,</div><div class="line">                                    shape=training_data.shape)</div><div class="line">  label_initializer = tf.placeholder(dtype=training_labels.dtype,</div><div class="line">                                     shape=training_labels.shape)</div><div class="line">  input_data = tf.Variable(data_initializer, trainable=<span class="keyword">False</span>, collections=[])</div><div class="line">  input_labels = tf.Variable(label_initializer, trainable=<span class="keyword">False</span>, collections=[])</div><div class="line">  ...</div><div class="line">  sess.run(input_data.initializer,</div><div class="line">           feed_dict=&#123;data_initializer: training_data&#125;)</div><div class="line">  sess.run(input_labels.initializer,</div><div class="line">           feed_dict=&#123;label_initializer: training_labels&#125;)</div></pre></td></tr></table></figure>
<p>设定<code>trainable=False</code> 可以防止该变量被数据流图的 <code>GraphKeys.TRAINABLE_VARIABLES</code> 收集, 这样我们就不会在训练的时候尝试更新它的值； 设定 <code>collections=[]</code> 可以防止<code>GraphKeys.VARIABLES</code> 把它收集后做为保存和恢复的中断点。</p>
<p>无论哪种方式，我们可以用<code>tf.train.slice_input_producer</code>函数每次产生一个切片。这样就会让样本在整个迭代中被打乱，所以在使用批处理的时候不需要再次打乱样本。所以我们不使用<code>shuffle_batch</code>函数，取而代之的是纯<code>tf.train.batch</code> 函数。 如果要使用多个线程进行预处理，需要将<code>num_threads</code>参数设置为大于1的数字。</p>
<p><br></p>
<h1 id="Reading-from-files"><a href="#Reading-from-files" class="headerlink" title="Reading from files"></a>Reading from files</h1><p>从文件中读取数据一般包含以下步骤：</p>
<ol>
<li>文件名列表</li>
<li>文件名随机排序（可选的）</li>
<li>迭代控制（可选的）</li>
<li>文件名队列</li>
<li>针对输入文件格式的阅读器</li>
<li>记录解析器</li>
<li>预处理器（可选的）</li>
<li>样本队列</li>
</ol>
<p><br></p>
<h2 id="文件名、随机排序和迭代控制"><a href="#文件名、随机排序和迭代控制" class="headerlink" title="文件名、随机排序和迭代控制"></a>文件名、随机排序和迭代控制</h2><p>我们首先要有个文件名列表，为了产生文件名列表，我们可以手动用Python输入字符串，例如：</p>
<ul>
<li><code>[&quot;file0&quot;, &quot;file1&quot;]</code></li>
<li><code>[(&quot;file%d&quot; % i) for i in range(2)]</code></li>
<li><code>[(&quot;file%d&quot; % i) for i in range(2)]</code></li>
</ul>
<p>我们也可以用<code>tf.train.match_filenames_once</code>函数来生成文件名列表。</p>
<p>有了文件名列表后，我们需要把它送入<code>tf.train.string_input_producer</code>函数中生成一个先入先出的文件名队列，文件阅读器需要从该队列中读取文件名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">string_input_producer(</div><div class="line">    string_tensor,</div><div class="line">    num_epochs=<span class="keyword">None</span>,</div><div class="line">    shuffle=<span class="keyword">True</span>,</div><div class="line">    seed=<span class="keyword">None</span>,</div><div class="line">    capacity=<span class="number">32</span>,</div><div class="line">    shared_name=<span class="keyword">None</span>,</div><div class="line">    name=<span class="keyword">None</span>,</div><div class="line">    cancel_op=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>一个<code>QueueRunner</code>每次会把每批次的所有文件名送入队列中，可以通过设置<code>string_input_producer</code>函数的<code>shuffle</code>参数来对文件名随机排序，或者通过设置<code>num_epochs</code>来决定对<code>string_tensor</code>里的文件使用多少次，类型为整型，如果想要迭代控制则需要设置了<code>num_epochs</code>参数，同时需要添加<code>tf.local_variables_initializer()</code>进行初始化，如果不初始化会报错。</p>
<p>这个<code>QueueRunner</code>的工作线程独立于文件阅读器的线程， 因此随机排序和将文件名送入到文件名队列这些过程不会阻碍文件阅读器的运行。</p>
<p><br></p>
<h2 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h2><p>根据不同的文件格式， 应该选择对应的文件阅读器， 然后将文件名队列提供给阅读器的<code>read</code>方法。阅读器每次从队列中读取一个文件，它的<code>read</code>方法会输出一个<code>key</code>来表征读入的文件和其中的纪录(对于调试非常有用)，同时得到一个字符串标量， 这个字符串标量可以被一个或多个解析器，或者转换操作将其解码为张量并且构造成为样本。</p>
<p>根据不同的文件类型，有三种不同的文件阅读器：</p>
<ul>
<li><code>tf.TextLineReader</code></li>
<li><code>tf.FixedLengthRecordReader</code></li>
<li><code>tf.TFRecordReader</code></li>
</ul>
<p>它们分别用于单行读取(如CSV文件)、固定长度读取(如CIFAR-10的.bin二进制文件)、TensorFlow标准格式读取。</p>
<p>根据不同的文件阅读器，有三种不同的解析器，它们分别对应上面三种阅读器：</p>
<ul>
<li><code>tf.decode_csv</code></li>
<li><code>tf.decode_raw</code></li>
<li><code>tf.parse_single_example</code>和<code>tf.parse_example</code></li>
</ul>
<p><br></p>
<h3 id="CSV文件"><a href="#CSV文件" class="headerlink" title="CSV文件"></a>CSV文件</h3><p>当我们读入CSV格式的文件时，我们可以使用<code>tf.TextLineReader</code>阅读器和<code>tf.decode_csv</code>解析器。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">filename_queue = tf.train.string_input_producer([<span class="string">"file0.csv"</span>, <span class="string">"file1.csv"</span>]) </div><div class="line"><span class="comment"># 创建一个Filename Queue</span></div><div class="line"><span class="comment"># 该例csv文件中共有5列数据，前四列为features，最后一列为label</span></div><div class="line"></div><div class="line">reader = tf.TextLineReader() <span class="comment"># 文件阅读器</span></div><div class="line">key, value = reader.read(filename_queue) <span class="comment"># 每次执行阅读器都从文件读一行内容</span></div><div class="line"></div><div class="line"><span class="comment"># Default values, in case of empty columns. Also specifies the type of the decoded result.</span></div><div class="line">record_defaults = [[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]] <span class="comment"># 文件数据皆为整数</span></div><div class="line">col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line">features = tf.stack([col1, col2, col3, col4])</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Start populating the filename queue.</span></div><div class="line">  coord = tf.train.Coordinator() <span class="comment">#创建一个协调器，管理线程</span></div><div class="line">  threads = tf.train.start_queue_runners(coord=coord) <span class="comment">#启动QueueRunner, 此时文件名队列已经进队。</span></div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1200</span>):</div><div class="line">    <span class="comment"># Retrieve a single instance:</span></div><div class="line">    example, label = sess.run([features, col5])</div><div class="line"></div><div class="line">  coord.request_stop()</div><div class="line">  coord.join(threads)</div></pre></td></tr></table></figure>
<p>每次<code>read</code>的执行都会从文件中读取一行内容， <code>decode_csv</code> 操作会解析这一行内容并将其转为张量列表。在调用<code>run</code>或者<code>eval</code>去执行<code>read</code>之前， 必须先调用<code>tf.train.start_queue_runners</code>来将文件名填充到队列。否则<code>read</code>操作会被阻塞到文件名队列中有值为止。</p>
<p><code>record_defaults = [[1], [1], [1], [1], [1]]</code>代表了解析的摸版，默认用<code>,</code>隔开，是用于指定矩阵格式以及数据类型的，CSV文件中的矩阵是NXM的，则此处为1XM，例如上例中M=5。[1]表示解析为整型，如果矩阵中有小数，则应为float型，[1]应该变为[1.0]，[‘null’]解析为string类型。</p>
<p><code>col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults = record_defaults)</code>， 矩阵中有几列，这里就要写几个参数，比如5列，就要写到col5,不管你到底用多少。否则报错。</p>
<p><br></p>
<h3 id="固定长度记录"><a href="#固定长度记录" class="headerlink" title="固定长度记录"></a>固定长度记录</h3><p>我们也可以从<strong>二进制文件(.bin)</strong>中读取固定长度的数据，使用的是<code>tf.FixedLengthRecordReader</code>阅读器和<code>tf.decode_raw</code>解析器。<code>decode_raw</code>节点会把<code>string</code>转化为<code>uint8</code>类型的张量。</p>
<p>例如CIFAR-10数据集就采用的固定长度的数据，1字节的标签，后面跟着3072字节的图像数据。使用<code>uint8</code>类型张量的标准操作可以把每个图像的片段截取下来并且按照需要重组。下面有一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">reader = tf.FixedLengthRecordReader(record_bytes = record_bytes)</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line">label = tf.cast(tf.slice(record_bytes, [<span class="number">0</span>], [label_bytes]), tf.int32)</div><div class="line">image_raw = tf.slice(record_bytes, [label_bytes], [image_bytes])</div><div class="line">image_raw = tf.reshape(image_raw, [depth, height, width])</div><div class="line">image = tf.transpose(image_raw, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)) <span class="comment"># 图像形状为[height, width, channels]     </span></div><div class="line">image = tf.cast(image, tf.float32)</div></pre></td></tr></table></figure>
<p>这里介绍上述代码中出现的函数：<code>tf.slice()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">slice(</div><div class="line">    input_,</div><div class="line">    begin,</div><div class="line">    size,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>从一个张量<code>input</code>中提取出长度为<code>size</code>的一部分，提取的起点由<code>begin</code>定义。<code>size</code>是一个向量，它代表着在每个维度提取出的<code>tensor</code>的大小。<code>begin</code>表示提取的位置，它表示的是<code>input</code>的起点偏离值，也就是从每个维度第几个值开始提取。</p>
<p><code>begin</code>从0开始，<code>size</code>从1开始，如果<code>size[i]</code>的值为-1，则第i个维度从<code>begin</code>处到余下的所有值都被提取出来。</p>
<p>例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 'input' is [[[1, 1, 1], [2, 2, 2]],</div><div class="line">#             [[3, 3, 3], [4, 4, 4]],</div><div class="line">#             [[5, 5, 5], [6, 6, 6]]]</div><div class="line">tf.slice(input, [1, 0, 0], [1, 1, 3]) ==&gt; [[[3, 3, 3]]]</div><div class="line">tf.slice(input, [1, 0, 0], [1, 2, 3]) ==&gt; [[[3, 3, 3],</div><div class="line">                                            [4, 4, 4]]]</div><div class="line">tf.slice(input, [1, 0, 0], [2, 1, 3]) ==&gt; [[[3, 3, 3]],</div><div class="line">                                           [[5, 5, 5]]]</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="标准TensorFlow格式"><a href="#标准TensorFlow格式" class="headerlink" title="标准TensorFlow格式"></a>标准TensorFlow格式</h3><p>我们也可以把任意的数据转换为TensorFlow所支持的格式， 这种方法使TensorFlow的数据集更容易与网络应用架构相匹配。这种方法就是使用TFRecords文件，TFRecords文件包含了<code>tf.train.Example</code>的<em>protocol buffer</em>（里面包含了名为 <code>Features</code>的字段）。你可以写一段代码获取你的数据， 将数据填入到<code>Example</code>的<em>protocol buffer</em>，将<em>protocol buffer</em>序列化为一个字符串， 并且通过<code>tf.python_io.TFRecordWriter</code>类写入到TFRecords文件。</p>
<p>从TFRecords文件中读取数据， 可以使用<code>tf.TFRecordReader</code>阅读器以及<code>tf.parse_single_example</code>解析器。<code>parse_single_example</code>操作可以将<code>Example</code><em>protocol buffer</em>解析为张量。 具体可以参考如下例子，把MNIST数据集转化为TFRecords格式：</p>
<ul>
<li><a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/convert_to_records.py" target="_blank" rel="external">tensorflow/examples/how_tos/reading_data/convert_to_records.py</a></li>
</ul>
<p>SparseTensors这种稀疏输入数据类型使用队列来处理不是太好。如果要使用SparseTensors你就必须在批处理<strong>之后</strong>使用<code>tf.parse_example</code>去解析字符串记录 (而不是在批处理<strong>之前</strong>使用<code>tf.parse_single_example</code>) 。</p>
<p><br></p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>我们可以对输入的样本数据进行任意的预处理， 这些预处理不依赖于训练参数， 比如数据归一化， 提取随机数据片，增加噪声或失真等等。具体可以参考如下对CIFAR-10处理的例子：</p>
<ul>
<li><a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10.py" target="_blank" rel="external">tensorflow/models/image/cifar10/cifar10.py</a></li>
</ul>
<p><br></p>
<h2 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h2><p>经过了之前的步骤，在数据读取流程的最后， 我们需要有另一个队列来批量执行输入样本的训练，评估或者推断。根据要不要打乱顺序，我们常用的有两个函数：</p>
<ul>
<li><code>tf.train.batch()</code></li>
<li><code>tf.train.shuffle_batch()</code></li>
</ul>
<p>下面来分别介绍：</p>
<h3 id="tf-train-batch"><a href="#tf-train-batch" class="headerlink" title="tf.train.batch()"></a>tf.train.batch()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">tf.train.batch(</div><div class="line">    tensors,</div><div class="line">    batch_size,</div><div class="line">    num_threads=<span class="number">1</span>,</div><div class="line">    capacity=<span class="number">32</span>,</div><div class="line">    enqueue_many=<span class="keyword">False</span>,</div><div class="line">    shapes=<span class="keyword">None</span>,</div><div class="line">    dynamic_pad=<span class="keyword">False</span>,</div><div class="line">    allow_smaller_final_batch=<span class="keyword">False</span>,</div><div class="line">    shared_name=<span class="keyword">None</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>该函数将会使用一个队列，函数读取一定数量的<code>tensors</code>送入队列，然后每次从中选取<code>batch_size</code>个<code>tensors</code>组成一个新的<code>tensors</code>返回出来。</p>
<p><code>capacity</code>参数决定了队列的长度。</p>
<p><code>num_threads</code>决定了有多少个线程进行入队操作，如果设置的超过一个线程，它们将从不同文件不同位置同时读取，可以更加充分的混合训练样本。</p>
<p>如果<code>enqueue_many</code>参数为<code>False</code>，则输入参数<code>tensors</code>为一个形状为<code>[x, y, z]</code>的张量，输出为一个形状为<code>[batch_size, x, y, z]</code>的张量。如果<code>enqueue_many</code>参数为<code>True</code>，则输入参数<code>tensors</code>为一个形状为<code>[*, x, y, z]</code>的张量，其中所有<code>*</code>的数值相同，输出为一个形状为<code>[batch_size, x, y, z]</code>的张量。</p>
<p>当<code>allow_smaller_final_batch</code>为<code>True</code>时，如果队列中的张量数量不足<code>batch_size</code>，将会返回小于<code>batch_size</code>长度的张量，如果为<code>False</code>，剩下的张量会被丢弃。</p>
<h3 id="tf-train-shuffle-batch"><a href="#tf-train-shuffle-batch" class="headerlink" title="tf.train.shuffle_batch()"></a>tf.train.shuffle_batch()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">tf.train.shuffle_batch(</div><div class="line">    tensors,</div><div class="line">    batch_size,</div><div class="line">    capacity,</div><div class="line">    min_after_dequeue,</div><div class="line">    num_threads=1,</div><div class="line">    seed=None,</div><div class="line">    enqueue_many=False,</div><div class="line">    shapes=None,</div><div class="line">    allow_smaller_final_batch=False,</div><div class="line">    shared_name=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>该函数类似于上面的<code>tf.train.batch()</code>，同样创建一个队列，主要区别是会首先把队列中的张量进行乱序处理，然后再选取其中的<code>batch_size</code>个张量组成一个新的张量返回。但是新增加了几个参数。</p>
<p><code>capacity</code>参数依然为队列的长度，建议<code>capacity</code>的取值如下：</p>
<p><code>min_after_dequeue + (num_threads + a small safety margin) * batch_size</code></p>
<p><code>min_after_dequeue</code>这个参数的意思是队列中，做dequeue（取数据）的操作后，线程要保证队列中至少剩下<code>min_after_dequeue</code>个数据。如果<code>min_after_dequeue</code>设置的过少，则即使<code>shuffle</code>为<code>True</code>，也达不到好的混合效果。</p>
<blockquote>
<p>假设你有一个队列，现在里面有m个数据，你想要每次随机从队列中取n个数据，则代表先混合了m个数据，再从中取走n个。 </p>
<p>当第一次取走n个后，队列就变为m-n个数据；当你下次再想要取n个时，假设队列在此期间入队进来了k个数据，则现在的队列中有(m-n+k)个数据，则此时会从混合的(m-n+k)个数据中随机取走n个。</p>
<p>如果队列填充的速度比较慢，k就比较小，那你取出来的n个数据只是与周围很小的一部分(m-n+k)个数据进行了混合。</p>
<p>因为我们的目的肯定是想尽最大可能的混合数据，因此设置<code>min_after_dequeue</code>，可以保证每次dequeue后都有足够量的数据填充尽队列，保证下次dequeue时可以很充分的混合数据。</p>
<p>但是<code>min_after_dequeue</code>也不能设置的太大，这样会导致队列填充的时间变长，尤其是在最初的装载阶段，会花费比较长的时间。</p>
</blockquote>
<p>其他参数和<code>tf.train.batch()</code>相同。</p>
<p><br></p>
<p>这里我们使用<code>tf.train.shuffle_batch</code>函数来对队列中的样本进行乱序处理。如下的模版：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_my_file_format</span><span class="params">(filename_queue)</span>:</span></div><div class="line">  reader = tf.SomeReader()</div><div class="line">  key, record_string = reader.read(filename_queue)</div><div class="line">  example, label = tf.some_decoder(record_string)</div><div class="line">  processed_example = some_processing(example)</div><div class="line">  <span class="keyword">return</span> processed_example, label</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_pipeline</span><span class="params">(filenames, batch_size, num_epochs=None)</span>:</span></div><div class="line">  filename_queue = tf.train.string_input_producer(</div><div class="line">      filenames, num_epochs=num_epochs, shuffle=<span class="keyword">True</span>)</div><div class="line">  example, label = read_my_file_format(filename_queue)</div><div class="line">  <span class="comment"># min_after_dequeue 越大意味着随机效果越好但是也会占用更多的时间和内存</span></div><div class="line">  <span class="comment"># capacity 必须比 min_after_dequeue 大</span></div><div class="line">  <span class="comment"># 建议capacity的取值如下：</span></div><div class="line">  <span class="comment"># min_after_dequeue + (num_threads + a small safety margin) * batch_size</span></div><div class="line">  min_after_dequeue = <span class="number">10000</span></div><div class="line">  capacity = min_after_dequeue + <span class="number">3</span> * batch_size</div><div class="line">  example_batch, label_batch = tf.train.shuffle_batch(</div><div class="line">      [example, label], batch_size=batch_size, capacity=capacity,</div><div class="line">      min_after_dequeue=min_after_dequeue)</div><div class="line">  <span class="keyword">return</span> example_batch, label_batch</div></pre></td></tr></table></figure>
<p>一个具体的例子如下，该例采用了CIFAR-10数据集，采用了固定长度读取的<code>tf.FixedLengthRecordReader</code>阅读器和<code>tf.decode_raw</code>解析器，同时进行了数据预处理操作中的标准化操作，最后使用<code>tf.train.shuffle_batch</code>函数批量执行数据的乱序处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">cifar10_data</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filename_queue)</span>:</span></div><div class="line">        self.height = <span class="number">32</span></div><div class="line">        self.width = <span class="number">32</span></div><div class="line">        self.depth = <span class="number">3</span></div><div class="line">        self.label_bytes = <span class="number">1</span></div><div class="line">        self.image_bytes = self.height * self.width * self.depth</div><div class="line">        self.record_bytes = self.label_bytes + self.image_bytes</div><div class="line">        self.label, self.image = self.read_cifar10(filename_queue)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_cifar10</span><span class="params">(self, filename_queue)</span>:</span></div><div class="line">        reader = tf.FixedLengthRecordReader(record_bytes = self.record_bytes)</div><div class="line">        key, value = reader.read(filename_queue)</div><div class="line">        record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line">        label = tf.cast(tf.slice(record_bytes, [<span class="number">0</span>], [self.label_bytes]), tf.int32)</div><div class="line">        image_raw = tf.slice(record_bytes, [self.label_bytes], [self.image_bytes])</div><div class="line">        image_raw = tf.reshape(image_raw, [self.depth, self.height, self.width])</div><div class="line">        image = tf.transpose(image_raw, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))        </div><div class="line">        image = tf.cast(image, tf.float32)</div><div class="line">        <span class="keyword">return</span> label, image</div><div class="line">    </div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(data_dir, batch_size, train = True, name = <span class="string">'input'</span>)</span>:</span></div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">        <span class="keyword">if</span> train:    </div><div class="line">            filenames = [os.path.join(data_dir,<span class="string">'data_batch_%d.bin'</span> % ii) </div><div class="line">                        <span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>)]</div><div class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line">                    </div><div class="line">            filename_queue = tf.train.string_input_producer(filenames)</div><div class="line">            read_input = cifar10_data(filename_queue)</div><div class="line">            images = read_input.image</div><div class="line">            images = tf.image.per_image_standardization(images)</div><div class="line">            labels = read_input.label</div><div class="line">            image, label = tf.train.shuffle_batch(</div><div class="line">                                    [images,labels], batch_size = batch_size, </div><div class="line">                                    min_after_dequeue = <span class="number">20000</span>, capacity = <span class="number">20192</span>)</div><div class="line">        </div><div class="line">            <span class="keyword">return</span> image, tf.reshape(label, [batch_size])</div><div class="line">            </div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            filenames = [os.path.join(data_dir,<span class="string">'test_batch.bin'</span>)]</div><div class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line">                    </div><div class="line">            filename_queue = tf.train.string_input_producer(filenames)</div><div class="line">            read_input = cifar10_data(filename_queue)</div><div class="line">            images = read_input.image</div><div class="line">            images = tf.image.per_image_standardization(images)</div><div class="line">            labels = read_input.label</div><div class="line">            image, label = tf.train.shuffle_batch(</div><div class="line">                                    [images,labels], batch_size = batch_size, </div><div class="line">                                    min_after_dequeue = <span class="number">20000</span>, capacity = <span class="number">20192</span>)</div><div class="line">        </div><div class="line">            <span class="keyword">return</span> image, tf.reshape(label, [batch_size])</div></pre></td></tr></table></figure>
<p>这里介绍下函数<code>tf.image.per_image_standardization(image)</code>，该函数对图像进行线性变换使它具有零均值和单位方差，即规范化。其中参数<code>image</code>是一个3-D的张量，形状为<code>[height, width, channels]</code>。</p>
<p><br></p>
<h2 id="多个样本和多个阅读器"><a href="#多个样本和多个阅读器" class="headerlink" title="多个样本和多个阅读器"></a>多个样本和多个阅读器</h2><p>下面讲分别展示三个不同Reader数目和不同样本数的代码示例：</p>
<h3 id="文件准备"><a href="#文件准备" class="headerlink" title="文件准备"></a>文件准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Alpha1,A1\nAlpha2,A2\nAlpha3,A3"</span> &gt; A.csv</div><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Bee1,B1\nBee2,B2\nBee3,B3"</span> &gt; B.csv</div><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Sea1,C1\nSea2,C2\nSea3,C3"</span> &gt; C.csv</div><div class="line">$ cat A.csv</div><div class="line">Alpha1,A1</div><div class="line">Alpha2,A2</div><div class="line">Alpha3,A3</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="单个Reader，单个样本"><a href="#单个Reader，单个样本" class="headerlink" title="单个Reader，单个样本"></a>单个Reader，单个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 生成一个先入先出队列和一个QueueRunner</span></div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># 定义Reader</span></div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line"><span class="comment"># 定义Decoder</span></div><div class="line">example, label = tf.decode_csv(value, record_defaults=[[<span class="string">'null'</span>], [<span class="string">'null'</span>]])</div><div class="line"><span class="comment"># 运行Graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()  <span class="comment">#创建一个协调器，管理线程</span></div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)  <span class="comment">#启动QueueRunner, 此时文件名队列已经进队。</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example.eval()   <span class="comment">#取样本的时候，一个Reader先从文件名队列中取出文件名，读出数据，Decoder解析后进入样本队列。</span></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line"><span class="comment"># outpt</span></div><div class="line">Alpha1</div><div class="line">Alpha2</div><div class="line">Alpha3</div><div class="line">Bee1</div><div class="line">Bee2</div><div class="line">Bee3</div><div class="line">Sea1</div><div class="line">Sea2</div><div class="line">Sea3</div><div class="line">Alpha1</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="单个Reader，多个样本"><a href="#单个Reader，多个样本" class="headerlink" title="单个Reader，多个样本"></a>单个Reader，多个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">example, label = tf.decode_csv(value, record_defaults=[[<span class="string">'null'</span>], [<span class="string">'null'</span>]])</div><div class="line"><span class="comment"># 使用tf.train.batch()会多加了一个样本队列和一个QueueRunner。Decoder解后数据会进入这个队列，再批量出队。</span></div><div class="line"><span class="comment"># 虽然这里只有一个Reader，但可以设置多线程，通过在tf.train.batch()中添加“num_threads="，相应增加线程数会提高读取速度，但并不是线程越多越好。</span></div><div class="line">example_batch, label_batch = tf.train.batch(</div><div class="line">      [example, label], batch_size=<span class="number">5</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example_batch.eval()</div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line"><span class="comment"># output</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div><div class="line"><span class="comment"># ['Bee3' 'Sea1' 'Sea2' 'Sea3' 'Alpha1']</span></div><div class="line"><span class="comment"># ['Alpha2' 'Alpha3' 'Bee1' 'Bee2' 'Bee3']</span></div><div class="line"><span class="comment"># ['Sea1' 'Sea2' 'Sea3' 'Alpha1' 'Alpha2']</span></div><div class="line"><span class="comment"># ['Alpha3' 'Bee1' 'Bee2' 'Bee3' 'Sea1']</span></div><div class="line"><span class="comment"># ['Sea2' 'Sea3' 'Alpha1' 'Alpha2' 'Alpha3']</span></div><div class="line"><span class="comment"># ['Bee1' 'Bee2' 'Bee3' 'Sea1' 'Sea2']</span></div><div class="line"><span class="comment"># ['Sea3' 'Alpha1' 'Alpha2' 'Alpha3' 'Bee1']</span></div><div class="line"><span class="comment"># ['Bee2' 'Bee3' 'Sea1' 'Sea2' 'Sea3']</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="多个Reader，多个样本"><a href="#多个Reader，多个样本" class="headerlink" title="多个Reader，多个样本"></a>多个Reader，多个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">record_defaults = [[<span class="string">'null'</span>], [<span class="string">'null'</span>]]</div><div class="line">example_list = [tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line">                  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]  <span class="comment"># Reader设置为2</span></div><div class="line"><span class="comment"># 使用tf.train.batch_join()，可以使用多个reader，并行读取数据。每个Reader使用一个线程。</span></div><div class="line">example_batch, label_batch = tf.train.batch_join(</div><div class="line">      example_list, batch_size=<span class="number">5</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example_batch.eval()</div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line">    </div><div class="line"><span class="comment"># output</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div><div class="line"><span class="comment"># ['Bee3' 'Sea1' 'Sea2' 'Sea3' 'Alpha1']</span></div><div class="line"><span class="comment"># ['Alpha2' 'Alpha3' 'Bee1' 'Bee2' 'Bee3']</span></div><div class="line"><span class="comment"># ['Sea1' 'Sea2' 'Sea3' 'Alpha1' 'Alpha2']</span></div><div class="line"><span class="comment"># ['Alpha3' 'Bee1' 'Bee2' 'Bee3' 'Sea1']</span></div><div class="line"><span class="comment"># ['Sea2' 'Sea3' 'Alpha1' 'Alpha2' 'Alpha3']</span></div><div class="line"><span class="comment"># ['Bee1' 'Bee2' 'Bee3' 'Sea1' 'Sea2']</span></div><div class="line"><span class="comment"># ['Sea3' 'Alpha1' 'Alpha2' 'Alpha3' 'Bee1']</span></div><div class="line"><span class="comment"># ['Bee2' 'Bee3' 'Sea1' 'Sea2' 'Sea3']</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p><code>tf.train.batch</code>与<code>tf.train.shuffle_batch</code>函数是单个Reader读取，但是可以多线程，通过设置<code>num_threads</code>参数来设置多线程。<code>tf.train.batch_join</code>与<code>tf.train.shuffle_batch_join</code>可设置多Reader读取，每个Reader使用一个线程。至于两种方法的效率，单Reader时，2个线程就达到了速度的极限。多Reader时，2个Reader就达到了极限。所以并不是线程越多越快，甚至更多的线程反而会使效率下降。</p>
<p>上述两种方法，前者相比于后者的好处是：</p>
<ul>
<li>避免了两个不同的线程从同一个文件中读取同一个样本。</li>
<li>避免了过多的磁盘搜索操作。</li>
</ul>
<p>那么具体需要多少个读取线程呢？ 函数<code>tf.train.shuffle_batch*</code>为<code>graph</code>提供了获取文件名队列中的元素个数之和的方法。 如果你有足够多的读取线程， 文件名队列中的元素个数之和应该一直是一个略高于0的数。具体可以参考TensorBoard的教程。</p>
<p><br></p>
<h2 id="创建线程并使用QueueRunner对象来获取"><a href="#创建线程并使用QueueRunner对象来获取" class="headerlink" title="创建线程并使用QueueRunner对象来获取"></a>创建线程并使用QueueRunner对象来获取</h2><p>我们要添加<code>tf.train.QueueRunner</code>对象到数据流图中，在运行任何训练步骤之前，需要调用<code>tf.train.start_queue_runners</code>函数，否则数据流图将一直挂起，该函数将会启动输入管道的线程，填充样本到队列中，以便出队操作可以从队列中拿到样本。这种情况下最好配合使用一个<code>tf.train.Coordinator</code>，这样可以在发生错误的情况下正确地关闭这些线程。如果我们对训练迭代数做了限制，那么需要使用一个训练迭代数计数器，并且需要初始化它。推荐的代码模板如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create the graph, etc.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Create a session for running operations in the Graph.</span></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line"><span class="comment"># Initialize the variables (like the epoch counter).</span></div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"><span class="comment"># Start input enqueue threads.</span></div><div class="line">coord = tf.train.Coordinator()</div><div class="line">threads = tf.train.start_queue_runners(sess=sess, coord=coord)</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">        <span class="comment"># Run training steps or whatever</span></div><div class="line">        sess.run(train_op)</div><div class="line"></div><div class="line"><span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">    print(<span class="string">'Done training -- epoch limit reached'</span>)</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    <span class="comment"># When done, ask the threads to stop.</span></div><div class="line">    coord.request_stop()</div><div class="line"></div><div class="line"><span class="comment"># Wait for threads to finish.</span></div><div class="line">coord.join(threads)</div><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p><br></p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170704205652_Agg5Zf_AnimatedFileQueues.gif" alt="Animated File Queues" width="900"></center>

<p>如上图所示，每个<code>QueueRunner</code>负责一个阶段，处理那些需要在线程中运行的入队操作的列表。一旦数据流图构造成功，<code>tf.train.start_queue_runners</code>函数就会要求数据流图中每个<code>QueueRunner</code>去开始它的线程运行入队操作。</p>
<p>如果一切顺利的话，我们可以执行训练步骤，同时队列也会被后台线程来填充。如果设置了最大训练迭代数，在某些时候，样本出队的操作可能会得到一个<code>tf.OutOfRangeError</code>的错误。这其实是TensorFlow的“文件结束”（EOF）——这就意味着已经达到了最大训练迭代数，已经没有更多可用的样本了。</p>
<p>最后一个因素是<code>Coordinator</code>。这是负责在收到任何关闭信号的时候，让所有的线程都知道。最常见的情况是在发生异常时，比如说其中一个线程在运行某些操作时出现错误（或一个普通的Python异常）。</p>
<p><br></p>
<h3 id="疑问：在达到最大训练迭代数的时候如何关闭线程"><a href="#疑问：在达到最大训练迭代数的时候如何关闭线程" class="headerlink" title="疑问：在达到最大训练迭代数的时候如何关闭线程?"></a>疑问：在达到最大训练迭代数的时候如何关闭线程?</h3><p>想象一下，我们有一个模型并且设置了最大训练迭代数。这意味着，生成文件的那个线程只会在产生<code>OutOfRange</code>错误之前运行。<code>QueueRunner</code>会捕获该错误，并且关闭文件名的队列，最后退出线程。关闭队列做了两件事情：</p>
<ul>
<li>如果试着对文件名队列执行入队操作将发生错误。</li>
<li>当前或将来的出队操作要么成功（如果队列中还有足够的元素）或立即失败（发生<code>OutOfRange</code>错误）。它们不会等待更多的元素被添加到队列中，因为上面的一点已经保证了这种情况不会发生。</li>
</ul>
<p>关键是，当在文件名队列被关闭时候，有可能还有许多文件名在该队列中，这样下一阶段的流水线（包括reader和其它预处理）还可以继续运行一段时间。 一旦文件名队列空了之后，如果后面的流水线还要尝试从文件名队列中取出一个文件名，这将会触发<code>OutOfRange</code>错误。在这种情况下，即使你可能有一个<code>QueueRunner</code>关联着多个线程，如果该出错线程不是<code>QueueRunner</code>中最后的那个线程，那么<code>OutOfRange</code>错误只会使得这一个线程退出。而其他那些正处理自己的最后一个文件的线程继续运行，直至他们完成为止。（但如果你使用的是<code>tf.train.Coordinator</code>来管理所有的线程，那么其他类型的错误将导致所有线程停止）。一旦所有的reader线程触发<code>OutOfRange</code>错误，样本队列才会被关闭。</p>
<p>同样，样本队列中会有一些已经入队的元素，所以样本训练将一直持续直到样本队列中再没有样本为止。如果样本队列是一个<code>RandomShuffleQueue</code>，因为你使用了<code>shuffle_batch</code> 或者 <code>shuffle_batch_join</code>，所以通常不会出现以往那种队列中的元素会比<code>min_after_dequeue</code> 定义的更少的情况。 然而，一旦该队列被关闭，<code>min_after_dequeue</code>设置的限定值将失效，最终队列将为空。在这一点来说，当实际训练线程尝试从样本队列中取出数据时，将会触发<code>OutOfRange</code>错误，然后训练线程会退出。一旦所有的训练线程完成，<code>tf.train.Coordinator.join</code>会返回，你就可以正常退出了。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/reading_data" target="_blank" rel="external">Reading data  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/extend/new_data_formats" target="_blank" rel="external">Custom Data Readers  |  TensorFlow</a></li>
<li><a href="http://www.cnblogs.com/Charles-Wan/p/6197019.html" target="_blank" rel="external">TF Boys (TensorFlow Boys ) 养成记（二）： TensorFlow 数据读取 - Charles-Wan - 博客园</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer" target="_blank" rel="external">tf.train.string_input_producer  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/batch" target="_blank" rel="external">tf.train.batch  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch" target="_blank" rel="external">tf.train.shuffle_batch  |  TensorFlow</a></li>
<li><a href="http://honggang.io" target="_blank" rel="external">honggang.io</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/reading_data.html#AUTOGENERATED-reading-from-files" target="_blank" rel="external">数据读取 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（八）：线程和队列]]></title>
      <url>/2017/07/03/TensorFlow_8/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文介绍了TensorFlow的线程和队列。在使用TensorFlow进行异步计算时，队列是一种强大的机制。正如TensorFlow中的其他组件一样，队列就是TensorFlow图中的节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，其他节点可以把新元素插入到队列后端(rear)，也可以把队列前端(front)的元素删除。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>为了感受一下队列，先来看一个简单的例子。我们先创建一个“先入先出”的队列（FIFOQueue），并将其内部所有元素初始化为零。然后，我们构建一个TensorFlow图，它从队列前端取走一个元素，加上1之后，放回队列的后端。慢慢地，队列的元素的值就会增加。</p>
<center><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/IncremeterFifoQueue.gif" alt="Incremeter Fifo Queue" width="800"></center>

<p><code>Enqueue</code>、 <code>EnqueueMany</code>和<code>Dequeue</code>都是特殊的节点，在Python API中，它们都是队列对象的方法（例如<code>q.enqueue(...)</code>）。</p>
<p>下面我们深入了解下细节。</p>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>诸如<code>FIFOQueue</code>和<code>RandomShuffleQueue</code>这样的队列，在TensorFlow的<code>tensor</code>异步计算时非常重要。</p>
<p>例如，一个典型的输入结构：使用一个<code>RandomShuffleQueue</code>来作为模型训练的输入：</p>
<ul>
<li>多个线程准备训练样本，并且把这些样本推入队列。</li>
<li>一个训练线程执行一个训练操作，此操作会从队列中移除最小批次的样本（mini-batches)。</li>
</ul>
<p>TensorFlow的<code>Session</code>对象是可以支持多线程的，因此多个线程可以很方便地使用同一个会话（Session）并且并行地执行操作。然而，在Python程序实现这样的并行运算却并不容易。所有线程都必须能被同步终止，异常必须能被正确捕获并报告，会话终止的时候， 队列必须能被正确地关闭。</p>
<p>TensorFlow提供了两个类来帮助多线程的实现：<code>tf.Coordinator</code>和 <code>tf.QueueRunner</code>，通常来说这两个类必须被一起使用。<code>Coordinator</code>类用来同时停止多个工作线程并且向那个在等待所有工作线程终止的程序报告异常。<code>QueueRunner</code>类用来协调多个工作线程并将多个张量推入同一个队列中。</p>
<p><br></p>
<h1 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h1><p><code>Coordinator</code>类用来帮助多个线程协同工作，多个线程同步终止。 其主要方法有：</p>
<ul>
<li><code>should_stop()</code>：如果线程应该停止则返回True。</li>
<li><code>request_stop(&lt;exception&gt;)</code>：请求该线程停止。</li>
<li><code>join(&lt;list of threads&gt;)</code>：等待被指定的线程终止。</li>
</ul>
<p>首先创建一个<code>Coordinator</code>对象，然后建立一些使用<code>Coordinator</code>对象的线程。这些线程通常一直循环运行，每次循环前首先判断<code>should_stop()</code>是否返回<code>True</code>，如果是的话就停止。 任何线程都可以决定什么时候应该停止，它只需要调用<code>request_stop()</code>，同时其他线程的<code>should_stop()</code>将会返回<code>True</code>，然后就都停下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Thread body: loop until the coordinator indicates a stop was requested.</span></div><div class="line"><span class="comment"># If some condition becomes true, ask the coordinator to stop.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">MyLoop</span><span class="params">(coord)</span>:</span></div><div class="line">  <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">    ...do something...</div><div class="line">    <span class="keyword">if</span> ...some condition...:</div><div class="line">      coord.request_stop()</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="comment"># ...</span></div><div class="line">  coord = tf.train.Coordinator()</div><div class="line">  <span class="comment"># Start a number of threads, passing the coordinator to each of them.</span></div><div class="line">  ...start thread <span class="number">1</span> MyLoop(coord)</div><div class="line">  ...start thread N MyLoop(coord)</div><div class="line">  <span class="comment"># Wait for all the threads to terminate.</span></div><div class="line">  coord.join(threads)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">  coord.request_stop()</div><div class="line">coord.join(threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="QueueRunner"><a href="#QueueRunner" class="headerlink" title="QueueRunner"></a>QueueRunner</h1><p><code>QueueRunner</code>类会创建一组线程， 这些线程可以重复的执行Enquene操作， 他们使用同一个<code>Coordinator</code>来处理线程同步终止。此外，一个<code>QueueRunner</code>会运行一个用于异常处理的<em>closer thread</em>，当<code>Coordinator</code>收到异常报告时，这个<em>closer thread</em>会自动关闭队列。</p>
<p>我们可以使用一个一个<code>QueueRunner</code>来实现上述结构。 首先建立一个TensorFlow图表，这个图表使用队列来输入样本，处理样本并将样本推入队列中，用training操作来移除队列中的样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">example = ...ops to create one example...</div><div class="line"><span class="comment"># Create a queue, and an op that enqueues examples one at a time in the queue.</span></div><div class="line">queue = tf.RandomShuffleQueue(...)</div><div class="line">enqueue_op = queue.enqueue(example)</div><div class="line"><span class="comment"># Create a training graph that starts by dequeuing a batch of examples.</span></div><div class="line">inputs = queue.dequeue_many(batch_size)</div><div class="line">train_op = ...use <span class="string">'inputs'</span> to build the training part of the graph...</div></pre></td></tr></table></figure>
<p>在Python的训练程序中，创建一个<code>QueueRunner</code>来运行几个线程， 这几个线程处理样本，并且将样本推入队列。创建一个<code>Coordinator</code>，让queue runner使用<code>Coordinator</code>来开始它的线程，同时创建一个训练的循环， 并且使用<code>Coordinator</code>来控制<code>QueueRunner</code>这些线程的终止。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a queue runner that will run 4 threads in parallel to enqueue</span></div><div class="line"><span class="comment"># examples.</span></div><div class="line">qr = tf.train.QueueRunner(queue, [enqueue_op] * <span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph.</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># Create a coordinator, launch the queue runner threads.</span></div><div class="line">coord = tf.train.Coordinator()</div><div class="line">enqueue_threads = qr.create_threads(sess, coord=coord, start=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Run the training loop, controlling termination with the coordinator.</span></div><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</div><div class="line">    <span class="keyword">if</span> coord.should_stop():</div><div class="line">        <span class="keyword">break</span></div><div class="line">    sess.run(train_op)</div><div class="line"><span class="comment"># When done, ask the threads to stop.</span></div><div class="line">coord.request_stop()</div><div class="line"><span class="comment"># And wait for them to actually do it.</span></div><div class="line">coord.join(enqueue_threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h1><p>通过queue runners启动的线程不仅仅推送样本到队列。它们还捕捉和处理由队列产生的异常，包括<code>OutOfRangeError</code>异常，这个异常是用于报告队列被关闭。 使用<code>Coordinator</code>训练时在主循环中必须同时捕捉和报告异常。 下面是对上面训练循环的改进版本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</div><div class="line">        <span class="keyword">if</span> coord.should_stop():</div><div class="line">            <span class="keyword">break</span></div><div class="line">        sess.run(train_op)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">    <span class="comment"># Report exceptions to the coordinator.</span></div><div class="line">    coord.request_stop(e)</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    <span class="comment"># Terminate as usual. It is safe to call `coord.request_stop()` twice.</span></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/threading_and_queues" target="_blank" rel="external">Threading and Queues  |  TensorFlow</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html" target="_blank" rel="external">线程和队列 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator" target="_blank" rel="external">tf.train.Coordinator  |  TensorFlow</a> </li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（七）：Exponential_decay]]></title>
      <url>/2017/07/01/TensorFlow_7/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>在神经网络的训练过程中，<strong>学习率(learning rate)</strong>控制着参数的更新速度，<code>tf.train</code>类下面的五种不同的学习速率的衰减方法。</p>
<ul>
<li><code>tf.train.exponential_decay</code></li>
<li><code>tf.train.inverse_time_decay</code></li>
<li><code>tf.train.natural_exp_decay</code></li>
<li><code>tf.train.piecewise_constant</code></li>
<li><code>tf.train.polynomial_decay</code></li>
</ul>
<p>本文只对<code>exponential_decay</code>做整理。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="exponential-decay"><a href="#exponential-decay" class="headerlink" title="exponential_decay"></a>exponential_decay</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tf.train.exponential_decay(</div><div class="line">    learning_rate,</div><div class="line">    global_step,</div><div class="line">    decay_steps,</div><div class="line">    decay_rate,</div><div class="line">    staircase=<span class="keyword">False</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<ul>
<li><code>learning_rate</code>：初始学习率</li>
<li><code>global_step</code>：当前迭代次数</li>
<li><code>decay_steps</code>：衰减迭代次数（在迭代到该次数时学习率衰减为<code>earning_rate * decay_rate</code>）</li>
<li><code>decay_rate</code>：学习率衰减率，通常介于0-1之间。</li>
</ul>
<p>学习率会按照以下公式变化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</div></pre></td></tr></table></figure>
<p>直观解释：假设给定初始学习率<code>learning_rate</code>为0.1，学习率衰减率为0.1，<code>decay_steps</code>为10000，则随着迭代次数从1到10000，当前的学习率<code>decayed_learning_rate</code>慢慢的从0.1降低为<code>0.1*0.1=0.01</code>，当迭代次数到20000，当前的学习率慢慢的从0.01降低为<code>0.1*0.1^2=0.001</code>，以此类推。也就是说每10000次迭代，学习率衰减为前10000次的十分之一，该衰减是连续的，这是在<code>staircase</code>为<code>False</code>的情况下。</p>
<p>如果<code>staircase</code>为<code>True</code>，则<code>global_step / decay_steps</code>始终取整数，也就是说衰减是突变的，每<code>decay_steps</code>次变化一次，变化曲线是阶梯状。</p>
<p>例子：每100000次迭代衰减一次，学习率衰减率为0.96。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### ...</span></div><div class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</div><div class="line">starter_learning_rate = <span class="number">0.1</span></div><div class="line">learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, <span class="number">100000</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Passing global_step to minimize() will increment it at each step.</span></div><div class="line">learning_step = (</div><div class="line">    tf.train.GradientDescentOptimizer(learning_rate)</div><div class="line">    .minimize(...my loss..., global_step=global_step)</div><div class="line">)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong>这里的<code>global_step</code>变量的<code>trainable</code>要设置为<code>False</code>，它代表着当前的迭代次数，我们不能对它进行训练，系统会自动更新它的值，初始化为0，从1开始。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay" target="_blank" rel="external">tf.train.exponential_decay  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（六）：Moving Average]]></title>
      <url>/2017/07/01/TensorFlow_6/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了TensorFlow训练模型过程中对参数的<strong>滑动平均(moving average)</strong>的计算，在测试数据上评估模型性能时用这些平均值总会提升预测结果表现，用到的类主要为<code>tf.train.ExponentialMovingAverage</code>。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="MovingAverage"><a href="#MovingAverage" class="headerlink" title="MovingAverage"></a>MovingAverage</h1><p>常规的滑动平均的计算方法十分简单，对于一个给定的数列，首先设定一个固定的值k，然后分别计算第1项到第k项，第2项到第k+1项，第3项到第k+2项的平均值，依次类推。</p>
<p>以<code>1、2、3、4、5</code>共5个数为例，window为3，计算过程为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(1+2+3)/3=2</div><div class="line">(2+3+4)/3=3</div><div class="line">(3+4+5)/3=4</div></pre></td></tr></table></figure>
<p>下图很好的反映了原始数据和滑动平均之间的关系，其中绿线为原始数据，红线为MovingAverage：</p>
<ul>
<li>当window为3:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_TyB8hT_MovingAverage1.jpeg" alt="window=3" width="700"></p>
<ul>
<li>当window为10:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_UWRLDA_MovingAverage2.jpeg" alt="window=10" width="700"></p>
<p>可以发现当我们使用滑动平均时，会十分有效的提升模型在测试数据上的<strong>健壮性(robustness)</strong>。</p>
<p><br></p>
<h1 id="ExponentialMovingAverage"><a href="#ExponentialMovingAverage" class="headerlink" title="ExponentialMovingAverage"></a>ExponentialMovingAverage</h1><p>在TensorFlow中，我们计算的是<strong>指数滑动平均(ExponentialMovingAverage)</strong>，我们通过使用一个<strong>指数衰减(exponential decay)</strong>来维持着变量的滑动平均。</p>
<p>当我们训练一个模型时，计算训练参数的滑动平均经常是十分有利的，当我们用这些平均后的参数来评估模型时有时会得到比使用常规的训练参数好很多的结果。</p>
<p>我们用一个<code>apply()</code>函数返回一个<code>ops</code>来添加变量的一个副本同时得到原变量的滑动平均，它在我们训练模型的时候使用。该<code>ops</code>得到原变量的滑动平均始终是在每一次训练迭代结束后。</p>
<p><code>average()</code>和<code>average_name()</code>函数返回影子变量和它们的名字，它们在我们对测试数据进行模型评估时使用，它们用参数的滑动平均值来代替最终的训练值来对模型进行评估。它们也可以在我们从一个<code>checkpoint file</code>继续开始训练模型时使用。</p>
<p>滑动平均值用一个指数衰减来计算，当我们创建<code>ExponentialMovingAverage</code>对象时会把该<code>decay</code>值输入进去。影子变量的初始化值和原始变量初始化值相同。每个影子变量计算滑动平均值的公式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">shadow_variable = decay * shadow_variable + (1 - decay) * variable</div></pre></td></tr></table></figure>
<p>通常我们定义<code>decay</code>时会让它尽可能接近于1.0，一般来说我们会让它为0.999、0.9999等。</p>
<p>如下是我们训练一个模型时的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create variables.</span></div><div class="line">var0 = tf.Variable(...)</div><div class="line">var1 = tf.Variable(...)</div><div class="line"><span class="comment"># ... use the variables to build a training model...</span></div><div class="line">...</div><div class="line"><span class="comment"># Create an op that applies the optimizer.  This is what we usually</span></div><div class="line"><span class="comment"># would use as a training op.</span></div><div class="line">opt_op = opt.minimize(my_loss, [var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an ExponentialMovingAverage object</span></div><div class="line">ema = tf.train.ExponentialMovingAverage(decay=<span class="number">0.9999</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create the shadow variables, and add ops to maintain moving averages</span></div><div class="line"><span class="comment"># of var0 and var1.</span></div><div class="line">maintain_averages_op = ema.apply([var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an op that will update the moving averages after each training</span></div><div class="line"><span class="comment"># step.  This is what we will use in place of the usual training op.</span></div><div class="line"><span class="keyword">with</span> tf.control_dependencies([opt_op]):</div><div class="line">    training_op = tf.group(maintain_averages_op)</div><div class="line"></div><div class="line"><span class="comment"># ...train the model by running training_op...</span></div></pre></td></tr></table></figure>
<p>当我们使用滑动平均来预测时，有两种用法：</p>
<ol>
<li>用影子变量代替原始变量，使用<code>average()</code>函数来返回给定变量的影子变量。</li>
<li>通过使用影子变量的<code>name</code>来载入<code>checkpoint files</code>，我们在这里使用<code>average_name()</code>函数。对于这种用法有如下例子：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a Saver that loads variables from their saved shadow values.</span></div><div class="line">shadow_var0_name = ema.average_name(var0)</div><div class="line">shadow_var1_name = ema.average_name(var1)</div><div class="line">saver = tf.train.Saver(&#123;shadow_var0_name: var0, shadow_var1_name: var1&#125;)</div><div class="line">saver.restore(...checkpoint filename...)</div><div class="line"><span class="comment"># var0 and var1 now hold the moving average values</span></div></pre></td></tr></table></figure>
<p>详情可以查看<code>tf.train.Saver</code>，下面介绍<code>ExponentialMovingAverage</code>的相关函数。</p>
<p><br></p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul>
<li><code>__init__</code></li>
<li><code>apply</code></li>
<li><code>average</code></li>
<li><code>average_name</code></li>
<li><code>variables_to_restore</code></li>
</ul>
<p><br></p>
<h2 id="init"><a href="#init" class="headerlink" title="__init__"></a>__init__</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">__init__(</div><div class="line">    decay,</div><div class="line">    num_updates=<span class="keyword">None</span>,</div><div class="line">    zero_debias=<span class="keyword">False</span>,</div><div class="line">    name=<span class="string">'ExponentialMovingAverage'</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>创建一个<code>ExponentialMovingAverage</code>对象。</p>
<ul>
<li><code>decay</code>一般取值接近于1.0。</li>
<li><code>num_updates</code>允许<code>dacay</code>值动态的变化，在训练开端<code>dacay</code>速率较低，这使得滑动均值更快，如果有值的话，实际<code>decay</code>速率公式为：<code>min(decay, (1 + num_updates) / (10 + num_updates))</code></li>
<li><code>name</code>将会给<code>apply()</code>中的<code>ops</code>添加一个额外的前置名字。</li>
</ul>
<p><br></p>
<h2 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apply(var_list=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>该函数维护变量的滑动平均，返回一个<code>op</code>来更新所有影子变量。<code>var_list</code>必须是一个变量或者<code>Tensor</code>对象的列表，这个函数创造<code>var_list</code>中所有变量的副本，对于变量副本，初始化值和原变量初始化值相同。变量类型必须是<code>float</code>相关的类型。</p>
<p><code>apply()</code>函数对于不同的<code>var_list</code>可以被调用多次。</p>
<p><br></p>
<h2 id="average"><a href="#average" class="headerlink" title="average"></a>average</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>的滑动平均影子变量，返回类型为<code>Variable</code>。前提是该<code>var</code>使用了<code>apply()</code>函数来维护。</p>
<p><br></p>
<h2 id="average-name"><a href="#average-name" class="headerlink" title="average_name"></a>average_name</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average_name(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>变量的滑动平均影子变量的<code>name</code>。该函数一个典型的应用是在训练过程中计算原始变量的滑动平均，并且在测试时根据影子变量的<code>name</code>恢复出原始变量。</p>
<p>为了恢复原始变量，我们必须知道影子变量的<code>name</code>，影子变量的<code>name</code>和原始变量可以在训练阶段利用<code>Saver()</code>对象来保存，操作为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.train.Saver(&#123;ema.average_name(var): var&#125;)</div></pre></td></tr></table></figure>
<p><code>average_name()</code>函数在<code>apply()</code>函数调用之前或之后都可以使用。</p>
<p><br></p>
<h2 id="variables-to-restore"><a href="#variables-to-restore" class="headerlink" title="variables_to_restore"></a>variables_to_restore</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">variables_to_restore(moving_avg_variables=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>返回一个从<code>restore_name</code>到<code>Variables</code>的映射，如果一个变量有滑动平均值，那么就用该滑动平均影子变量的<code>name</code>来作为<code>restore name</code>，否则，就用原始变量的<code>name</code>。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">variables_to_restore = ema.variables_to_restore()</div><div class="line">saver = tf.train.Saver(variables_to_restore)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage" target="_blank" rel="external">tf.train.ExponentialMovingAverage  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/u014365862/article/details/54380313" target="_blank" rel="external">MovingAverage-滑动平均 - 小鹏的专栏 - 博客频道 - CSDN.NET</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（五）：常用函数和模型的保存与恢复]]></title>
      <url>/2017/06/30/TensorFlow_5/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文学习了TensorFlow的一些常用的基础函数，主要为以下几种：</p>
<ul>
<li>tf.group</li>
<li>tf.Graph.control_dependencies</li>
<li>tf.train.Saver</li>
</ul>
<p>同时介绍了训练过程中的模型保存和恢复方法，主要用到<code>tf.train.Saver</code>类。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a>tf.group</h1><p><code>tf.group(inputs)</code>创建一个<code>op</code>把多个<code>ops</code>给组合起来，该<code>op</code>无输出。当该<code>op</code>结束操作，其中的所有<code>ops</code>都会结束。</p>
<p>其中<code>inputs</code>为空或者很多<code>tensors</code>。</p>
<p><br></p>
<h1 id="tf-Graph-control-dependencies"><a href="#tf-Graph-control-dependencies" class="headerlink" title="tf.Graph.control_dependencies"></a>tf.Graph.control_dependencies</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">control_dependencies(control_inputs)</div></pre></td></tr></table></figure>
<p>参数<code>control_inputs</code>是一个包含<code>op</code>或者<code>tensor</code>的列表，该列表内的对象必须在控制区域内的<code>ops</code>之前执行。可以为<code>None</code>来清空控制依赖。</p>
<p>通常用<code>with</code>操作来定义一个区域，在该区域下所有的<code>ops</code>都要在<code>control_inputs</code>执行结束后才能执行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</div><div class="line">  <span class="comment"># `d` and `e` will only run after `a`, `b`, and `c` have executed.</span></div><div class="line">  d = ...</div><div class="line">  e = ...</div></pre></td></tr></table></figure>
<p>多次用<code>with</code>调用该函数会得到叠加的依赖，区域内的<code>ops</code>将会在以上所有层次的<code>control_inputs</code>运行结束后才能得到运行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">    <span class="comment"># Ops constructed here run after `a`, `b`, `c`, and `d`.</span></div></pre></td></tr></table></figure>
<p>我们可以用<code>None</code>来清空控制所有依赖。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies(<span class="keyword">None</span>):</div><div class="line">    <span class="comment"># Ops constructed here run normally, not waiting for either `a` or `b`.</span></div><div class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">      <span class="comment"># Ops constructed here run after `c` and `d`, also not waiting</span></div><div class="line">      <span class="comment"># for either `a` or `b`.</span></div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p>控制依赖起作用的区域内只有<code>ops</code>会被执行，仅仅把一个节点放在该区域是不起作用的。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># WRONG</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  t = tf.matmul(tensor, tensor)</div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created outside the context, so no control</span></div><div class="line">    <span class="comment"># dependency will be added.</span></div><div class="line">    <span class="keyword">return</span> t</div><div class="line"></div><div class="line"><span class="comment"># RIGHT</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created in the context, so a control dependency</span></div><div class="line">    <span class="comment"># will be added.</span></div><div class="line">    <span class="keyword">return</span> tf.matmul(tensor, tensor)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="tf-train-Saver"><a href="#tf-train-Saver" class="headerlink" title="tf.train.Saver"></a>tf.train.Saver</h1><p>在训练过程中我们可能会想到每隔一段训练保存一次模型，一方面为了防止过拟合，另一方面如果训练过程被意外打断还可以从某个保存点继续开始训练。之前已经学习过<code>Variable</code>的概念，今天来学习下如何用<code>tf.train.Saver</code>来保存和恢复一个模型。关于<code>tf.train.Saver</code>更详细的内容请参考官方文档。</p>
<p><code>Saver</code>类添加<code>ops</code>来保存变量到<em>checkpoints</em>或者从<em>checkpoints</em>中恢复变量，它同时提供了一些函数方法来运行这些<code>ops</code>。<em>checkpoints</em>是一种二进制文件，它把<code>variable name</code>和<code>tensor</code>值联系起来，默认的为<code>tf.Variable.name</code>。使用<em>checkpoints</em>最好的方法就是用<code>Saver</code>来载入它。</p>
<p><code>Saver</code>可以自动的给<em>checkpoints</em>文件命名，这使得我们在不同的训练阶段可以保存不同的<em>checkpoints</em>。例如我们可以用训练迭代次数来给<em>checkpoints</em>命名，为了防止训练阶段磁盘空间占用量过多，我们还可以选择只保存最近N个文件，或者每N个小时保存一次文件。</p>
<p><br></p>
<h2 id="保存变量"><a href="#保存变量" class="headerlink" title="保存变量"></a>保存变量</h2><p>用<code>tf.train.Saver()</code>创建一个<code>Saver</code>对象来控制模型中所有的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add an op to initialize the variables.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, initialize the variables, do some work, save the</span></div><div class="line"><span class="comment"># variables to disk.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  sess.run(init_op)</div><div class="line">  <span class="comment"># Do some work with the model.</span></div><div class="line">  <span class="comment"># ..</span></div><div class="line">  <span class="comment"># Save the variables to disk.</span></div><div class="line">  save_path = saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model saved in file: %s"</span> % save_path)</div></pre></td></tr></table></figure>
<p>为了自动给<em>checkpoints</em>文件命名我们可以传入一个<code>global_step</code>值给<code>save()</code>函数。例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">saver.save(sess, 'my-model', global_step=0) ==&gt; filename: 'my-model-0'</div><div class="line">...</div><div class="line">saver.save(sess, 'my-model', global_step=1000) ==&gt; filename: 'my-model-1000'</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="恢复变量"><a href="#恢复变量" class="headerlink" title="恢复变量"></a>恢复变量</h2><p>我们用同一个<code>Saver</code>对象来恢复变量，注意当我们从一个文件恢复变量时我们没必要提前对变量初始化。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and</span></div><div class="line"><span class="comment"># do some work with the model.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Restore variables from disk.</span></div><div class="line">  saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model restored."</span>)</div><div class="line">  <span class="comment"># Do some work with the model</span></div><div class="line">  <span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="保存和恢复部分变量"><a href="#保存和恢复部分变量" class="headerlink" title="保存和恢复部分变量"></a>保存和恢复部分变量</h2><p>如果我们没有给<code>tf.train.Saver()</code>任何参数，它默认保存所有变量，每个变量都和它们的<code>name</code>联系起来。</p>
<p>有时候我们想要在<em>checkpoint</em>文件中给某个变量定义一个具体的<code>name</code>，例如我们想把一个变量取名为<code>&quot;weights&quot;</code>，我们想恢复它的值到一个新的名字叫<code>&quot;param&quot;</code>的变量中。</p>
<p>有时候我们想保存和恢复模型的某一组变量，例如我们训练了一个5层的神经网络，但我们想训练一个新的6层的神经网络，并且从5层的神经网络中恢复参数到新的6层模型的前5层中。</p>
<p>我们可以传入一个变量列表到<code>tf.train.Saver()</code>中，变量在<code>checkpoint</code>文件中的<code>name</code>就是<code>op</code>的<code>name</code>。我们也可以很简单的把<code>names</code>和变量组织成一个Python字典传入<code>tf.train.Saver()</code>中来保存下来，<code>keys</code>是我们使用的<code>names</code>，<code>values</code>是我们的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(..., name=<span class="string">'v1'</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">'v2'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Pass the variables as a dict:</span></div><div class="line">saver = tf.train.Saver(&#123;<span class="string">'v1'</span>: v1, <span class="string">'v2'</span>: v2&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Or pass them as a list.</span></div><div class="line">saver = tf.train.Saver([v1, v2])</div><div class="line"><span class="comment"># Passing a list is equivalent to passing a dict with the variable op names</span></div><div class="line"><span class="comment"># as keys:</span></div><div class="line">saver = tf.train.Saver(&#123;v.op.name: v <span class="keyword">for</span> v <span class="keyword">in</span> [v1, v2]&#125;)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ol>
<li>如果我们想保存不同模型的几组变量，我们可以创建很多个<code>saver</code>对象。而且相同的变量可以存储在不同的<code>saver</code>对象中，它们的值只有在<code>restore()</code>函数执行后才改变。</li>
<li>如果我们想恢复一组变量到模型中，我们必须事先初始化其他变量。</li>
</ol>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="external">tf.train.Saver  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" rel="external">Variables: Creation, Initialization, Saving, and Loading  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（四）：TensorBoard可视化]]></title>
      <url>/2017/06/27/TensorFlow_4/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文对TensorFlow的重要可视化工具TensorBoard进行学习。</p>
<p>当我们训练庞大且复杂的神经网络时，为了便于我们理解和调试TensorFlow程序，可视化工具显得尤为重要。而TensorFlow有一个很方便的可视化工具名为TensorBoard，我们可以利用它来可视化我们的<code>graph</code>以及训练的细节。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<hr>
<p>TensorBoard的界面如下图所示：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627153131_zidI6p_tensorboard.jpeg" alt="MNIST TensorBoard" width="700"></p>
<p>界面主要提供以下几种不同的数据可视化类型：</p>
<ul>
<li>SCALARS</li>
<li>IMAGES</li>
<li>AUDIO</li>
<li>GRAPHS</li>
<li>DISTRIBUTIONS</li>
<li>HISTOGRAMS</li>
<li>EMBEDDINGS</li>
<li>TEXT</li>
</ul>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在TensorFlow运行过程中，我们可以通过<code>summary operations</code>操作来保存它的运行日志，TensorBoard通过读取这些运行日志来实现可视化功能。下面是使用TensorBoard的生命周期：</p>
<ol>
<li>创建一个<code>TensorFlow graph</code>，用<code>summary operations</code>来保存我们需要了解的节点的信息。</li>
<li>我们需要整合所有节点的信息，如果一个一个提取太耗费精力和时间，我们可以选择<code>tf.summary.merge_all()</code>函数来把所有的<code>op</code>节点整合成一个单一节点，然后通过运行该节点来提取出所有<code>summary op</code>的信息。</li>
<li>用<code>tf.summary.FileWriter()</code>函数把提取出的所有的<code>summary op</code>信息存入磁盘中。我们同时可以用该函数把<code>graph</code>的信息存入磁盘，用来可视化神经网络的结构。</li>
<li>在训练过程中用<code>add_summary</code>命令把训练信息存入磁盘，训练结束后记得把<code>FileWriter</code>对象<code>close</code>。</li>
<li>打开终端，键入以下命令运行TensorBoard：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure>
<p>得到提示信息<code>“Starting TensorBoard...”</code>则运行成功，我们可以打开浏览器，在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板。</p>
<p><br></p>
<h1 id="Summary-Operations"><a href="#Summary-Operations" class="headerlink" title="Summary Operations"></a>Summary Operations</h1><p>我们利用该操作来从TensorFlow运行过程中获得信息，然后在TensorBoard上展示出来，<code>summary ops</code>同样是<code>ops</code>的一种，它们和<code>tf.matmul</code>或者<code>tf.nn.relu</code>等操作一样。这意味着它们和其他<code>ops</code>相同，包含在一个<code>graph</code>中，读入一个<code>tensors</code>并且输出一个<code>tensors</code>。但是它们也有不同的地方，<code>summary ops</code>输出的向量还包含着连续的<code>protobufs</code>信息，它们可以被保存到磁盘上并且被TensorBoard读取进行可视化。</p>
<p>常见的<code>summary ops</code>如下：</p>
<ul>
<li>tf.summary.scalar</li>
<li>tf.summary.image</li>
<li>tf.summary.audio</li>
<li>tf.summary.text</li>
<li>tf.summary.histogram</li>
</ul>
<p><br></p>
<p>我们在这里主要介绍三种：</p>
<h2 id="Scalar"><a href="#Scalar" class="headerlink" title="Scalar"></a>Scalar</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, accuracy)</div></pre></td></tr></table></figure>
<p><code>Scalar</code>存储一些非向量的单一数值，通常会随着迭代次数变化，例如<code>accuracy</code>或者<code>cross_entropy</code>。我们可以在TensorBoard的<code>SCALARS</code>面板看到该类数据的详细信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_XxfLyL_scalar.jpeg" alt="SCALARS" width="700"></p>
<h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.image(<span class="string">'input'</span>, x_image, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<p>我们可以输出包含图像的<code>summary protobufs</code>，图像信息保存在<code>tensor</code>中，并且必须是4-D的形式<code>[batch_size, height, width, channels]</code>，其中，<code>channels</code>可以是以下形式：</p>
<ul>
<li>1: <code>tensor</code> is interpreted as Grayscale.</li>
<li>3: <code>tensor</code> is interpreted as RGB.</li>
<li>4: <code>tensor</code> is interpreted as RGBA.</li>
</ul>
<p>我们可以在TensorBoard的<code>IMAGES</code>面板看到图片信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_YAlrJU_images.jpeg" alt="IMAGES" width="700"></p>
<h2 id="Histogram"><a href="#Histogram" class="headerlink" title="Histogram"></a>Histogram</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.histogram(<span class="string">"weights"</span>, w)</div></pre></td></tr></table></figure>
<p>很多时候我们想要知道某些向量的分布，比如参数<code>weight</code>、<code>bias</code>或者某一层的输出向量，以及它们随着时间的变化规律，这时可以采用Histogram。</p>
<p>在TensorBoard中有两种显示向量分布的方式，分别是面板中的<code>DISTRIBUTIONS</code>和<code>HISTOGRAMS</code>，如下两图所示。</p>
<p><strong>DISTRIBUTIONS：</strong></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_w23F61_distribution.jpeg" alt="DISTRIBUTIONS" width="700"></p>
<p><strong>HISTOGRAMS：</strong></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_uvshkp_histogram.jpeg" alt="HISTOGRAMS" width="700"></p>
<h2 id="Merge-all"><a href="#Merge-all" class="headerlink" title="Merge_all"></a>Merge_all</h2><p>当我们定义好所有ops，我们可以用以下函数来把所有的<code>op</code>节点整合成一个单一节点，然后通过运行该节点来提取出所有<code>summary op</code>的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum = tf.summary.merge_all()</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h1><p>当我们训练无比庞大且复杂的神经网络时，我们一般会希望对整个网络的结构进行可视化，本文开头的图片展示的便是对MNIST数据集一个简单的五层神经网络的可视化。在提取了所有<code>summary_op</code>信息后，我们可以用以下函数把它们写进给定的磁盘目录下，在此同时可以把<code>graph</code>的信息一起写进去，这样我们便可以在TensorBoard的<code>GRAPHS</code>下对整个神经网络的结构进行可视化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">file_writer = tf.summary.FileWriter(&apos;/path/to/logs&apos;, sess.graph)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="Name-scoping-and-nodes"><a href="#Name-scoping-and-nodes" class="headerlink" title="Name scoping and nodes"></a>Name scoping and nodes</h2><p>然而直接保存得到的结构往往无比晦涩难懂，这时我们就可以考虑用前一个笔记中学到的<code>name_scope</code>对变量和<code>ops</code>进行分层。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'layer2'</span>):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'weight'</span>):</div><div class="line">        W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'bias'</span>):</div><div class="line">        b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'conv'</span>):</div><div class="line">        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'pool'</span>):</div><div class="line">        h_pool2 = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<p>这样我们就可以得到本文开端处的图片中展示的结构图，通常默认只显示最高层，双击每一个模块或者单机模块右上方的<code>+</code>，可以进一步展开，我们可以得到进一步详细的结构信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180059_C4hmAd_graph.jpeg" alt="GRAPHS" width="700"></p>
<p>TensorBoard的<code>graph</code>有两种依赖，<strong>数据依赖</strong>和<strong>控制依赖</strong>。数据依赖展示了<code>tensors</code>在两个<code>ops</code>之间的流动以及流动方向；控制依赖运用了虚线来表示。TensorBoard显示<code>graph</code>有两个区域，<strong>主区域</strong>和<strong>辅助区域</strong>，为了便于观察我们可以把一些高层次的依赖项多的节点从主区域移出到辅助区域，只需要在节点右键点击<code>Remove from main graph</code>即可，如上图所示我们把<code>train</code>移到辅助区域。</p>
<p><br></p>
<h2 id="Runtime-statistics"><a href="#Runtime-statistics" class="headerlink" title="Runtime statistics"></a>Runtime statistics</h2><p>通常来说搜集运行时间的统计数据对我们是很有帮助的，例如总的内存使用，总的计算时间，以及节点的<code>tensor</code>形状。我们可以在<code>GRAPHS</code>的侧边栏处通过选择<code>Compute time</code>和<code>Memory</code>来看每个节点处响应的信息，颜色越深表明相应的值越大。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170629223648_L7I2IB_Run metadata graph.jpeg" alt="Run metadata graph" width="700"></p>
<p>如下是一段代码示例，我们在第99、199、299……1999处保存<code>Runtime</code>统计数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000</span>):</div><div class="line">    <span class="comment">#...</span></div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:  <span class="comment"># Record execution statistics</span></div><div class="line">        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</div><div class="line">        run_metadata = tf.RunMetadata()</div><div class="line">        summary, _ = sess.run([merged, train_step],</div><div class="line">                              feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;,</div><div class="line">                              options=run_options,</div><div class="line">                              run_metadata=run_metadata)</div><div class="line">        train_writer.add_run_metadata(run_metadata, <span class="string">'step%d'</span> % i)</div><div class="line">        train_writer.add_summary(summary, i)</div><div class="line">        print(<span class="string">'Adding run metadata for'</span>, i)</div><div class="line">    <span class="comment">#...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="运行-TensorBoard"><a href="#运行-TensorBoard" class="headerlink" title="运行 TensorBoard"></a>运行 TensorBoard</h1><p>打开终端，键入以下命令运行TensorBoard：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure>
<p>这里的路径是我们用<code>tf.summary.FileWriter()</code>写入的路径。这里注意我们用<code>FileWriter</code>写入时可能会分为<code>train</code>和<code>test</code>两个路径，这时的TensorBoard路径应该为它们两个的上层路径，例如：</p>
<p><strong>.py文件:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_writer = tf.summary.FileWriter(<span class="string">'/tmp/tensorflow/mnist/my_example/train'</span>, sess.graph)</div><div class="line">test_writer = tf.summary.FileWriter(<span class="string">'/tmp/tensorflow/mnist/my_example/test'</span>)</div></pre></td></tr></table></figure>
<p><strong>终端命令：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=<span class="string">'/tmp/tensorflow/mnist/my_example/'</span></div></pre></td></tr></table></figure>
<p>如果我们想比较不同的网络模型的运行情况，例如我们想改超参数，想知道哪个值能获得更高的准确率，TensorBoard允许我们打开不同的模型保存的<code>log</code>，当TensorBoard打开一个路径时，它会遍历此路径下包括子目录下的所有的事件文件，每次它进入一个子目录，它都会载入它作为一个新的<code>run</code>，前段界面也会通过这个路径来组织数据。例如如下有一个已经组织好的TensorBoard <code>log</code>目录，有两个<code>runs</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/some/path/mnist_experiments/run1/events.out.tfevents.1456525581.name</div><div class="line">/some/path/mnist_experiments/run2/events.out.tfevents.1456525385.name</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=/some/path/mnist_experiments</div></pre></td></tr></table></figure>
<p>然后打开浏览器，在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板即可看到可视化结果。</p>
<p>如下是一个不同模型结构训练结果可视化的例子（代码在下面）：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630113432_HnboU8_more model.jpeg" alt="Different models" width="700"></p>
<p><br></p>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><p>我在MNIST上进行了简单的实现，为了更好的调试超参数，我实验了不同的卷积层、全连接层和是否开启<code>dropout</code>，完整代码可以在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/TensorFlow/TensorBoard" target="_blank" rel="external">GitHub</a>上下载，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Copyright 2017 Zangbo. All Rights Reserved.</span></div><div class="line"><span class="comment"># A simple MNIST classifier which displays summaries in TensorBoard.</span></div><div class="line"><span class="comment"># This is an unimpressive MNIST model, but it is a good example of using</span></div><div class="line"><span class="comment"># tf.name_scope to make a graph legible in the TensorBoard graph explorer, and of</span></div><div class="line"><span class="comment"># naming summary tags so that they are grouped meaningfully in TensorBoard.</span></div><div class="line"><span class="comment"># ==============================================================================</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment">### MNIST datasets ###</span></div><div class="line">LOGDIR = <span class="string">'/tmp/mnist_tutorial/'</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(train_dir=LOGDIR + <span class="string">'data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_layer</span><span class="params">(input, size_in, size_out, name=<span class="string">"conv"</span>)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">    w = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, size_in, size_out], stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div><div class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[size_out]), name=<span class="string">"B"</span>)</div><div class="line">    conv = tf.nn.conv2d(input, w, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line">    act = tf.nn.relu(conv + b)</div><div class="line">    tf.summary.histogram(<span class="string">"weights"</span>, w)</div><div class="line">    tf.summary.histogram(<span class="string">"biases"</span>, b)</div><div class="line">    tf.summary.histogram(<span class="string">"activations"</span>, act)</div><div class="line">    <span class="keyword">return</span> tf.nn.max_pool(act, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_layer</span><span class="params">(input, size_in, size_out, name=<span class="string">"fc"</span>)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">    w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div><div class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[size_out]), name=<span class="string">"B"</span>)</div><div class="line">    act = tf.nn.relu(tf.matmul(input, w) + b)</div><div class="line">    tf.summary.histogram(<span class="string">"weights"</span>, w)</div><div class="line">    tf.summary.histogram(<span class="string">"biases"</span>, b)</div><div class="line">    tf.summary.histogram(<span class="string">"activations"</span>, act)</div><div class="line">    <span class="keyword">return</span> act</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnist_model</span><span class="params">(learning_rate, use_two_conv, use_two_fc, use_dropout, hparam)</span>:</span></div><div class="line">  tf.reset_default_graph()</div><div class="line">  sess = tf.Session()</div><div class="line"></div><div class="line">  <span class="comment"># Setup placeholders, and reshape the data</span></div><div class="line">  x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">"x"</span>)</div><div class="line">  x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">  tf.summary.image(<span class="string">'input'</span>, x_image, <span class="number">3</span>)</div><div class="line">  y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">"labels"</span>)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> use_two_conv:</div><div class="line">    conv1 = conv_layer(x_image, <span class="number">1</span>, <span class="number">32</span>, <span class="string">"conv1"</span>)</div><div class="line">    conv_out = conv_layer(conv1, <span class="number">32</span>, <span class="number">64</span>, <span class="string">"conv2"</span>)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    conv1 = conv_layer(x_image, <span class="number">1</span>, <span class="number">64</span>, <span class="string">"conv"</span>)</div><div class="line">    conv_out = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">  flattened = tf.reshape(conv_out, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">if</span> use_two_fc:</div><div class="line">    fc1 = fc_layer(flattened, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>, <span class="string">"fc1"</span>)</div><div class="line">    <span class="keyword">if</span> use_dropout:</div><div class="line">      <span class="keyword">with</span> tf.name_scope(<span class="string">'dropout'</span>):</div><div class="line">        keep_prob = tf.constant(<span class="number">0.5</span>, name=<span class="string">'keep_prob'</span>)</div><div class="line">        fc1_dropout = tf.nn.dropout(fc1, keep_prob)</div><div class="line">        logits = fc_layer(fc1_dropout, <span class="number">1024</span>, <span class="number">10</span>, <span class="string">"fc2"</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      logits = fc_layer(fc1, <span class="number">1024</span>, <span class="number">10</span>, <span class="string">"fc2"</span>)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    logits = fc_layer(flattened, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">10</span>, <span class="string">"fc"</span>)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"xent"</span>):</div><div class="line">    xent = tf.reduce_mean(</div><div class="line">        tf.nn.softmax_cross_entropy_with_logits(</div><div class="line">            logits=logits, labels=y), name=<span class="string">"xent"</span>)</div><div class="line">    tf.summary.scalar(<span class="string">"xent"</span>, xent)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"train"</span>):</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"accuracy"</span>):</div><div class="line">    correct_prediction = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">    tf.summary.scalar(<span class="string">"accuracy"</span>, accuracy)</div><div class="line"></div><div class="line">  summ = tf.summary.merge_all()</div><div class="line"></div><div class="line">  sess.run(tf.global_variables_initializer())</div><div class="line">  writer = tf.summary.FileWriter(LOGDIR + hparam)</div><div class="line">  writer.add_graph(sess.graph)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2001</span>):</div><div class="line">    batch = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">      [train_accuracy, s] = sess.run([accuracy, summ], feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;)</div><div class="line">      writer.add_summary(s, i)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:  <span class="comment"># Record execution statistics</span></div><div class="line">      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</div><div class="line">      run_metadata = tf.RunMetadata()</div><div class="line">      _, s = sess.run([train_step, summ],</div><div class="line">                      feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;,</div><div class="line">                      options=run_options,</div><div class="line">                      run_metadata=run_metadata)</div><div class="line">      writer.add_run_metadata(run_metadata, <span class="string">'step%03d'</span> % i) <span class="comment">#add_run_metadata(run_metadata, tag, global_step=None)</span></div><div class="line">      writer.add_summary(s, i)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      sess.run(train_step, feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;)</div><div class="line">  writer.close()</div><div class="line">  sess.close()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_hparam_string</span><span class="params">(learning_rate, use_two_fc, use_two_conv, use_dropout)</span>:</span></div><div class="line">  conv_param = <span class="string">"conv=2"</span> <span class="keyword">if</span> use_two_conv <span class="keyword">else</span> <span class="string">"conv=1"</span></div><div class="line">  fc_param = <span class="string">"fc=2"</span> <span class="keyword">if</span> use_two_fc <span class="keyword">else</span> <span class="string">"fc=1"</span></div><div class="line">  dropout_param = <span class="string">"dropout"</span> <span class="keyword">if</span> use_dropout <span class="keyword">else</span> <span class="string">"no_dropout"</span></div><div class="line">  <span class="keyword">return</span> <span class="string">"lr_%.0E,%s,%s,%s"</span> % (learning_rate, conv_param, fc_param,dropout_param)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(LOGDIR):</div><div class="line">    tf.gfile.DeleteRecursively(LOGDIR)</div><div class="line">  tf.gfile.MakeDirs(LOGDIR)</div><div class="line">  <span class="comment">#Add other param to try different learning rate</span></div><div class="line">  <span class="keyword">for</span> learning_rate <span class="keyword">in</span> [<span class="number">1E-4</span>]:</div><div class="line">    <span class="comment"># Try different model architectures</span></div><div class="line">    <span class="keyword">for</span> use_two_fc <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">      <span class="keyword">for</span> use_two_conv <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">        <span class="keyword">for</span> use_dropout <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">          <span class="comment"># Construct a hyperparameter string for each one (example: "lr_1E-4,fc=2,conv=2,dropout)</span></div><div class="line">          hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv, use_dropout)</div><div class="line">          print(<span class="string">'Starting run for %s'</span> % hparam)</div><div class="line"></div><div class="line">          <span class="comment"># Actually run with the new settings</span></div><div class="line">          mnist_model(learning_rate, use_two_fc, use_two_conv, use_dropout, hparam)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  main()</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/tensorboard/README.md" target="_blank" rel="external">tensorflow/README.md at r1.2 · tensorflow/tensorflow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/summary" target="_blank" rel="external">Module: tf.summary  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard: Visualizing Learning  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/get_started/graph_viz" target="_blank" rel="external">TensorBoard: Graph Visualization  |  TensorFlow</a></li>
<li><a href="https://www.youtube.com/watch?v=eBbEDRsCmv4" target="_blank" rel="external">TensorBoard实践介绍（2017年TensorFlow开发大会） - YouTube</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 可视化 </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（三）：Name_Scope和变量共享]]></title>
      <url>/2017/06/24/TensorFlow_3/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文学习了TensorFlow中的四个基本函数以及<strong>变量共享(Sharing Variables)</strong>机制：</p>
<ul>
<li><code>tf.Variable()</code></li>
<li><code>tf.get_variable()</code></li>
<li><code>tf.name_scope()</code></li>
<li><code>tf.variable_scope()</code></li>
</ul>
<p>其中<code>tf.get_variable()</code>和<code>tf.variable_scope()</code>共同构成了Tensorflow的<strong>变量共享</strong>机制。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Variable-和get-variable"><a href="#Variable-和get-variable" class="headerlink" title="Variable()和get_variable()"></a>Variable()和get_variable()</h1><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable()"></a>Variable()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</div></pre></td></tr></table></figure>
<p>函数创建一个变量，该变量的<code>type</code>和<code>shape</code>由初始化的值而定，可以用<code>assign</code>函数改变变量的<code>shape</code>。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create two variables.</span></div><div class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),</div><div class="line">                      name=<span class="string">"weights"</span>)</div><div class="line">biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=<span class="string">"biases"</span>)</div><div class="line">...</div><div class="line"><span class="comment"># Add an op to initialize the variables.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Later, when launching the model</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Run the init operation.</span></div><div class="line">  sess.run(init_op)</div><div class="line">  ...</div><div class="line">  <span class="comment"># Use the model</span></div><div class="line">  ...</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="get-variable"><a href="#get-variable" class="headerlink" title="get_variable()"></a>get_variable()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = tf.get_variable(name, shape, dtype, initializer)</div></pre></td></tr></table></figure>
<p>1、函数创建一个新的变量或者返回一个已经存在的变量，它是<code>variable_scope</code>的重要组成部分。</p>
<p>创建一个新的变量，这时默认<code>tf.get_variable_scope().reuse == False</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/v:0"</span></div></pre></td></tr></table></figure>
<p>2、为了重新使用(reuse)已经存在的变量，这时改变<code>tf.get_variable_scope().reuse == True</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ol>
<li>该函数必须指定<code>name</code>，它会在当前的<code>variable_scope</code>进行<code>reuse</code>检查。</li>
<li>该函数必须指定<code>shape</code>。</li>
<li>该函数定义时可以不进行初始化， 这样会使用<code>variable_scope</code>的默认初始化，如果这两者都没有，那么就使用<code>glorot_uniform_initializer</code>，即<code>xavier</code>进行初始化。</li>
</ol>
<p>一个基本的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])  <span class="comment"># v.name == "foo/v:0"</span></div><div class="line">    w = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])  <span class="comment"># w.name == "foo/w:0"</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>)  <span class="comment"># v.name == "foo/v:0"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ul>
<li><code>tf.Variable()</code><ol>
<li>可以不定义<code>name</code>。</li>
<li>检测到<code>name</code>冲突，系统会自己处理，不会报错。</li>
<li>每次都创建新的变量，<code>reuse=True</code>对它无影响。</li>
<li>需要初始化。</li>
</ol>
</li>
<li><code>tf.get_variable()</code><ol>
<li>必须定义<code>name</code>。</li>
<li>检测到<code>name</code>冲突，系统不会处理，会报错。</li>
<li><code>reuse=True</code>对它有影响，此时如果已经创建了变量对象，则把对象返回，如果没有，就创建一个新的。</li>
<li>可以不用初始化。</li>
<li>共享变量时使用。</li>
</ol>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>):</div><div class="line">    w1 = tf.get_variable(<span class="string">"w1"</span>, shape=[])</div><div class="line">    w2 = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"w2"</span>)</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    w1_p = tf.get_variable(<span class="string">"w1"</span>, shape=[])</div><div class="line">    w2_p = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w2"</span>)</div><div class="line"></div><div class="line">print(w1 <span class="keyword">is</span> w1_p, w2 <span class="keyword">is</span> w2_p)</div><div class="line"></div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#True  False</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="name-scope-和variable-scope"><a href="#name-scope-和variable-scope" class="headerlink" title="name_scope()和variable_scope()"></a>name_scope()和variable_scope()</h1><h2 id="name-scope"><a href="#name-scope" class="headerlink" title="name_scope()"></a>name_scope()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">name_scope(name)</div></pre></td></tr></table></figure>
<p>TensorFlow 可以有数以千计的节点，如此多而难以一下全部看到，甚至无法使用标准图表工具来展示。为简单起见，我们为<code>op</code>名划定范围，并且把该信息用于在图表中的节点上定义一个层级。默认情况下， 只有顶层节点会显示。一个<code>graph</code>是无数<code>name_scope</code>堆叠而来，用命令<code>with name_scope(...):</code>可以添加一个新的<code>name_scope</code>。</p>
<ul>
<li>如果输入一个字符串，那么会建立一个以该字符串命名的<code>name_scope</code>，在此下面定义的所有<code>op</code>均属于该<code>name_scope</code>。如果该字符串已经被定义，则会把命名改为<code>字符串_*</code>，<code>*</code>代表从1开始的阿拉伯数字。例如已经存在一个名为<code>&quot;a&quot;</code>的<code>name_scope</code>，再定义一个名为<code>&quot;a&quot;</code>的<code>name_scope</code>时，会自动把该<code>name_scope</code>改名为<code>&quot;a_1&quot;</code>，数字会依次累加。</li>
<li>如果用<code>with name_scope(...) as scope:</code>指令，那么可以利用该<code>name_scope</code>通过命令<code>with name_scope(scope):</code>重新进入已经建立好的<code>name_scope</code>下定义<code>op</code>。</li>
<li>如果<code>with name_scope(&quot;&quot;)</code>参数为空，则会重置当前<code>name_scope</code>为顶层空<code>name_scope</code>。</li>
</ul>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">  c = tf.constant(<span class="number">5.0</span>, name=<span class="string">"c"</span>)</div><div class="line">  <span class="keyword">assert</span> c.op.name == <span class="string">"c"</span></div><div class="line">  c_1 = tf.constant(<span class="number">6.0</span>, name=<span class="string">"c"</span>)</div><div class="line">  <span class="keyword">assert</span> c_1.op.name == <span class="string">"c_1"</span></div><div class="line"></div><div class="line">  <span class="comment"># Creates a scope called "nested"</span></div><div class="line">  <span class="keyword">with</span> g.name_scope(<span class="string">"nested"</span>) <span class="keyword">as</span> scope:</div><div class="line">    nested_c = tf.constant(<span class="number">10.0</span>, name=<span class="string">"c"</span>)</div><div class="line">    <span class="keyword">assert</span> nested_c.op.name == <span class="string">"nested/c"</span></div><div class="line"></div><div class="line">    <span class="comment"># Creates a nested scope called "inner".</span></div><div class="line">    <span class="keyword">with</span> g.name_scope(<span class="string">"inner"</span>):</div><div class="line">      nested_inner_c = tf.constant(<span class="number">20.0</span>, name=<span class="string">"c"</span>)</div><div class="line">      <span class="keyword">assert</span> nested_inner_c.op.name == <span class="string">"nested/inner/c"</span></div><div class="line"></div><div class="line">    <span class="comment"># Create a nested scope called "inner_1".</span></div><div class="line">    <span class="keyword">with</span> g.name_scope(<span class="string">"inner"</span>):</div><div class="line">      nested_inner_1_c = tf.constant(<span class="number">30.0</span>, name=<span class="string">"c"</span>)</div><div class="line">      <span class="keyword">assert</span> nested_inner_1_c.op.name == <span class="string">"nested/inner_1/c"</span></div><div class="line"></div><div class="line">      <span class="comment"># Treats `scope` as an absolute name_scope, and</span></div><div class="line">      <span class="comment"># switches to the "nested/" scope.</span></div><div class="line">      <span class="keyword">with</span> g.name_scope(scope):</div><div class="line">        nested_d = tf.constant(<span class="number">40.0</span>, name=<span class="string">"d"</span>)</div><div class="line">        <span class="keyword">assert</span> nested_d.op.name == <span class="string">"nested/d"</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> g.name_scope(<span class="string">""</span>):</div><div class="line">          e = tf.constant(<span class="number">50.0</span>, name=<span class="string">"e"</span>)</div><div class="line">          <span class="keyword">assert</span> e.op.name == <span class="string">"e"</span></div></pre></td></tr></table></figure>
<p><code>with g.name_scope(...) as scope:</code>中的<code>scope</code>存放了该<code>scope</code>的名字，可以在之后的<code>op</code>中直接使用该<code>scope</code>来给<code>op</code>命名。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">  <span class="keyword">with</span> g.name_scope(<span class="string">'my_layer'</span>) <span class="keyword">as</span> scope:</div><div class="line">    output = tf.constant(<span class="number">1</span>, name=scope)</div><div class="line"><span class="keyword">assert</span> output.op.name == <span class="string">"my_layer"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="variable-scope"><a href="#variable-scope" class="headerlink" title="variable_scope()"></a>variable_scope()</h2><p><code>variable_scope</code>函数将会在变量名前面加个前缀，同时标注<code>reuse</code>。利用缩进形式会分成有层次的<code>variable_scope</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">        v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/bar/v:0"</span></div></pre></td></tr></table></figure>
<p>我们可以通过<code>tf.get_variable_scope()</code>来得到当前的<code>variable_scope</code>，同时可以利用<code>tf.get_variable_scope().reuse_variables():</code>把当前的<code>variable_scope</code>的<code>reuse</code>改成<code>True</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">    tf.get_variable_scope().reuse_variables()</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div></pre></td></tr></table></figure>
<p>当一个<code>variable_scope</code>的<code>reuse</code>默认为<code>False</code>，但被改成了<code>True</code>以后，是无法再改回<code>False</code>的，而且如果一个<code>variable_scope</code>的<code>reuse</code>被改成了<code>True</code>，它下属的所有<code>variable_scope</code>的<code>reues</code>就都是<code>True</code>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"root"</span>):</div><div class="line">    <span class="comment"># At start, the scope is not reusing.</span></div><div class="line">    <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">        <span class="comment"># Opened a sub-scope, still not reusing.</span></div><div class="line">        <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">        <span class="comment"># Explicitly opened a reusing scope.</span></div><div class="line">        <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">True</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">            <span class="comment"># Now sub-scope inherits the reuse flag.</span></div><div class="line">            <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">True</span></div><div class="line">    <span class="comment"># Exited the reusing scope, back to a non-reusing one.</span></div><div class="line">    <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div></pre></td></tr></table></figure>
<p>我们也可以直接用<code>variable_scope</code>来代替参数名字进入该<code>variable_scope</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>) <span class="keyword">as</span> foo_scope:</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(foo_scope):</div><div class="line">    w = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(foo_scope, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">    w1 = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div><div class="line"><span class="keyword">assert</span> w1 <span class="keyword">is</span> w</div></pre></td></tr></table></figure>
<p>当我们用之前存在的<code>variable_scope</code>进入一个<code>variable_scope</code>时，我们会跳出当前的<code>variable_scope</code>而进入一个完全不同的<code>variable_scope</code>，哪怕它在某个<code>variable_scope</code>的缩进下面，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>) <span class="keyword">as</span> foo_scope:</div><div class="line">    <span class="keyword">assert</span> foo_scope.name == <span class="string">"foo"</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"baz"</span>) <span class="keyword">as</span> other_scope:</div><div class="line">        <span class="keyword">assert</span> other_scope.name == <span class="string">"bar/baz"</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(foo_scope) <span class="keyword">as</span> foo_scope2:</div><div class="line">            <span class="keyword">assert</span> foo_scope2.name == <span class="string">"foo"</span>  <span class="comment"># Not changed.</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="区别-1"><a href="#区别-1" class="headerlink" title="区别"></a>区别</h2><p><code>name_scope</code>是给<code>op_name</code>加前缀，<code>variable_scope</code>是给<code>get_variable()</code>创建的变量的名字加前缀。</p>
<p><code>name_scope</code>可以在<code>variable_scope</code>中打开，但是<code>name_scope</code>仅仅会影响<code>op</code>的<code>name</code>，而不会影响<code>variable</code>的<code>name</code>，但是<code>variable_scope</code>的会影响<code>op</code>的<code>name</code>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"bar"</span>):</div><div class="line">        v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">        x = <span class="number">1.0</span> + v</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/v:0"</span></div><div class="line"><span class="keyword">assert</span> x.op.name == <span class="string">"foo/bar/add"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Sharing-Variables"><a href="#Sharing-Variables" class="headerlink" title="Sharing Variables"></a>Sharing Variables</h1><p>Tensorflow的<strong>变量共享(Sharing Variables)</strong>机制主要是由<code>tf.get_variable()</code>和<code>tf.variable_scope()</code>构成的。</p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>设想一下，假如我们有一个神经网络，它只有两层卷积层组成，而且两层卷积层的输入输出尺寸相同并且卷积核尺寸也相同，如果我们用<code>tf.Variable()</code>来定义这样一个模型，代码大概是下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></div><div class="line">    conv1_weights = tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>]),</div><div class="line">        name=<span class="string">"conv1_weights"</span>)</div><div class="line">    conv1_biases = tf.Variable(tf.zeros([<span class="number">32</span>]), name=<span class="string">"conv1_biases"</span>)</div><div class="line">    conv1 = tf.nn.conv2d(input_images, conv1_weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    relu1 = tf.nn.relu(conv1 + conv1_biases)</div><div class="line"></div><div class="line">    conv2_weights = tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>]),</div><div class="line">        name=<span class="string">"conv2_weights"</span>)</div><div class="line">    conv2_biases = tf.Variable(tf.zeros([<span class="number">32</span>]), name=<span class="string">"conv2_biases"</span>)</div><div class="line">    conv2 = tf.nn.conv2d(relu1, conv2_weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    <span class="keyword">return</span> tf.nn.relu(conv2 + conv2_biases)</div></pre></td></tr></table></figure>
<p>其中有四个不同的变量：<code>conv1_weights</code>、<code>conv1_biases</code>、<code>conv2_weights</code>、<code>conv2_biases</code>，那么问题出现了，当我们反复使用这样一个模型时，每次都要开辟新的存储空间给四个变量，例如我们有两张不同的图片<code>image1</code>和<code>image2</code>，我们先后把它们送入该模型中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># First call creates one set of 4 variables.</span></div><div class="line">result1 = my_image_filter(image1)</div><div class="line"><span class="comment"># Another set of 4 variables is created in the second call.</span></div><div class="line">result2 = my_image_filter(image2)</div></pre></td></tr></table></figure>
<p>总共创建了8个不同的变量。极大的浪费了资源，如果图片数量增多，模型变得复杂，那么耗费的资源是巨大的。因此为了解决这个问题，Tensorflow出现了变量共享的机制。</p>
<p><br></p>
<h2 id="变量共享方法"><a href="#变量共享方法" class="headerlink" title="变量共享方法"></a>变量共享方法</h2><p>下面，我们用<strong>变量共享</strong>来实现上面的模型：</p>
<p>首先利用<code>tf.get_variable()</code>定义一个实现卷积操作的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></div><div class="line">    <span class="comment"># Create variable named "weights".</span></div><div class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape,</div><div class="line">        initializer=tf.random_normal_initializer())</div><div class="line">    <span class="comment"># Create variable named "biases".</span></div><div class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape,</div><div class="line">        initializer=tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    conv = tf.nn.conv2d(input, weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</div></pre></td></tr></table></figure>
<p>然后利用<code>tf.variable_scope()</code>来创建两个在不同<code>variable_scope</code>的卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</div><div class="line">        <span class="comment"># Variables created here will be named "conv1/weights", "conv1/biases".</span></div><div class="line">        relu1 = conv_relu(input_images, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</div><div class="line">        <span class="comment"># Variables created here will be named "conv2/weights", "conv2/biases".</span></div><div class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>如果我们直接调用<code>my_image_filter</code>两次会发生什么呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">result1 = my_image_filter(image1)</div><div class="line">result2 = my_image_filter(image2)</div><div class="line"><span class="comment"># ValueError(... conv1/weights already exists ...)</span></div></pre></td></tr></table></figure>
<p>程序报错，<code>tf.get_variable()</code>会检查变量是否已经存在并且是否共享。如果想共享它们，可以通过设置<code>reuse_variables()</code>来实现，它会把当前的<code>reuse</code>改为<code>True</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"image_filters"</span>) <span class="keyword">as</span> scope:</div><div class="line">    result1 = my_image_filter(image1)</div><div class="line">    scope.reuse_variables()</div><div class="line">    result2 = my_image_filter(image2)</div></pre></td></tr></table></figure>
<p>这样不管我们处理多少张图片，都只会创建四个变量，极大的节省了资源。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="external">tf.Variable  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/get_variable" target="_blank" rel="external">tf.get_variable  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/name_scope" target="_blank" rel="external">tf.name_scope  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope" target="_blank" rel="external">tf.Graph  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" target="_blank" rel="external">tf.variable_scope  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/variable_scope" target="_blank" rel="external">Sharing Variables  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（四）：获取某地历史天气数据]]></title>
      <url>/2017/06/22/Crawler_4/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了爬虫的一个简单实战应用，因为前不久做了个竞赛需要用到历史天气，所以就写了个简单的爬虫程序，利用Requests库和Beautiful Soup库爬取某地的历史天气记录。</p>
<p>中间的数据整合方便起见调用了numpy库和pandas库，关于两者的详细使用方法大家可以去查阅相关资料。</p>
<p>所有代码基于python3.5版本。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>最近参加了天池的一个大数据竞赛，因为需要用到江苏省扬中市的历史天气信息，所以就写了个简单的爬虫程序来爬取。有很多网站都提供了历史天气，这里我们选择的网站是：<a href="http://lishi.tianqi.com/yangzhong/index.html" target="_blank" rel="external">扬中市历史天气-历史天气网</a>。</p>
<p><br></p>
<h1 id="分析网站"><a href="#分析网站" class="headerlink" title="分析网站"></a>分析网站</h1><p>1、打开上述网站，往下滑动，可以看到有一栏记录了该地每个月的天气信息，右键打开审查元素，查看该信息所在的标签，发现每个月的天气信息都在标签<code>&lt;div class=&#39;tqtongji1&#39;&gt;</code>下面的<code>&lt;a&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_p0q980_2.jpeg" alt="历史月份信息"></center>

<p><br></p>
<p>2、用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;tqtongji1&#39;).find_all(&#39;a&#39;)</code>命令得到所有月份的信息，提取出其中的链接，这里只提取最近30个月的信息。</p>
<p><br></p>
<p>3、打开其中一个月份链接，右键审查元素，发现该月份每天的天气信息都在标签<code>&lt;div class=&#39;tqtongji2&#39;&gt;</code>下面的<code>&lt;li&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_XtWKBE_3.jpeg" alt="部分天气信息"></center>

<p><br></p>
<p>4、用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;tqtongji2&#39;).find_all(&#39;li&#39;)</code>命令得到该月份每天的部分天气信息并保存在<code>list</code>中，同时用<code>find(&#39;div&#39;, class_=&#39;tqtongji2&#39;).find_all(&#39;a&#39;)</code>命令提取出其中的链接。</p>
<p><br></p>
<p>5、打开其中一天的链接，右键审查元素，发现该天的其他天气信息都在标签<code>&lt;div class=&#39;history_sh&#39;&gt;</code>下面的<code>&lt;span&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_XNW30r_4.jpeg" alt="其他天气信息"></center>

<p><br></p>
<p>6、用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;history_sh&#39;).find_all(&#39;span&#39;)</code>命令提取出该天的其他天气信息并保存在<code>list</code>中。</p>
<p><br></p>
<p>7、利用numpy数组和pandas的DataFrame结构对提取出的数据进行整合，最后输出为CSV文件。</p>
<p><br></p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">	</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">History_weather</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_url</span><span class="params">(self, url)</span>:</span></div><div class="line">        all_weather_index = self.get_all_weather_index(url)</div><div class="line">        all_weather_index = all_weather_index[:<span class="number">30</span>]</div><div class="line">        print(<span class="string">"Get all weather index!"</span>)</div><div class="line">        result_weather = self.get_all_weather(all_weather_index)</div><div class="line">        result_weather.to_csv(<span class="string">'all_weather.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">        print(<span class="string">'Save all weather success!'</span>)</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_soup</span><span class="params">(self, url)</span>:</span></div><div class="line">        headers = &#123;</div><div class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 \</span></div><div class="line">            (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8'&#125;</div><div class="line">        html = requests.get(url, headers)</div><div class="line">        soup = BeautifulSoup(html.text, <span class="string">'lxml'</span>)</div><div class="line">        <span class="keyword">return</span> soup</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_all_weather_index</span><span class="params">(self, url)</span>:</span></div><div class="line">        all_weather_soup = self.get_soup(url)</div><div class="line">        all_weather_index = all_weather_soup.find(</div><div class="line">            <span class="string">'div'</span>, class_=<span class="string">'tqtongji1'</span>).find_all(<span class="string">'a'</span>)</div><div class="line">        <span class="keyword">return</span> all_weather_index</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_all_weather</span><span class="params">(self, all_weather_index)</span>:</span></div><div class="line">        all_month_weather = list()</div><div class="line">        day_weather = list()</div><div class="line">        	</div><div class="line">        <span class="keyword">for</span> weather <span class="keyword">in</span> all_weather_index:</div><div class="line">            month_url = weather[<span class="string">'href'</span>]</div><div class="line">            month_name = weather.get_text()</div><div class="line">            month_weather_soup = self.get_soup(month_url)</div><div class="line">            month_weather = month_weather_soup.find(</div><div class="line">                <span class="string">'div'</span>, class_=<span class="string">'tqtongji2'</span>).find_all(<span class="string">'li'</span>)</div><div class="line">            day_weather_url = month_weather_soup.find(</div><div class="line">                <span class="string">'div'</span>, class_=<span class="string">'tqtongji2'</span>).find_all(<span class="string">'a'</span>)</div><div class="line">            <span class="keyword">for</span> day <span class="keyword">in</span> day_weather_url:</div><div class="line">                day_url = day[<span class="string">'href'</span>]</div><div class="line">                day_soup = self.get_soup(day_url)</div><div class="line">                day_text = day_soup.find(</div><div class="line">                    <span class="string">'div'</span>, class_=<span class="string">'history_sh'</span>).find_all(<span class="string">'span'</span>)</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> day_text:</div><div class="line">                    day_weather.append(i.get_text())</div><div class="line">            weather_list = list()</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> month_weather:</div><div class="line">                weather_list.append(i.get_text())</div><div class="line">            weather_list = weather_list[<span class="number">6</span>:]</div><div class="line">            all_month_weather = all_month_weather + weather_list</div><div class="line">            print(<span class="string">"Get weather :"</span> + month_name)</div><div class="line">            	</div><div class="line">        day_weather = np.array(day_weather).reshape(<span class="number">-1</span>, <span class="number">8</span>)</div><div class="line">        day_weather = DataFrame(day_weather, columns=[</div><div class="line">            <span class="string">'Ultraviolet'</span>, <span class="string">'Dress'</span>, <span class="string">'Travel'</span>, <span class="string">'Comfort_level'</span>,</div><div class="line">            <span class="string">'Morning_exercise'</span>, <span class="string">'Car_wash'</span>, <span class="string">'Drying_index'</span>, <span class="string">'Breath_allergy'</span>])</div><div class="line">        all_month_weather = np.array(all_month_weather).reshape(<span class="number">-1</span>, <span class="number">6</span>)</div><div class="line">        all_month_weather = DataFrame(all_month_weather, columns=[</div><div class="line">            <span class="string">'Date'</span>, <span class="string">'Max_temp'</span>, <span class="string">'Min_temp'</span>, </div><div class="line">            <span class="string">'Whether'</span>, <span class="string">'Wind_direction'</span>, <span class="string">'Wind_power'</span>])</div><div class="line">        print(all_month_weather.shape)</div><div class="line">        print(day_weather.shape)</div><div class="line">        result_weather = pd.merge(</div><div class="line">            all_month_weather, day_weather, left_index=<span class="keyword">True</span>, right_index=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> result_weather</div><div class="line">	</div><div class="line">history_weather = History_weather()</div><div class="line">begin_url = <span class="string">'http://lishi.tianqi.com/yangzhong/index.html'</span></div><div class="line">history_weather.begin_url(begin_url)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>打开<code>all_weather.csv</code>文件：</p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Max_temp</th>
<th>Min_temp</th>
<th>Whether</th>
<th>…</th>
<th>Breath_allergy</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017-04-01</td>
<td>18</td>
<td>8</td>
<td>晴</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>2017-04-02</td>
<td>22</td>
<td>10</td>
<td>晴</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>2017-04-03</td>
<td>24</td>
<td>12</td>
<td>多云</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>2014-11-30</td>
<td>10</td>
<td>2</td>
<td>多云转晴</td>
<td>…</td>
<td>极不易发</td>
</tr>
</tbody>
</table>
<p>共得到910天、14项天气特征数据。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>以上只是简单的调用Requests和Beautiful Soup来进行爬取，是一个非常简单的爬虫实现，后面我们会学习更加高阶的爬虫方法。</p>
<p>爬虫源代码可以在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/Web%20Crawler/history%20weather" target="_blank" rel="external">Github</a>上下载。</p>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（三）：BeautifulSoup基础]]></title>
      <url>/2017/06/15/Crawler_3/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Beautiful Soup库以及一些基础用法，并主要介绍了<code>find_all()</code>和<code>find()</code>    两个搜索方法。利用Beautiful Soup我们可以很方便的从页面中提取信息，该库兼容python2和3版本。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Beautiful-Soup介绍"><a href="#Beautiful-Soup介绍" class="headerlink" title="Beautiful Soup介绍"></a>Beautiful Soup介绍</h1><p>我们之前已经用Requests获得了需要的页面，接下来需要从页面中提取出我们想要的信息，我们可以选择用正则表达式，但对于很多新手来说正则表达式使用的不熟练会产生很多不必要的麻烦。而对于很多任务来说，有一个工具即高效又便捷，它就是Beautiful Soup。</p>
<p>Beautiful Soup是一个可以从HTML或XML文件中提取数据的Python库，它能够通过你喜欢的转换器实现惯用的文档导航，查找，修改文档的方式。因为BeautifulSoup用法太多，本文仅介绍在爬虫中常用的功能，如果需要了解更多，请查阅官方文档。</p>
<p>文章中代码输出结果用#表示。</p>
<p><br></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>下面一段HTML代码将作为我们的例子反复使用：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">html_doc = """</div><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">title</span>&gt;</span>The Dormouse's story<span class="tag">&lt;/<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span><span class="tag">&lt;<span class="name">b</span>&gt;</span>The Dormouse's story<span class="tag">&lt;/<span class="name">b</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"story"</span>&gt;</span>Once upon a time there were three little sisters; and their names were</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/elsie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link1"</span>&gt;</span>Elsie<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/lacie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link2"</span>&gt;</span>Lacie<span class="tag">&lt;/<span class="name">a</span>&gt;</span> and</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/tillie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link3"</span>&gt;</span>Tillie<span class="tag">&lt;/<span class="name">a</span>&gt;</span>;</div><div class="line">and they lived at the bottom of a well.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"story"</span>&gt;</span>...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">"""</div></pre></td></tr></table></figure>
<p>使用BeautifulSoup解析这段代码，能够得到一个<code>BeautifulSoup</code>的对象，并能按照标准的缩进格式的结构输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc)</div><div class="line">	</div><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;title&gt;</span></div><div class="line"><span class="comment">#    The Dormouse's story</span></div><div class="line"><span class="comment">#   &lt;/title&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="title"&gt;</span></div><div class="line"><span class="comment">#    &lt;b&gt;</span></div><div class="line"><span class="comment">#     The Dormouse's story</span></div><div class="line"><span class="comment">#    &lt;/b&gt;</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;</span></div><div class="line"><span class="comment">#     Elsie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ,</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Lacie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    and</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/tillie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Tillie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ; and they lived at the bottom of a well.</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    ...</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="安装-Beautiful-Soup"><a href="#安装-Beautiful-Soup" class="headerlink" title="安装 Beautiful Soup"></a>安装 Beautiful Soup</h1><p>Beautiful Soup有3和4两个版本，3版本目前已经停止开发，建议使用4版本。</p>
<p>我们可以很方便的使用pip进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install beautifulsoup4</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="安装解析器"><a href="#安装解析器" class="headerlink" title="安装解析器"></a>安装解析器</h1><p>Beautiful Soup支持Python标准库中的HTML解析器，还支持一些第三方的解析器，下表列出了主要的解析器，以及它们的优缺点：</p>
<table>
<thead>
<tr>
<th>解析器</th>
<th>使用方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python标准库</td>
<td><code>BeautifulSoup(markup, &quot;html.parser&quot;)</code></td>
<td>Python的内置标准库执行速度适中文档容错能力强</td>
<td>Python 2.7.3 or 3.2.2前的版本中文档容错能力差</td>
</tr>
<tr>
<td>lxml HTML 解析器</td>
<td><code>BeautifulSoup(markup, &quot;lxml&quot;)</code></td>
<td>速度快文档容错能力强</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>lxml XML 解析器</td>
<td><code>BeautifulSoup(markup, [&quot;lxml&quot;, &quot;xml&quot;])``BeautifulSoup(markup, &quot;xml&quot;)</code></td>
<td>速度快唯一支持XML的解析器</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>html5lib</td>
<td><code>BeautifulSoup(markup, &quot;html5lib&quot;)</code></td>
<td>最好的容错性以浏览器的方式解析文档生成HTML5格式的文档</td>
<td>速度慢不依赖外部扩展</td>
</tr>
</tbody>
</table>
<p>这里推荐使用lxml作为解析器，因为它的效率更高。</p>
<p>可以用pip来安装lxml：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install lxml</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="使用Beautiful-Soup"><a href="#使用Beautiful-Soup" class="headerlink" title="使用Beautiful Soup"></a>使用Beautiful Soup</h1><p>将一段文档传入BeautifulSoup 的构造方法，就能得到一个文档的对象，我们可以制定解析器。例如，我们将开头的例子传入BeautifulSoup，并使用lxml解析器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure>
<p>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种： <code>Tag</code> , <code>NavigableString</code> , <code>BeautifulSoup</code> , <code>Comment</code> 。</p>
<p><br></p>
<p>我们重点讲两个搜索方法：<code>find()</code>和<code>find_all()</code>，我们用这两种方法可以根据HTML文档的标签快速找到我们需要的内容。</p>
<p>以开头提到的文档作为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">html_doc = <span class="string">"""</span></div><div class="line">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</div><div class="line">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</div><div class="line">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</div><div class="line">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</div><div class="line">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</div><div class="line">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</div><div class="line">and they lived at the bottom of a well.&lt;/p&gt;</div><div class="line">&lt;p class="story"&gt;...&lt;/p&gt;</div><div class="line">"""</div><div class="line">	</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc,<span class="string">'lxml'</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="find-all"><a href="#find-all" class="headerlink" title="find_all()"></a>find_all()</h2><p><strong>find_all(name, attrs, recursive, text, kwargs)</strong></p>
<h3 id="name-参数"><a href="#name-参数" class="headerlink" title="name 参数"></a>name 参数</h3><p><code>name</code>参数可以查找所有名字为 <code>name</code> 的tag：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div></pre></td></tr></table></figure>
<p><code>name</code>参数的值可以是字符串、正则表达式、列表、方法或是 <code>True</code></p>
<p>例如：</p>
<p>在<code>find_all()</code>中传入一个<strong>字符串</strong>参数，Beautiful Soup会查找与字符串完整匹配的内容，下面的例子用于查找文档中所有的<code>&lt;b&gt;</code>标签:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'b'</span>)</div><div class="line"><span class="comment"># [&lt;b&gt;The Dormouse's story&lt;/b&gt;]</span></div></pre></td></tr></table></figure>
<p>如果传入<strong>列表</strong>参数，Beautiful Soup会将与列表中任一元素匹配的内容返回。下面代码找到文档中所有<code>&lt;a&gt;</code>标签和<code>&lt;b&gt;</code>标签:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all([<span class="string">"a"</span>, <span class="string">"b"</span>])</div><div class="line"><span class="comment"># [&lt;b&gt;The Dormouse's story&lt;/b&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><code>find_all()</code>方法搜索当前tag的所有tag子节点，并判断是否符合过滤器的条件。</p>
<p><br></p>
<h3 id="keyword-参数"><a href="#keyword-参数" class="headerlink" title="keyword 参数"></a>keyword 参数</h3><p>1、如果一个指定名字的参数不是搜索内置的参数名，搜索时会把该参数当作指定名字tag的属性来搜索，如果包含一个名字为 <code>id</code> 的参数，Beautiful Soup会搜索每个tag的”id”属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(id=<span class="string">'link2'</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><br></p>
<p>2、如果传入 <code>href</code> 参数，Beautiful Soup会搜索每个tag的”href”属性:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(href=re.compile(<span class="string">"elsie"</span>))</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><br></p>
<p>3、利用<code>true</code>参数在文档树中查找所有包含 <code>id</code> 属性的tag，无论 <code>id</code> 的值是什么:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.find_all(id=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p>搜索指定名字的属性时可以使用的参数值包括字符串、正则表达式、列表或是 <code>True</code></p>
<p><br></p>
<h3 id="按CSS搜索"><a href="#按CSS搜索" class="headerlink" title="按CSS搜索"></a>按CSS搜索</h3><p>按照CSS类名搜索tag的功能非常实用，但标识CSS类名的关键字 <code>class</code> 在Python中是保留字，使用 <code>class</code> 做参数会导致语法错误。这里我们可以通过 <code>class_</code> 参数搜索有指定CSS类名的tag:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>, class_=<span class="string">"sister"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><code>class_</code> 参数同样接受不同类型的过滤器：字符串、正则表达式、方法或 <code>True</code> 。</p>
<p><br></p>
<h3 id="text-参数"><a href="#text-参数" class="headerlink" title="text 参数"></a>text 参数</h3><p>通过 <code>text</code> 参数可以搜搜文档中的字符串内容。与 <code>name</code> 参数的可选值一样，<code>text</code> 参数接受字符串、正则表达式、方法或 <code>True</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">soup.find_all(text=<span class="string">"Elsie"</span>)</div><div class="line"><span class="comment"># [u'Elsie']</span></div><div class="line">	</div><div class="line">soup.find_all(text=[<span class="string">"Tillie"</span>, <span class="string">"Elsie"</span>, <span class="string">"Lacie"</span>])</div><div class="line"><span class="comment"># [u'Elsie', u'Lacie', u'Tillie']</span></div><div class="line">	</div><div class="line">soup.find_all(text=re.compile(<span class="string">"Dormouse"</span>))</div><div class="line"><span class="comment"># [u"The Dormouse's story", u"The Dormouse's story"]</span></div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_the_only_string_within_a_tag</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="string">""</span>Return <span class="keyword">True</span> <span class="keyword">if</span> this string <span class="keyword">is</span> the only child of its parent tag.<span class="string">""</span></div><div class="line">    <span class="keyword">return</span> (s == s.parent.string)</div><div class="line">	</div><div class="line">soup.find_all(text=is_the_only_string_within_a_tag)</div><div class="line"><span class="comment"># [u"The Dormouse's story", u"The Dormouse's story", u'Elsie', u'Lacie', u'Tillie', u'...']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h3><p>下面两行代码是等价的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>)</div><div class="line">soup(<span class="string">"a"</span>)</div></pre></td></tr></table></figure>
<p>这两行也是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.title.find_all(text=<span class="keyword">True</span>)</div><div class="line">soup.title(text=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="find"><a href="#find" class="headerlink" title="find()"></a>find()</h2><p><strong>find(name, attrs, recursive, text, kwargs)</strong></p>
<p><code>find_all()</code> 方法将返回文档中符合条件的所有tag，尽管有时候我们只想得到一个结果.比如文档中只有一个<code>&lt;body&gt;</code>标签，那么使用 <code>find_all()</code>方法来查找<code>&lt;body&gt;</code>标签就不太合适， 使用 <code>find_all</code> 方法并设置 <code>limit=1</code> 参数不如直接使用 <code>find()</code> 方法。下面两行代码是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'title'</span>, limit=<span class="number">1</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div><div class="line">	</div><div class="line">soup.find(<span class="string">'title'</span>)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure>
<p>唯一的区别是 <code>find_all()</code> 方法的返回结果是值包含一个元素的列表,而 <code>find()</code> 方法直接返回结果.</p>
<p><code>find_all()</code> 方法没有找到目标是返回空列表， <code>find()</code> 方法找不到目标时返回 <code>None</code>。</p>
<p><br></p>
<h3 id="简写-1"><a href="#简写-1" class="headerlink" title="简写"></a>简写</h3><p><code>soup.head.title</code>是利用tag的名字来简写，这个简写的原理就是多次调用当前tag的<code>find()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.head.title</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div><div class="line">	</div><div class="line">soup.find(<span class="string">"head"</span>).find(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="输出文本"><a href="#输出文本" class="headerlink" title="输出文本"></a>输出文本</h1><p>如果只想得到tag中包含的文本内容，那么可以采用<code>get_text()</code>方法，这个方法获取到tag中包含的所有文版内容，包括子孙tag中的内容，并将结果作为Unicode字符串返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">'&lt;a href="http://example.com/"&gt;\nI linked to &lt;i&gt;example.com&lt;/i&gt;\n&lt;/a&gt;'</span></div><div class="line">soup = BeautifulSoup(markup)</div><div class="line">	</div><div class="line">soup.get_text()</div><div class="line"><span class="comment"># u'\nI linked to example.com\n'</span></div><div class="line">soup.i.get_text()</div><div class="line"><span class="comment"># u'example.com'</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="关于编码"><a href="#关于编码" class="headerlink" title="关于编码"></a>关于编码</h1><p>使用Beautiful Soup解析后，文档都被转换成了Unicode。</p>
<p>Beautiful Soup用了<strong>编码自动检测</strong>子库来识别当前文档编码并转换成Unicode编码。<code>BeautifulSoup</code>对象的<code>.original_encoding</code>属性记录了自动识别编码的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">"&lt;h1&gt;Sacr\xc3\xa9 bleu!&lt;/h1&gt;"</span></div><div class="line">soup = BeautifulSoup(markup,<span class="string">'lxml'</span>)</div><div class="line">soup.original_encoding</div><div class="line"><span class="comment"># 'utf-8'</span></div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p>如果先用<code>r = requests.get(url)</code>指令获得了html页面，建议先用指令<code>r.encoding</code>判断<code>r.text</code>所用的编码方式和源html是否相同（可以通过查看html中的charset= 来判断源html采用的编码方式），如果不同的话要用<code>r.encoding=</code>进行修改，再用<code>soup = BeautifulSoup(r.text,&#39;lxml&#39;)</code>命令。因为<code>r.text</code>会根据当前的编码方式把html转化为Unicode编码输出，如果html包含汉字，而没有采用<code>utf-8</code>进行编码的话，<code>r.text</code>没有办法把汉字转化为相应的Unicode编码，再送进Beautiful Soup中也就无法得到汉字信息了。</p>
<p>也可以直接用<code>soup = BeautifulSoup(r.content,&#39;lxml&#39;)</code>指令，因为<code>r.content</code>返回的是bytes型原始编码信息，而Beautiful Soup可以用<strong>编码自动检测</strong>子库来识别当前文档编码并转换成Unicode编码，但获取文本信息时不建议这样使用，一般来说图像等多媒体内容用content更多一些。</p>
<p><br></p>
<p>而通过Beautiful Soup<strong>输出文档</strong>时，不管输入文档是什么编码方式，输出编码均为UTF-8编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">b'''</span></div><div class="line">&lt;html&gt;</div><div class="line">  &lt;head&gt;</div><div class="line">    &lt;meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" /&gt;</div><div class="line">  &lt;/head&gt;</div><div class="line">  &lt;body&gt;</div><div class="line">    &lt;p&gt;Sacr\xe9 bleu!&lt;/p&gt;</div><div class="line">  &lt;/body&gt;</div><div class="line">&lt;/html&gt;</div><div class="line">'''</div><div class="line">	</div><div class="line">soup = BeautifulSoup(markup)</div><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;meta content="text/html; charset=utf-8" http-equiv="Content-type" /&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p&gt;</span></div><div class="line"><span class="comment">#    Sacré bleu!</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure>
<p>如果不想用UTF-8编码输出，可以将编码方式传入<code>prettify()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(soup.prettify(<span class="string">"latin-1"</span>))</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;meta content="text/html; charset=latin-1" http-equiv="Content-type" /&gt;</span></div><div class="line"><span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>关于Beautiful Soup还有很多其他用法，这里只是重点介绍了运用最多的两个搜索方法<code>find_all()</code>和<code>find()</code>，如果想了解更多的使用方法，请参考官方文档。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>官方文档：<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html#" target="_blank" rel="external">Beautiful Soup Documentation — Beautiful Soup 4.4.0 documentation</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（二）：Requests基础]]></title>
      <url>/2017/06/14/Crawler_2/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Requests库以及一些基础用法，基于python3.5版本<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Requests介绍"><a href="#Requests介绍" class="headerlink" title="Requests介绍"></a>Requests介绍</h1><p>Requests是一个基于urllib3的HTTP库，简单优美，支持python2.6–3.5的各版本，具有无比强大的功能，完全满足今日web的需求。在爬虫学习过程中，我们用它向服务器发送请求，以获取我们需要的信息。</p>
<p><br></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>只需要在终端中运行以下代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install requests</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>我们先来看一个具体的实例来感受下Request：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'http://zangbo.me'</span>, auth=(<span class="string">'user'</span>, <span class="string">'pass'</span>))</div><div class="line">r.status_code</div><div class="line">r.headers[<span class="string">'content-type'</span>]</div><div class="line">r.encoding</div><div class="line">r.text</div></pre></td></tr></table></figure>
<p>以下是输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">200</span></div><div class="line"><span class="string">'text/html; charset=utf-8'</span></div><div class="line"><span class="string">'utf-8'</span></div><div class="line"><span class="string">'&lt;!DOCTYPE html&gt;'</span></div></pre></td></tr></table></figure>
<p>以上代码我们请求了本站的地址，然后依次打印出状态码，请求头，编码方式和相应内容。</p>
<p>是不是功能强大，下面，我们正式开始学习Requests。</p>
<p><br></p>
<h1 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h1><p>首先导入Requests模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div></pre></td></tr></table></figure>
<p>Requests包含各种不同的请求方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>)</div><div class="line">r = requests.put(<span class="string">"http://httpbin.org/put"</span>)</div><div class="line">r = requests.delete(<span class="string">"http://httpbin.org/delete"</span>)</div><div class="line">r = requests.head(<span class="string">"http://httpbin.org/get"</span>)</div><div class="line">r = requests.options(<span class="string">"http://httpbin.org/get"</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get请求</h2><p>我们用的最多的是get请求，用来获取某个网页：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</div></pre></td></tr></table></figure>
<p>现在，我们有一个名叫r的Response对象，它包含了所有关于该页面的信息，我们可以从中提取出我们想要的信息。</p>
<p><br></p>
<h2 id="带参数的get请求"><a href="#带参数的get请求" class="headerlink" title="带参数的get请求"></a>带参数的get请求</h2><p>如果你是手工构建 URL，那么数据会以键/值对的形式置于 URL 中，跟在一个问号的后面，我们可以用字典的形式传递URL参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</div><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=payload)</div><div class="line">print(r.url)</div></pre></td></tr></table></figure>
<p>可以得到输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http://httpbin.org/get?key2=value2&amp;key1=value1</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h2><p>如果我们想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。只需简单地传递一个字典给 data 参数，该字典在发出请求时会自动编码为表单形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=payload)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  ...</div><div class="line">  <span class="string">"form"</span>: &#123;</div><div class="line">    <span class="string">"key2"</span>: <span class="string">"value2"</span>,</div><div class="line">    <span class="string">"key1"</span>: <span class="string">"value1"</span></div><div class="line">  &#125;,</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="获取响应内容"><a href="#获取响应内容" class="headerlink" title="获取响应内容"></a>获取响应内容</h1><h2 id="响应状态码"><a href="#响应状态码" class="headerlink" title="响应状态码"></a>响应状态码</h2><p>我们可以检测发送请求是否成功获得想要的数据，通过检测响应状态码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</div><div class="line">r.status_code</div></pre></td></tr></table></figure>
<p>若输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">200</div></pre></td></tr></table></figure>
<p>表明获取成功。</p>
<p><br></p>
<h2 id="文本内容"><a href="#文本内容" class="headerlink" title="文本内容"></a>文本内容</h2><p>以百度首页为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">"https://www.baidu.com"</span>)</div><div class="line">r.text</div></pre></td></tr></table></figure>
<p>Requests会基于HTTP的头部对响应的编码进行推测，让我们访问<code>r.text</code>时，Requests会使用其推测的文本编码，我们可以用<code>r.encoding</code>改变解码方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">r.encoding</div><div class="line">r.encoding = <span class="string">'ISO-8859-1'</span></div><div class="line">r.encoding</div></pre></td></tr></table></figure>
<p>以上代码输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="string">'utf-8'</span></div><div class="line"><span class="string">'ISO-8859-1'</span></div></pre></td></tr></table></figure>
<p>如果你改变了编码，每当你访问 <code>r.text</code> ，Request 都将会使用 <code>r.encoding</code> 的新值。当不确定时，可以使用 <code>r.content</code> 来找到编码，然后设置 <code>r.encoding</code> 为相应的编码。这样就能使用正确的编码解析 <code>r.text</code> 了。关于<code>r.content</code>下面会介绍。</p>
<p><br></p>
<h2 id="二进制内容"><a href="#二进制内容" class="headerlink" title="二进制内容"></a>二进制内容</h2><p>对于非文本请求，例如图片等多媒体内容，可以以字节的方式访问：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r.content</div></pre></td></tr></table></figure>
<p>这里注意两者的区别：</p>
<ul>
<li>r.text返回的是Unicode型的数据。如果是读取文本，可以通过r.text。</li>
<li>r.content返回的是bytes型也就是二进制的数据。如果读取图片，文件，可以通过r.content来实现。</li>
</ul>
<p>例如我们下载并保存一张图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">jpg_url = <span class="string">'http://mm.howkuai.com/wp-content/uploads/2017a/04/13/01.jpg'</span><span class="comment">#一张美女图</span></div><div class="line">content = requests.get(jpg_url).content</div><div class="line"><span class="keyword">with</span> open(<span class="string">'image.jpg'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f: <span class="comment">#写入多媒体文件必须要 b 这个参数！</span></div><div class="line">    f.write(content)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="JSON内容"><a href="#JSON内容" class="headerlink" title="JSON内容"></a>JSON内容</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'https://github.com/timeline.json'</span>)</div><div class="line">r.json()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="响应头"><a href="#响应头" class="headerlink" title="响应头"></a>响应头</h2><p>通过命令可以查看服务器响应头，以一个Python字典形式展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'http://zangbo.me'</span>)</div><div class="line">r.headers</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">	<span class="string">'Server'</span>: <span class="string">'GitHub.com'</span>, </div><div class="line">	<span class="string">'Date'</span>: <span class="string">'Thu, 15 Jun 2017 04:04:47 GMT'</span>, </div><div class="line">	<span class="string">'Content-Type'</span>: <span class="string">'text/html; charset=utf-8'</span>, </div><div class="line">	<span class="string">'Transfer-Encoding'</span>: <span class="string">'chunked'</span>, </div><div class="line">	<span class="string">'Last-Modified'</span>: <span class="string">'Wed, 14 Jun 2017 15:41:23 GMT'</span>, </div><div class="line">	<span class="string">'Access-Control-Allow-Origin'</span>: <span class="string">'*'</span>, </div><div class="line">	<span class="string">'Expires'</span>: <span class="string">'Thu, 15 Jun 2017 04:14:47 GMT'</span>, </div><div class="line">	<span class="string">'Cache-Control'</span>: <span class="string">'max-age=600'</span>, </div><div class="line">	<span class="string">'Content-Encoding'</span>: <span class="string">'gzip'</span>, </div><div class="line">	<span class="string">'X-GitHub-Request-Id'</span>: <span class="string">'E1D6:0B0F:15941BE:1E637E3:5942075F'</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="获取响应内容总结"><a href="#获取响应内容总结" class="headerlink" title="获取响应内容总结"></a>获取响应内容总结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">r.status_code<span class="comment">#获取状态码</span></div><div class="line">r.text<span class="comment">#文本内容</span></div><div class="line">r.encoding<span class="comment">#查看编码</span></div><div class="line">r.encoding = <span class="string">'ISO-8859-1'</span><span class="comment">#改变编码</span></div><div class="line">r.content<span class="comment">#二进制内容</span></div><div class="line">r.json()<span class="comment">#JSON内容</span></div><div class="line">r.raw<span class="comment">#原始内容</span></div><div class="line">r.headers<span class="comment">#响应头</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="定制请求头"><a href="#定制请求头" class="headerlink" title="定制请求头"></a>定制请求头</h1><p>有些网站具有反爬虫机制，我们有时候需要把爬虫伪装成浏览器来访问网页，因此我们需要为请求添加HTTP头部，只需要简单地传递一个<code>dict</code>给<code>headers</code>参数即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">url = <span class="string">"https://www.baidu.com"</span></div><div class="line">headers = &#123;<span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8'</span>&#125;</div><div class="line">r = requests.get(url,headers=headers)</div></pre></td></tr></table></figure>
<p>注：如何查看自己浏览器的userAgent</p>
<p>打开浏览器的控制台，输入：javascript:alert(navigator.userAgent)</p>
<p><br></p>
<h1 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h1><p>我们可以告诉 requests 在经过以 <code>timeout</code> 参数设定的秒数时间之后停止等待响应：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">requests.get(<span class="string">'http://github.com'</span>, timeout=<span class="number">0.001</span>)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p><code>timeout</code> 仅对连接过程有效，与响应体的下载无关。 <code>timeout</code> 并不是整个下载响应的时间限制，而是如果服务器在 <code>timeout</code> 秒内没有应答，将会引发一个异常（更精确地说，是在 <code>timeout</code> 秒内没有从基础套接字上接收到任何字节的数据时）</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>本文基于Requests官方文档：</p>
<ul>
<li>英文版：<a href="http://docs.python-requests.org/en/master/" target="_blank" rel="external">Requests: HTTP for Humans</a></li>
<li>中文版：<a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">Requests: 让HTTP服务人类</a></li>
</ul>
<p>本文仅介绍基础用法，更高阶以及更详细的内容（Cookie、重定向与请求历史、错误与异常等）请参考官方文档。</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（一）：爬虫基础知识]]></title>
      <url>/2017/06/13/Crawler_1/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本系列记录了爬虫学习的历程，本文整理了学习爬虫需要掌握的基础背景知识，包括HTML、URL在内的基础概念以及爬虫的工作原理。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h1><p>HTML中文全称“超文本标记语言”。超文本指页面内可以包含图片、链接，甚至音乐、程序等非文字元素。</p>
<p>超文本标记语言包括Head部分和Body部分，其中Head部分提供关于网页的信息，Body部分提供网页的具体内容。</p>
<ul>
<li><code>&lt;head&gt;&lt;/head&gt;；</code>这2个标记符分别表示头部信息的开始和结尾。头部中包含的标记是页面的标题、序言、说明等内容，它本身不作为内容来显示，但影响网页显示的效果。</li>
<li><code>&lt;body&gt;&lt;/body&gt;；</code>网页中显示的实际内容均包含在这2个正文标记符之间。正文标记符又称为实体标记。</li>
</ul>
<p><br></p>
<h1 id="URI"><a href="#URI" class="headerlink" title="URI"></a>URI</h1><p>统一资源标识符(Uniform Resource Identifier)，是一个用于标识某一互联网资源名称的字符串。Web上可用的每种资源：HTML文档、图像、视频片段、程序等，由一个URI进行定位。</p>
<p>URI由三部分组成：</p>
<ol>
<li>访问资源的命名机制</li>
<li>存放资源的主机名</li>
<li>资源自身的名称，由路径表示，着重强调于资源。</li>
</ol>
<p><br></p>
<h1 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h1><p>统一资源定位符(Uniform Resource Location)，是URI的一个子集，URI确定一个资源，URL不仅确定一个资源，还告诉你它在哪里。互联网上每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。</p>
<p>URL一般由三部组成：</p>
<ol>
<li>协议(或称为服务方式)</li>
<li>存有该资源的服务器名称或者主机IP地址(有时也包括端口号)</li>
<li>主机资源的具体地址。如目录和文件名等。</li>
</ol>
<p>以下是一个例子：</p>
<p><code>https://tianchi.aliyun.com/competition/information.htm?raceId=231602</code></p>
<p>协议://授权/路径?查询</p>
<ul>
<li>第一部分和第二部分用“://”符号隔开，</li>
<li>第二部分和第三部分用“/”符号隔开。</li>
<li>第一部分和第二部分是不可缺少的，第三部分有时可以省略。</li>
</ul>
<p>URI表示请求服务器的路径，定义这么一个资源，而URL同时说明要如何访问这个资源（通过http://）</p>
<p><br></p>
<h1 id="浏览网页的过程和爬虫原理"><a href="#浏览网页的过程和爬虫原理" class="headerlink" title="浏览网页的过程和爬虫原理"></a>浏览网页的过程和爬虫原理</h1><p>浏览器作为一个客户端，输入地址后，会向服务器端发送一次请求(Request)，服务器正常的话会返回一个响应(Response)，响应的内容是请求打开的页面内容。浏览器收到相应后，对信息进行相应处理，页面就显示在我们面前了。</p>
<p>爬虫的流程相似，我们通过http库向目标站点发送一个Request(里面包含URL)，如果服务器响应，会得到一个Response，我们在利用正则表达式等工具对它进行解析，最后保存数据。</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[编码相关知识]]></title>
      <url>/2017/06/13/EncodingKnowledge/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文整理了一些常见的编码，它们的起源、区分以及在Python 3中的使用，因为平时被各种不同的编码所困扰，索性就整理出来。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="编码的起源"><a href="#编码的起源" class="headerlink" title="编码的起源"></a>编码的起源</h1><p>在计算机中，所有的数据在存储和运算时都要使用二进制数表示（因为计算机用高电平和低电平分别表示1和0），例如像a、b、c、d这样的52个字母（包括大写）、以及0、1等数字还有一些常用的符号（例如*、#、@等）在计算机中存储时也要使用二进制数来表示，而具体用哪些二进制数字表示哪个符号，当然每个人都可以约定自己的一套（这就叫编码），而大家如果要想互相通信而不造成混乱，那么大家就必须使用相同的编码规则。</p>
<p><br></p>
<h1 id="ASCII"><a href="#ASCII" class="headerlink" title="ASCII"></a>ASCII</h1><p>为了有一套统一的编码规则，美国国家标准协会就出台了ASCII编码，统一规定了上述常用符号用哪些二进制数来表示，后来被国际标准化组织（ISO）定位了国际标准。</p>
<p>标准ASCII码使用指定的7位二进制数组合来表示128种可能的字符，而由于电脑开发初期就建立了8位元位元组，即1byte=8bits，因此如果使用一个位元组来保存字元，则可以存储256个码，于是就诞生了扩展ASCII码。</p>
<p><br></p>
<h1 id="ISO8859-1"><a href="#ISO8859-1" class="headerlink" title="ISO8859-1"></a>ISO8859-1</h1><p>之前说过，标准ASCII是针对英语设计的，当处理带有音调标号（形如汉语的拼音）的亚洲文字时就会出现问题。因此，创建出了一些包括255个字符的由ASCII扩展的字符集。</p>
<p>其中一种扩展的8位字符集是ISO 8859-1 Latin 1，也简称为ISOLatin-1。它把位于128-255之间的字符用于拉丁字母表中特殊语言字符的编码，也因此而得名。ISOLatin-1向下兼容ASCII。</p>
<p><br></p>
<h1 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h1><p>然而欧洲语言不是地球上的唯一语言，亚洲和非洲语言并不能被8位字符集所支持。仅汉语字母表就有80000以上个字符。但是把汉语、日语和越南语的一些相似的字符结合起来，在不同的语言里，使不同的字符代表不同的字，这样只用2个字节就可以编码地球上几乎所有地区的文字。因此，创建了Unicode编码，它通过增加一个高字节对ISO Latin-1字符集进行扩展，当这些高字节位为0时，低字节就是ISO Latin-1字符。最初的Unicode标准UCS-2是使用两个字节表示一个字符，后来有人嫌少使用4个字节表示一个字符，即UCS-4，不过我们用的最多的还是UCS-2。</p>
<p>总的来说，Unicode是计算机科学领域里的一项业界标准，包括字符集、编码方案等。Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。</p>
<p><br></p>
<h1 id="UTF"><a href="#UTF" class="headerlink" title="UTF"></a>UTF</h1><p>对可以用ASCII表示的字符使用Unicode并不高效，因为Unicode比ASCII占用大一倍的空间，而对ASCII来说高字节的0对他毫无用处。为了解决这个问题，就出现了通用转换格式，即UTF（Unicode Transformation Format）。</p>
<center><img src="http://i1.buimg.com/597140/576550baa99295f6.png" alt="Markdown"></center>

<p>换句话说，在计算机<strong>内存</strong>中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，是由UTF负责的。最简单的一种形式是直接用UCS的码位来保存，这就是UTF-16，比如，”汉”直接使用\x6C\x49保存(UTF-16-BE)，或是倒过来使用\x49\x6C保存(UTF-16-LE)。但用着用着美国人觉得自己吃了大亏，以前英文字母只需要一个字节就能保存了，现在大锅饭一吃变成了两个字节，空间消耗大了一倍……于是UTF-8横空出世。</p>
<p><strong>UTF-8</strong>是一种可变长度字符编码。它用1到6个字节编码Unicode字符。用在网页上可以统一页面显示中文简体繁体及其它语言（如英文，日文，韩文）。</p>
<p>此外，常见的UTF格式还有：UTF-7、UTF-7.5、UTF-8、UTF-16、UTF-32</p>
<p><br></p>
<h2 id="Python-3中的编码"><a href="#Python-3中的编码" class="headerlink" title="Python 3中的编码"></a>Python 3中的编码</h2><p>在python 3版本中，字符串是以Unicode编码的，也就是说，python 3字符串支持中英或其他各种语言。此外，我们可以利用<code>.encode()</code>函数来把Unicode字符串进行编码，编码后的字符串前面会用b表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">u = <span class="string">'汉'</span></div><div class="line">print(u)</div><div class="line">s = u.encode(<span class="string">'UTF-8'</span>)</div><div class="line">print(s)</div><div class="line">d = s.decode(<span class="string">'UTF-8'</span>)</div><div class="line">print(d)</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">'汉'</span></div><div class="line">b<span class="string">'\xe6\xb1\x89'</span></div><div class="line"><span class="string">'汉'</span></div></pre></td></tr></table></figure>
<p>关于python 3中读取文件：</p>
<blockquote>
<p>文件总是存储为编码好的数据，因此，为了使用文件中读取的文本数据，必须首先将其解码为一个Unicode字符串。python 3中，文本正常情况下会自动为你解码，所以打开或读取文件会得到一个Unicode字符串。使用的解码方式取决系统，在Mac OS 或者 大多数Linux系统中，首选编码是UTF-8，但Windows不一定。可以使用locale.getpreferredencoding()方法得到系统的默认解码方式。</p>
</blockquote>
<p>注意：python 2和python 3在编码方面具有较大差别，具体可以查阅相关资料，因为我平时只用python 3，所以这里就不提供了。</p>
<p><br></p>
<h1 id="GB2312"><a href="#GB2312" class="headerlink" title="GB2312"></a>GB2312</h1><p>汉字编码字符集，GB2312编码适用于汉字处理、汉字通信等系统之间的信息交换，通行于中国大陆；新加坡等地也采用此编码。中国大陆几乎所有的中文系统和国际化的软件都支持GB 2312。</p>
<p><br></p>
<h1 id="GBK"><a href="#GBK" class="headerlink" title="GBK"></a>GBK</h1><p>GBK全称《汉字内码扩展规范》，向下与 GB 2312 编码兼容，使用了双字节编码方案，共23940个码位，共收录了21003个汉字。</p>
<p><br></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在计算机<strong>内存</strong>中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，是由UTF负责的，所以在很多网页的头文件可以看到编码方式utf-8，这表明该网页使用utf-8编码的。</p>
<p>另外在编程语言的使用过程中，切记关注编码方式是很重要的一件事。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://www.cnblogs.com/284628487a/p/5584714.html" target="_blank" rel="external">Python3 字符编码 - Coder25 - 博客园</a></li>
<li>百度百科</li>
</ul>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 计算机基础 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[配置GithubPage二级域名]]></title>
      <url>/2017/06/12/GithubPageDomainName/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>因为女友生日做了两个网站，就思考该如何把它们放在GithubPage下，并且设置成二级域名，查阅了一堆资料终于明白GithubPage二级域名该如何配置，于是整理在下面。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Github-Page种类"><a href="#Github-Page种类" class="headerlink" title="Github Page种类"></a>Github Page种类</h1><p>1、<code>UserPage</code>: 用户的整个站点，这个是最初github支持的类型，创建一个形如username.github.com的项目就可以，用户想要在github上托管网站，这个是必须要建的，而且命名方式固定。</p>
<p>2、<code>ProjectPage</code>: 用户创建出来的其他网站项目也可以托管在github上，首先新建一个项目，然后建立一个名叫<code>gh-pages</code>的branch，这个branch里的文件就是page的站点文件。</p>
<p><br></p>
<h1 id="UserPage默认域名"><a href="#UserPage默认域名" class="headerlink" title="UserPage默认域名"></a>UserPage默认域名</h1><p>用户站点的默认域名是<code>username.github.io</code>，比如我的站点就是<code>zangbo.github.io</code></p>
<p><br></p>
<h1 id="ProjectPage默认域名"><a href="#ProjectPage默认域名" class="headerlink" title="ProjectPage默认域名"></a>ProjectPage默认域名</h1><p>其他网站项目的默认域名，是使用UserPage域名加上二级目录实现的，比如我有个项目叫<code>mylove</code>，那么该项目的站点就是访问 <code>zangbo.github.io/mylove</code></p>
<p><br></p>
<h1 id="UserPage自定义域名"><a href="#UserPage自定义域名" class="headerlink" title="UserPage自定义域名"></a>UserPage自定义域名</h1><p>我有自己的域名，如何绑定到UserPage? 比如用<code>www.zangbo.me</code>替代<code>zangbo.github.io</code>，它是使用CNAME技术来实现的。</p>
<p>具体步骤可以参考我前些日子写的文章：《Mac环境利用GitHub和Hexo搭建个人博客》，除了添加<code>CNAME</code>指向还应添加<code>A</code>指向。</p>
<blockquote>
<p>CNAME指向之后，当浏览器访问<code>www.zangbo.me</code>的时候浏览器就知道<code>实际上</code>是访问<code>zangbo.github.io</code><br>添加CNAME 文件之后，当GithubPage服务器接收到访问<code>www.zangbo.me</code>的http请求，就知道，对应的是这个工程了。</p>
</blockquote>
<p><br></p>
<h1 id="ProjectPage自定义域名"><a href="#ProjectPage自定义域名" class="headerlink" title="ProjectPage自定义域名"></a>ProjectPage自定义域名</h1><p>比如用<code>mylove.zangbo.me</code>替代<code>zangbo.github.io/mylove</code></p>
<p>1、同样的，去域名注册商那里，做一个<code>CNAME</code>指向，将<code>mylove.zangbo.me</code> 指向 <code>zangbo.github.io</code>，如果以后会有很多二级域名都指过来，操作方式相同。</p>
<p>2、在<code>zangbo/mylove</code>这个项目(也就是page项目)根目录下建一个<code>CNAME</code>文件，里面填写<code>mylove.zangbo.me</code>，然后提交到仓库;</p>
<p>3、等几分钟。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.zhaoxiaodan.com/%E5%85%B6%E4%BB%96/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEGithubPage%E7%9A%84%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D.html" target="_blank" rel="external">如何配置GithubPage的二级域名</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 教程 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 个人网站 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[送给女友的生日礼物]]></title>
      <url>/2017/06/12/GiftForGirlfriend/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>女朋友生日来临之际，自己在别人模版基础上修改着做了两个网站当作礼物，并没有兼容手机端，最好在电脑端登录观看效果。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<div id="aplayer0" class="aplayer" style="margin-bottom: 20px;"></div>
		<script>
			new APlayer({
				element: document.getElementById("aplayer0"),
				narrow: false,
				autoplay: false,
				showlrc: 0,
				music: {
					title: "童话镇",
					author: "陈一发儿",
					url: "http://mp3.haoduoge.com/s/2016-12-24/1482568978.mp3",
					pic: "http://p4.music.126.net/tfa811GLreJI_S0h9epqRA==/3394192426154346.jpg?param=130y130",
				}
			});
		</script>
<p><br></p>
<h1 id="我的礼物"><a href="#我的礼物" class="headerlink" title="我的礼物"></a>我的礼物</h1><p>以下是我的两个网站：</p>
<ul>
<li>博莎小站：<a href="http://lisa.zangbo.me" target="_blank" rel="external">http://lisa.zangbo.me</a></li>
<li>送给Lisa的一封信：<a href="http://mylove.zangbo.me" target="_blank" rel="external">http://mylove.zangbo.me</a></li>
</ul>
<p>注：手机端打开会出现排版问题，建议电脑浏览器浏览。</p>
<p><br></p>
<h1 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a>参考网站</h1><p>第一个网站参考：<a href="http://huyuqian.chengdazhi.com" target="_blank" rel="external">治芊小站</a></p>
<p>作者主页：<a href="http://chengdazhi.com" target="_blank" rel="external">程大治</a></p>
<p>发现这个网站是在知乎问题 <a href="https://www.zhihu.com/question/37804443" target="_blank" rel="external">程序员能给女朋友做什么特别浪漫的礼物？ - 知乎</a> 下，其中不乏很多很好的创意，通过评论得知作者也是在网上找的模版，很容易就搜到了：<a href="http://www.cssmoban.com/cssthemes/6077.shtml" target="_blank" rel="external">简洁瀑布流布局动物世界博客模板</a>，于是直接进行二次加工。</p>
<p><br></p>
<p>第二个网站参考：<a href="http://hackerzhou.me/ex_love/" target="_blank" rel="external">Our Love Story</a></p>
<p>作者GitHub：<a href="https://github.com/hackerzhou" target="_blank" rel="external">hackerzhou</a></p>
<p>这个网站在圈子里可以说是很有名了，作者开发于2011年，当年送的人如今已经变成了前女友，不免一阵唏嘘。作者把代码放在GitHub上允许大家自由的进行二次开发，我便拿来进行修改，因为女朋友并非程序员，所以原本的网页中代码生成部分改为了一封信。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>因为妹子是文科生，送之前我也不知道妹子会不会喜欢这种理工男的浪漫，所以这两个网页只能算是礼物的一部分，自然还是要准备其他实质性的东西的。</p>
<p><br></p>
</the></excerpt>]]></content>
      
        
        <tags>
            
            <tag> 个人网站 </tag>
            
            <tag> 前端 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Word2Vec笔记：Negative Sample]]></title>
      <url>/2017/06/06/Word2Vec_Negative-Sample/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Word2Vec中对Skip Gram模型的改进——Negative Sample方法。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在skip-gram模型中，我们发现这是个十分庞大的神经网络，如果我们把词向量设置成300，那么在一个具有10000个单词的字典中，隐层和输出层分别具有3百万个参数。执行梯度下降算法时将会十分缓慢，更糟糕的是，我们需要事先在一个数量庞大的数据集上训练，以便于fine-tune参数和避免过拟合。因此对模型的训练将十分困难。</p>
<p>为了解决这个问题，作者提出了三个修改方案：</p>
<ol>
<li>把常见的短语或单词对当成一个单词来对待</li>
<li>降采样高频单词来见少训练样本数量</li>
<li>用一种他们称之为“负采样”的方式来修改优化目标，这将使得每个训练样本只更新一小部分的模型参数。</li>
</ol>
<p>它们均可以使得训练变的可行同时也改善了结果词向量的质量。</p>
<p><br></p>
<h1 id="单词对和短语"><a href="#单词对和短语" class="headerlink" title="单词对和短语"></a>单词对和短语</h1><p>作者指出一个单词对像“Boston Globe”(一个报纸名)，它的含义和“Boston”以及“Globe”都有着很大的区别，因此把“Boston Globe”当作一个单词来对待是很有意义的，它将会得到一个词向量来表示。在Google News数据集上，这个改变可以把模型字典尺寸从一千亿降低为三百万。Google开源了它们训练好得到的词向量，长度是300，共计三百万个。至于如何从单词中区分短语并且缩短字典尺寸，一个方法是可以参考维基百科的词条目录。</p>
<p><br></p>
<h1 id="降采样高频单词"><a href="#降采样高频单词" class="headerlink" title="降采样高频单词"></a>降采样高频单词</h1><p>在之前的例子中，针对一些通用单词例如“the”存在一些问题：</p>
<ol>
<li>当我们看一些单词对的时候，(“fox”，“the”)没有告诉我们过多的关于“fox”的含义的信息，“the”出现在几乎每个单词的前面。</li>
<li>我们拥有太多的关于(“the”，…)的采样，而我们学习一个好的关于“the”的向量不需要这么多的采样。</li>
</ol>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_S9yXV9_1.jpeg" alt=""></center>

<p>Word2Vec采用了一个叫做“降采样”的框架来解决这个问题。对于每个出现在我们训练文件中的单词，它们将会有一定的概率被删除相关的训练数据，概率的高低取决于它们出现的频率。</p>
<p>如果我们有一个尺寸为10的窗口，那么我们将移除一个特定的关于“the”的训练样本：</p>
<ol>
<li>当我们训练其他单词，“the”将不会在任何窗口中出现。</li>
<li>当我们训练“the”作为输入时，窗口尺寸会小于10。</li>
</ol>
<p><br></p>
<h1 id="采样频率"><a href="#采样频率" class="headerlink" title="采样频率"></a>采样频率</h1><p>关于如何计算字典中一个单词被保留下来的概率，有一套C语言的代码实现的公式。</p>
<p>Wi代表单词，z(wi)表示单词在语料库中出现的频率，例如“peanut”在一个拥有10亿单词语料库中出现了1000次，那么z(‘peanut’)=1E-6。</p>
<p>代码中还有一个参数称之为’sample’，它将控制采样发生的力度，默认是0.001，该数值越小则单词越不可能被保存。</p>
<p>P(wi)为保留单词的概率：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_BJPh3y_5.jpeg" alt=""></center>

<p>我们可以画出该公式的图：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_f7DguJ_6.jpeg" alt=""></center>

<p>如果我们使用默认采样值为0.001，那么有以下有趣的点：</p>
<ul>
<li>P=1，当z&lt;=0.0026时，这意味着当单词数量小于0.26%时将会被全部保留下来。</li>
<li>P=0.5，当z=0.00746时，这意味着有50%的概率被保留下来</li>
<li>P=0.033，当z=1时，这意味着如果语料库中全部是同一个单词时，只有3.3%的概率会被保存下来，这种情况当然很荒谬。</li>
</ul>
<p>论文中定义的公式和C语言代码中有所区别，我觉得C语言代码的实现更加权威。</p>
<p><br></p>
<h1 id="负采样-Negative-Sampling"><a href="#负采样-Negative-Sampling" class="headerlink" title="负采样(Negative Sampling)"></a>负采样(Negative Sampling)</h1><p>训练一个神经网络意味着每个样本都要更新所有参数，使用skip-gram模型将会每次都要更新数量庞大的模型，负采样采用的策略是更新每个样本时之更新很小比例的参数，而不是更新所有参数。</p>
<p>我们训练一个单词对时，例如(“fox”，“quick”)，当输入是“fox”时标签是“quick”的one-hot向量，那么也就是说输出标签神经元中只有一个是1，其他的成千上万个都是0。</p>
<p>因此当我们使用负采样时，我们将随机选择很小数量的“负”单词(例如选择5个)来更新权值，在我们的语料库中，一个“负”单词指的是我们希望让输出的标签的神经元为0，我们依然会更新“正”单词的神经元参数(也就是标签中为1的神经元)，论文中提到对于一个小规模的数据集，可以选择5-20个单词，如果是庞大的数据集，可以选择2-5个。</p>
<p>之前我们的输出层是300<em>10000的参数矩阵，但如今我们更新权值时只需要更新(“quick”)所代表的神经元的参数(输出为1)，以及再加上5个随机的输出为0的神经元，总共只需要更新6个神经元的参数，也就是300</em>6=1800个参数，这仅仅是原来输出层参数的0.06%！</p>
<p>注意在隐藏层中，不管是否使用负采样策略，都只有输入单词的权值被更新。</p>
<p><br></p>
<h1 id="选择负采样"><a href="#选择负采样" class="headerlink" title="选择负采样"></a>选择负采样</h1><p>选择一个单词作为负采样和它出现的频率有关，出现频率越高的单词越容易被选为负样本。在word2vec的C语言实现中，我们可以看到一个计算概率的公式：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_OWB2Gx_7.jpeg" alt=""></center>

<p>这种在C语言中实现的方式很有趣。他们有一个100M大小的元素数组（他们称之为unigram表），他们用词表中每个单词的索引多次填充这个表。然后，要实际选择一个负样本，只需生成0到100M之间的随机整数，并选择表中该索引处的单词。由于较高概率单词在表中出现次数较多，因此更有可能选择这些单词。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这里有一些关于Word2Vec的学习资源：</p>
<p><a href="http://mccormickml.com/2016/04/27/word2vec-resources/" target="_blank" rel="external">Word2Vec Resources · Chris McCormick</a></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="external">Word2Vec Tutorial Part 2 - Negative Sampling · Chris McCormick</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 自然语言处理 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Word2Vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Word2Vec笔记：Skip-Gram模型]]></title>
      <url>/2017/06/05/Word2Vec_skip-gram/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Word2Vec的基本概念，同时对Word2Vec的经典模型——The skip gram neural network model进行详细的学习和整理。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="模型任务"><a href="#模型任务" class="headerlink" title="模型任务"></a>模型任务</h1><p>我们将要训练一个简单的神经网络来执行某个确定的任务，它只有一个隐层，但是我们却不去使用该训练好的网络，我们只是来学习隐层的权值，该权值就是我们的词向量(word vectors)。</p>
<p>具体任务如下：在一个句子中间部分选择一个单词作为输入单词，然后网络会告诉我们字典中每个单词在该单词附近的概率，这里的附近范围指的是该输入单词前面5个和后面5个(共10个)单词。比如我们输入单词是“苏联”，那么字典中“联邦”和“俄罗斯”的概率会高于“西瓜”和“袋鼠”</p>
<p>我们训练文件中存在很多成对的单词，我们不停的把一对单词送入神经网络进行训练，神经网络会统计单词出现的频率，比如（“苏联”，“联邦”）出现的次数更多，那么当我们输入“苏联”这个单词时，“联邦”的概率将会更大。如下是一个窗口为2的例子，前面2个后面2个，蓝色的为输入单词。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_S9yXV9_1.jpeg" alt=""></center>

<p><br></p>
<h1 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h1><p>我们不可能直接把一个单词送入神经网络中训练，因此我们需要找到一个方式来代替单词。为了实现这点，我们先从训练文件中建立一个单词字典，假设我们有10000个独立单词。</p>
<p>我们讲吧每个输入单词例如“ants”作为一个one-hot向量，这个向量有10000维，我们将某一维设置为1，其他位置设置为0来表示单词“蚂蚁”。在训练网络时，输入是10000维向量(表示输入单词)，输出标签是与它相关单词对中的另外一个单词的one-hot向量，也是10000维。但是当我们使用该网络时，输出层使用softmax层，输出将变成一个10000维的概率向量(因为字典中有10000个单词)，其中的每一个维度都表示字典中的一个单词在该输入单词附近的概率。下图为使用时的模型：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_EBQYfm_2.jpeg" alt=""></center>

<h1 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h1><p>如果我们用300个单元来表示隐藏层，那么我们学到的词向量就是300维的，因此隐层的参数矩阵是[10000,300]的，300是Google在发行的数据库中使用的长度，这是个超参数，可以根据自己的需求更改。</p>
<p>因此我们的目标实际上是学习这样一个参数矩阵，输出层在我们训练完网络就丢弃不用。那么输入的one-hot向量什么作用呢，因为该向量大部分都是0，只有一个维度是1，因此可以起到选择的作用，如下简单的例子：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_HQ30yx_3.jpeg" alt=""></center>

<p>我们可以看出隐层的输出是输入one-hot向量的一个词向量表示。</p>
<p><br></p>
<h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><p>1*300的词向量”ants”将会被送到输出层，输出层是一个softmax回归分类器，分类器会输出字典中出现过的每一个单词在它附近的概率，这些概率在0-1之间，而且加起来的和为1。特别的，每个输出单元都有个权值向量和词向量相乘，然后用exp()函数得出结果，最后为了使得所有的向量概率加起来为1，还要除以10000个词向量的exp()结果的和。</p>
<p>以下是计算输出神经元输出“car”这个单词的图示：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_XjsDH7_4.jpeg" alt=""></center>

<p>注意即使训练时某个单词在另一个单词附近的概率是100%，但使用时输出改单词的概率也不是100%，这和softmax函数有关。</p>
<p><br></p>
<h1 id="一些直觉"><a href="#一些直觉" class="headerlink" title="一些直觉"></a>一些直觉</h1><p>如果两个单词又非常相似的语义，我们的模型将会对这两个单词的输入来输出非常相似的结果。其中的一个方法是这两个单词的词向量是相似的。因此如果两个单词有相似的语义，那么他们有相似的词向量。</p>
<p>那么什么叫相似的语义呢？比如“intelligent”和“smart”这种有相似意思的单词，或者“engine”和“transmission”这种相关的单词，或者“ant”和“ants”这种具有相同意思的不同词性的单词。</p>
<p><br></p>
<h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>这种模型包含了数量庞大的参数，对于300特征的隐层和10000单词的字典，需要隐层和输出层各3M大小的权值数量，训练一个庞大的数据库是很困难的，因此我们做了一些改进来便于训练，这就是下篇文章提到的Negative Sample。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="external">Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 自然语言处理 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Word2Vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习中几种不同的梯度下降算法]]></title>
      <url>/2017/06/04/RandomGradientDecreases/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文整理了关于梯度下降算法几个容易混淆的概念，它们都属于最速下降法。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<hr>
<p>之前关于Tensorflow的笔记有一个地方提到了随机梯度下降，官方文档提到说“这种随机抓取一小部分数据训练的方法称为随机梯度下降(SGD)”，但后来我又查了资料，这种方法准确来说应该称为Mini-batch梯度下降，因为标准的SGD是每次只随机取一个样本进行训练的，关于梯度下降算法几个容易混淆的概念我整理了下，它们都属于最速下降法：</p>
<p><br></p>
<p>1、<strong>批量梯度下降(Batch Gradient Descent，BGD)</strong></p>
<p>每次迭代的梯度方向计算由所有训练样本共同投票决定，即每次的参数更新需要把所有的训练样本都迭代一遍，可以采用线性搜索来确定最优步长，但缺点是参数更新速度太慢，而且如果训练样本数量特别庞大，对显存要求会很高。“Batch”的含义是训练集中所有样本参与每一轮迭代。</p>
<p><br></p>
<p>2、<strong>随机梯度下降(Stochastic Gradient Descent，SGD)</strong></p>
<p>和BGD是两个极端，SGD每次迭代只选择一个训练样本，计算梯度然后进行参数的更新，直到收敛，这种方法也可用作online learning。这种方法的优点是参数更新速度很快，不需要占用较大显存。但缺点也很明显，因为不同训练样本的差距和噪声的影响，每次参数更新方向未必是正确的优化方向，但实践证明总体趋势一般是朝着最优点方向的。</p>
<p><br></p>
<p>3、<strong>Mini-batch Gradient Descent</strong></p>
<p>这种方法在前两种方法里取一个折中，每次在训练集中取一部分数据进行迭代，然后更新参数。这是实践中使用最多的一种方法，具体选取的batch大小取决于数据集和显卡的质量，实验证明太大和太小都不好，应该选择一个适中的尺寸来达到训练效果最优。具体怎么选那就是一门玄学了，不过普通研究生平时科研的情况下选择自己显存能承受的最大值就可以了吧。</p>
<p><br></p>
<p>平时也有些地方把SGD和Mini-batch GD统称为SGD，前者是后者的一个特例，TensorFlow的官方文档大概就是这样。这两者虽然也叫最速下降法，但步长一般不采用线搜索来获得，因为参数更新频率太快，线搜索计算比较复杂会消耗大量时间，所以平时都是凭经验指定步长(即学习率)。因为只要下降方向正确，步长在一个可以接受的范围内依然可以迭代到最优值，但学习率如果选的太大则无法下降，选的太小又使得训练过程漫长很难达到最优值，因此关于学习率的选择又是一门玄学。</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 梯度下降算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（二）：MNIST进阶]]></title>
      <url>/2017/06/03/TensorFlow_2/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本系列是对TensorFlow官方文档进行学习的总结，本篇介绍了利用TensorFlow搭建一个卷积神经网络，并用它来对MNIST数据集进行分类。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这两天完成了官方文档第二部分的学习，之前我们用TensorFlow在MNIST数据集上实现了简单的Softmax回归，但准确率只有92%左右。因为这种方法忽略了图片的空间结构信息，而只是把图像展开成一维向量来输入。今天我们使用TensorFlow来搭建一个卷积神经网络，用它来对MNIST数据集进行分类。</p>
<p>接下来我们进行卷积神经网络的学习，我会一行行的解释用到的代码。</p>
<p><br></p>
<h1 id="初始化阶段"><a href="#初始化阶段" class="headerlink" title="初始化阶段"></a>初始化阶段</h1><p>首先是训练数据的准备工作，下面两行代码会下载并引用MNIST数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>然后我们启动一个会话，这里我们没有选择之前使用的<code>tf.Session()</code>，而是采用了交互式编程中更方便的Interactive类，这样我们可以在运行图的时候插入新的图，这在一些交互式环境比如ipython中更便利，我使用的是jupyter notebook，同样是交互式环境。如果没有使用 InteractiveSession ，那么需要在启动Session之前构建好整个计算图，然后才能启动该计算图。具体方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">sess = tf.InteractiveSession()</div></pre></td></tr></table></figure>
<p>接着我们定义两个占位符x和y_，分别用来输入图片数据和标签。它们均为二维向量，第一个维度用None来指代Batch大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</div></pre></td></tr></table></figure>
<p>为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题(dead neurons)。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div></pre></td></tr></table></figure>
<p>下面重点讲一下TensorFlow中的两个函数，它们分别用来做卷积操作和池化操作。</p>
<p>1、<code>tf.nn.conv2d(x,W,strides,padding)</code></p>
<ul>
<li>x：4D的tensor，格式为[batch,height,width,channels]。</li>
</ul>
<ul>
<li>W：核函数，同样是4D的tensor，格式为[height,width,in_channel,out_channel]。</li>
<li>strides：步长，也是4D的tensor，格式为[1,stride,stride,1]，一般第一维和第四维都是1，因为很少有对batch和channel进行卷积计算。</li>
<li>padding：分“SAME”或者“VALID”两种，前者会进行补0，使得卷积前后的尺寸相同，后者不补0。</li>
</ul>
<p>2、<code>tf.nn.max_pool(value, ksize, strides, padding)</code></p>
<ul>
<li>value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape</li>
<li>ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1</li>
<li>strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]</li>
<li>padding：和卷积类似，可以取’VALID’ 或者’SAME’，返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式。</li>
</ul>
<p><br></p>
<p>这里我们的卷积使用1步长(stride size)，补0(padding size)模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling。为了代码更简洁，我们把这部分抽象成一个函数。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="第一层卷积和池化"><a href="#第一层卷积和池化" class="headerlink" title="第一层卷积和池化"></a>第一层卷积和池化</h1><p>下面我们可以实现第一层了，它由一个卷积接一个max pooling完成。卷积在每个5x5的patch(卷积核)中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。而对于每一个输出通道都有一个对应的偏置量。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</div><div class="line">b_conv1 = bias_variable([<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>为了用这一层，我们把x变成一个4d向量，其第2、第3维对应图片的高、宽，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。 然后我们进行卷积和池化操作，卷积后要用ReLU激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="第二层卷积和池化"><a href="#第二层卷积和池化" class="headerlink" title="第二层卷积和池化"></a>第二层卷积和池化</h1><p>第二层中，每个5x5的patch会得到64个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">h_pool2 = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="全连接层fc1"><a href="#全连接层fc1" class="headerlink" title="全连接层fc1"></a>全连接层fc1</h1><p>现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>为了减少过拟合，我们采用dropout。说起dropout，这又是一门玄学了，dropout是指在网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，即随机让一些节点参数不工作。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。实验证明这种方法能有效的减少过拟合。</p>
<p>我们用一个placeholder来代表神经元的输出在dropout中保持不变的概率，这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">keep_prob = tf.placeholder(tf.float32)</div><div class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><p>最后，我们添加一个softmax层，就像前面的单层softmax regression一样。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line">b_fc2 = bias_variable([<span class="number">10</span>])</div><div class="line">y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h1><p>为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，只是我们会用更加复杂的ADAM优化器来做梯度最速下降，在feed_dict中加入额外的参数keep_prob来控制dropout比例。然后每100次迭代输出一次日志。 一共进行了20000次迭代，batch_size是50。实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">cross_entropy = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</div><div class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">sess.run(tf.global_variables_initializer())</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</div><div class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;</div><div class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</div><div class="line">    print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy))</div><div class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line">print(<span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</div><div class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</div></pre></td></tr></table></figure>
<p>我用的jupyter notebook交互式环境。官网给出的训练时间大概是一个半小时，但我只用了16分钟左右就训练完了，可能官网给的是用CPU训练的参考时间。最后我们得到的准确率是99.2%，和官方文档得到的是一样的。这个网络模型是两层卷积两层池化两层全连接，最后一层全连接是softmax层。</p>
<p>至此我们就用TensorFlow从头到尾实现了一个卷积神经网络，中间涉及了很多矩阵运算，且都是4维向量之间的运算。期间用到了部分最优化里的东西，比如最速下降法，但并不深入。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>深度学习到底还是个偏工程应用的学科，对于理论知识涉及面极广但并非很深入，而且很多地方还是玄学。比如dropout、迭代次数、学习率、batch size、正则项系数等等，这些参数全都要靠自己凭借经验调试，能出什么结果全凭运气。</p>
<p>好啦，我们关于卷积神经网络实现MNIST的内容就学习完啦，终于自己实现了一个网络hin开心，下面有时间会继续学习TensorFlow的其他部分。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/mnist/pros" target="_blank" rel="external">Deep MNIST for Experts  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（一）：MNIST入门]]></title>
      <url>/2017/06/02/Tensorflow_1/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本系列是对TensorFlow官方文档进行学习的总结，本篇主要关于TensorFlow的基础知识和在MNIST数据集上结合Softmax回归的简单实现。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近开始学习TensorFlow，关于TensorFlow网上有很多学习笔记，但基本上都来源于官方文档，目前已经有翻译好的中文版，因此决定直接从官方文档学起。代码部分我用的是Python3.5版本。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621230301_Z8l9h5_unnamed.jpeg" alt=""></center>

<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>TensorFlow支持Python、C和C++三种语言，但Python的库是最全的，也是使用最广泛的。</p>
<p>TensorFlow用<strong>张量(Tensor)</strong>表示数据，数据一般是多维数组，在Python中为numpy的ndarray对象。<strong>Flow(流)</strong>意味着基于数据流图(Data Flow Graphs)进行数值计算。TensorFlow即为张量从图的一端流动到另一端。</p>
<p>TensorFlow的运算可以用有向图表示，其中<strong>节点</strong>(operation,简称op)代表数学运算，<strong>边</strong>表示节点之间的某种联系，负责传输多维数据(Tensors)。节点可以被分配到多个计算设备上，可以异步和并行的地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。</p>
<p>图在定义的过程中不会被执行，必须在会话(Session)中被启动，会话把op分发到GPU或CPU设备上，同时提供执行方法。因此TensorFlow的程序通常被组织成<strong>构建阶段</strong>和<strong>执行阶段</strong>，在构建阶段，我们把所有op描述成一个图；在执行阶段，我们通过会话执行之前构建好的图。</p>
<p>如下例子是实现一个简单计数器:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">one = tf.constant(<span class="number">1</span>)</div><div class="line">state = tf.Variable(<span class="number">0</span>)</div><div class="line">new_value = tf.add(state, one)</div><div class="line">update = tf.assign(state, new_value)</div><div class="line">sess = tf.Session()</div><div class="line">tf.global_variables_initializer().run()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    print(sess.run(state))</div><div class="line">    sess.run(update)</div></pre></td></tr></table></figure>
<p>以下是输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">0</div><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td></tr></table></figure>
<p>最后记得关闭会话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p>节点(op)的常用定义：</p>
<p>1、<strong>常量(constant)</strong>，一般用作源节点，输出给其他节点做运算。</p>
<p>2、<strong>变量(variable)</strong>，一般用来存储需要更新的参数，需要初始化。</p>
<p>3、<strong>占位符(placeholder)</strong>，先定义好一个没有具体内容的节点，但需要指定数据类型，一般用作输入和输出节点，在执行会话run操作时用feed操作输入数据。</p>
<p>4、表示某种操作，比如：</p>
<p><code>+ - * /</code>，均为矩阵间对应项操作。</p>
<p><code>tf.matmul(a,b)</code>，矩阵乘法，不同于*。</p>
<p><code>tf.assign(a,b)</code>，赋值操作，把b的值赋给a。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621203909_FE7lvU_2.jpeg" width="800"></center>



<p>上例中，在<strong>构建阶段</strong>，我们定义一个常量one令它为1；定义了一个变量state令初始值为0；定义了一个加法操作<code>tf.add()</code>，它等同于+；定义了一个赋值操作<code>tf.assign()</code>，把更新后的值new_value赋给state。在该阶段所有操作都不会被执行。</p>
<p>在<strong>执行阶段</strong>，我们需要启动一个会话来执行图。我们定义一个会话<code>sess = tf.Session()</code>，通过<code>sess.run()</code>执行操作。如果括号内为常量、变量等，则会取出相应的值，我们称该操作为<strong>Fetch</strong>；如果括号内为某个运算操作，则会执行该操作同时返回结果tensor。在执行阶段的最初我们首先需要初始化所有变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.global_variables_initializer().run()</div></pre></td></tr></table></figure>
<p>在该例中我们通过循环反复执行update节点，并打印出state变量中的值，实现一个简单计数器的操作。</p>
<p>执行阶段结束后需要通过命令<code>sess.close()</code>结束会话，释放空间。也可以使用<code>with tf.Session() as sess:</code>代替<code>sess = tf.Session()</code>来建立会话，这样不需要手动释放空间。</p>
<p>需要注意的是，在执行会话的<code>sess.run()</code>操作之前，任何对op的定义都不会被执行。</p>
<p><br></p>
<h1 id="MNIST机器学习入门"><a href="#MNIST机器学习入门" class="headerlink" title="MNIST机器学习入门"></a>MNIST机器学习入门</h1><p>本章介绍了MNIST数据集和Softmax回归，前者是个入门级的计算机视觉数据集，包含各种手写数字图片，官方文档把MNIST称为机器学习届的“Hello World”。Softmax Regression是一个非常基础的数学模型，在多分类问题中被广泛使用，在斯坦福大学的公开课CS231n中，Softmax回归也被放在最前面来讲，可见其重要性。本章学习用Softmax回归训练一个机器学习模型用于预测MNIST数据集中的数字。</p>
<p>首先下载MNIST数据集，我们只需要用如下两行代码自动下载并且引用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p> 每张图片包含28*28个像素点，我们可以用一个数字矩阵来表示它。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621230309_tE1HOU_11.jpeg" width="800"></center>

<p>在该模型中，我们直接把这个数组展开成一个一维向量，也就是忽视它的空间结构，把每张图片都看作一个长度为28*28=784的向量。数据集的标签数据是”one-hot vectors” ，标签是介于0～9之间的数字，每个标签都是长为10的一维向量，只有一个元素为1其他均为0，例如标签9将表示成([0,0,0,0,0,0,0,0,0,0,1]) .</p>
<p>下面介绍下Softmax回归，对于输入的每张图片，我们希望得到每个标签对应的概率，且所有标签的概率加起来应为1，Softmax回归就实现了这样一个功能。</p>
<p>Softmax回归分两步：第一步，我们通过一系列的加权求和，如果某个像素具有很强的证据说明这张图片不属于该类，那么相应的权值为负数，相反如果这个像素拥有有利的证据支持这张图片属于这个类，那么权值是正数。 再加上一个额外的偏置项(bias)，因此对于给定输入x我们可以得到某张图片的得分：<br>$$<br>\text{evidence}_i = \sum_j W_{i,~ j} x_j + b_i<br>$$<br>第二步，我们利用Softmax函数把它转化为概率值：<br>$$<br>y = \text{softmax}(\text{evidence})<br>$$<br>其中，Softmax函数的具体形式为：</p>
<p>$$<br>\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}<br>$$<br>因此我们得到如下形式(假设x为一个长度为3的一维向量，标签数量同样为3)：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621230307_YKyvO7_softmax-regression-vectorequation.jpeg" width="800"></center>

<p>简写为：</p>
<p>$$<br>y = \text{softmax}(Wx + b)<br>$$<br>在我们的模型中，训练数据共60000张图片，但由于我们采用随机梯度下降来优化，因此我们x的尺寸为[batch_size,784]，W的尺寸为[784,10]，b的尺寸为[1,10]，最终输出的<code>y</code>尺寸和标签<code>y_</code>尺寸均为[batch_size,10]。具体实现我用的jupyter notebook，过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</div><div class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</div><div class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</div><div class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[<span class="number">1</span>]))</div><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>).minimize(cross_entropy)</div><div class="line">sess = tf.Session()</div><div class="line">tf.global_variables_initializer().run()</div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</div></pre></td></tr></table></figure>
<p>我们首先定义了变量W和b，用来存储模型参数，在一遍遍迭代中不断的对其进行更新，初始化为全零矩阵。然后定义占位符x，先在图中分配好节点，类型为float型，None表示此张量的第一个维度可以是任何长度的，在执行时可以利用feed操作输入任何数量的图片。接着定义了矩阵乘法操作<code>tf.matmul()</code>和加操作+，以及定义了节点y进行softmax计算。</p>
<p>我把数据流图简单画了下：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621203909_wnrrZf_9.jpeg" width="800"></center>

<p>我们已经定义了一个指标来说明一个模型是好的，但在机器学习，我们通常定义指标来表示一个模型是坏的，这个指标称为成本(cost)或损失(loss)，然后尽量最小化这个指标，这两种方式是相同的。</p>
<p>这里我们用到的成本函数是“交叉熵”(cross-entropy) ，它的定义如下：</p>
<p>$$<br>H_{y’}(y) = -\sum_i y’_i \log(y_i)<br>$$<br>y 是我们预测的概率分布, y’ 是实际的分布 </p>
<p>因此需要定义一个新的占位符<code>y_</code>来表示标签数据，来计算交叉熵，我们采用的batch_size为100，这里计算的是100张图片的交叉熵总和。在这里，我们定义一个最优化操作train_step，要求TensorFlow用梯度下降算法(gradient descent algorithm)以0.05的学习速率最小化交叉熵。</p>
<p>到这里，我们程序的构建阶段就完成了，接下来是执行阶段。我们启动一个会话<code>sess = tf.Session()</code>，首先初始化所有变量，然后开始启动训练，这里让模型循环训练迭代1000次，每次随机抓取训练数据中100个数据点，然后用它们替换之前定义的占位符<code>x</code>和<code>y_</code>中的参数(feed操作)。这种随机抓取数据训练的方法称为<strong>随机梯度下降</strong>，可以很好的节省训练开销，同时最大化的学习数据集的总体性能。</p>
<p>训练用的时间比较短，用CUDA只要几秒钟，训练结束后我们需要验证模型的性能，MNIST数据集有10000张测试图片用于评估模型的泛化性能。下面是测试阶段代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</div></pre></td></tr></table></figure>
<p>先介绍下用到的几个主要函数：</p>
<p><code>tf.argmax()</code>函数能给出tensor对象在某一维上的最大值所在的索引值。</p>
<p><code>tf.equal()</code>函数来对比两个tensor是否相同，返回一组布尔值。</p>
<p><code>tf.cast()</code>函数把tensor转化为某个特定类型，这里我们把布尔型转化为浮点数。例如： [True, False, True, True] 会变成 [1,0,1,1] 。</p>
<p><code>tf.reduce_mean()</code>函数用于计算tensor中所有元素的平均值，上例取平均值得到0.75。</p>
<p>在本模型中，我们定义节点correct_prediction来计算输出值和标签值的比较结果，得到一组布尔值；随后定义节点accuracy来执行转化浮点数操作和取平均值操作，最后返回的值即为我们模型最终测试的准确率。在测试时的执行阶段，我们用测试数据对占位符<code>x</code>和<code>y_</code>进行feed操作。</p>
<p>最终的准确率大约为91%，因为忽略了图片的空间结构信息，因此得到的准确率并非特别高，下一章会学习用卷积神经网络进行预测，可以达到99%的准确率。</p>
<p><br></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>TensorFlow相比于Caffe确实是难了一个等级，Caffe的模块化程度较高，给人一种黑箱子的感觉，模型建好是建好了，也能跑出来结果，但却对其中的运行过程毫不知情，就像搭积木。相比之下TensorFlow是一点点的自己去构造整个模型，更偏底层一些，因此真的是越用越喜欢，尤其是自己能独立的把整个模型构建起来后，成就感自然是Caffe不能比的。</p>
<p>因为Python进行复杂运算的效率较低，所以我们通常会使用各种函数库，比如NumPy，会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。但是从外部计算切回Python又是很大的开销，因此TensorFlow使用的方案是先一次性定义好所有需要的操作，然后全部一起放在Python外运行，这也是TensorFlow的程序分为构建阶段和执行阶段的原因。</p>
<p>因为节点可以被分配到多个计算设备上，可以异步和并行的地执行操作，所以我们就可以通过分布式运算极大的提高运行速度，这算是TensorFlow的优点之一吧。</p>
<p>本文部分图片和代码引用自TensorFlow官方文档，手绘数据流图为原创.</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="external">MNIST For ML Beginners  |  TensorFlow</a> </li>
</ul>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mac环境利用GitHub和Hexo搭建个人博客]]></title>
      <url>/2017/06/01/MacGitHubHexoBlog/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了在Mac环境下如何利用GitHub和Hexo搭建个人博客，网上有不少教程已经过时，自己也走了不少弯路。最近折腾了下，决定总结一下具体的流程。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<h1 id="Hexo介绍"><a href="#Hexo介绍" class="headerlink" title="Hexo介绍"></a>Hexo介绍</h1><p>互联网时代很多人选择用个人博客来展示自己，试想一下拥有一个属于自己的域名，同时搭建一个属于自己的博客是多么炫酷的一件事。然而做独立博客在技术上要面对一堆需要解决的问题，于是就催生出一些自动化的搭建和维护博客的工具。目前比较流行的工具有ghost、Jekyll和Hexo，ghost可以生成动态网站，依赖于数据库，对环境的依赖相对较高，操作起来也更复杂一些。只是构建个人独立博客的话，更建议选择后两者。Jekyll和hexo都是用来生成静态网站的工具，对环境依赖少，可移植性更高，两者都可以托管在GitHub上。Jekyll需要安装python、ruby和一些库，而Hexo仅仅依赖于node，于是思虑再三我决定选择Hexo。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_XCXY5G_1.jpeg" alt=""></center>

<p><br></p>
<h1 id="搭建流程"><a href="#搭建流程" class="headerlink" title="搭建流程"></a>搭建流程</h1><ol>
<li>获得域名</li>
<li>利用GitHub创建仓库</li>
<li>安装Git</li>
<li>安装Node.js</li>
<li>安装Hexo</li>
<li>推送网站</li>
<li>绑定域名</li>
<li>配置主题</li>
<li>更新文章</li>
<li>寻找图床</li>
<li>功能扩展</li>
</ol>
<p><br></p>
<h1 id="获得域名"><a href="#获得域名" class="headerlink" title="获得域名"></a>获得域名</h1><p>域名是网站的一个入口，也是别人对网站的第一印象，常见的域名有.com、.cn、.net等后缀，也有io、me、vip等后缀，一个巧妙的域名能给人留下深刻的印象，比如饿了么的域名：<a href="https://www.ele.me/" target="_blank" rel="external">https://www.ele.me/</a> 。要想搭建一个个人博客，给别人留下深刻印象，拥有一个属于自己的域名是必不可少的，而购买域名也是我们整个搭建过程中唯一需要花钱的地方，但花费并不多。</p>
<p>申请域名的地方有很多，以下列举了几个常用的注册商：</p>
<ul>
<li><a href="https://www.aliyun.com" target="_blank" rel="external">阿里云-为了无法计算的价值</a></li>
<li><a href="https://cloud.baidu.com" target="_blank" rel="external">百度云–智能，计算无限可能</a></li>
<li><a href="https://www.qcloud.com" target="_blank" rel="external">腾讯云 - 值得信赖</a></li>
<li><a href="https://www.godaddy.com" target="_blank" rel="external">GoDaddy 全球知名互联网域名注册商</a></li>
</ul>
<p>这里推荐阿里云，域名种类多而且提供隐私保护，购买后需要实名认证，如果不喜欢实名认证的同学可以选择最后一个GoDaddy，但是GoDaddy不提供隐私保护，会把你的邮箱和手机信息暴露出来，导致可能收到很多垃圾邮件和短信，而且GoDaddy提供的域名后缀种类有限，比如我喜欢的.me就没有。因此我选择了阿里云，申请入口<a href="https://wanwang.aliyun.com/domain/" target="_blank" rel="external">域名注册</a>。</p>
<p>阿里云购买.me域名首年只需要13元，支持支付宝付款，实名认证比较简单，只需要提交一张身份证照片即可，经过实名认证审核后，我们就拥有了属于自己的域名。</p>
<p><br></p>
<h1 id="利用GitHub创建仓库"><a href="#利用GitHub创建仓库" class="headerlink" title="利用GitHub创建仓库"></a>利用GitHub创建仓库</h1><p>创建网站不仅需要域名，而且需要网站空间。网站空间也叫虚拟主机，是用来存放网站内容的地方。但是租借虚拟主机需要很大的开销，幸运的是，我们可以把静态网站托管到GitHub上，GitHub是一个面向开源及私有软件项目的托管平台。</p>
<p>首先需要登陆到GitHub：<a href="https://github.com" target="_blank" rel="external">Build software better, together</a>，如果没有GitHub账号，可以使用邮箱注册，然后点击GitHub中的New repository创建新仓库，仓库名为：<strong>用户名</strong>.github.io，这里命名格式为固定的，<strong>用户名</strong>是你的GitHub账号，比如我的仓库名为：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_HZdID1_2.jpeg" alt=""></p>
<p>建好之后，我们就拥有了用于搭建个人博客的网站空间。</p>
<p><br></p>
<h1 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h1><p>Git是一款免费、开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。具体的细节可以看廖雪峰老师的教程：<a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="external">Git教程 - 廖雪峰的官方网站</a>，打开Git的官方网站下载Git安装包：<a href="https://git-scm.com/downloads" target="_blank" rel="external">Git - Downloads</a>，由于我们是Mac OS系统，这里选择相关的下载包，下载后根据提示安装。</p>
<p>安装结束后，打开终端输入git测试是否安装成功，安装成功后，需要把Git和GitHub账号绑定，首先设置user.name和user.email信息，在终端中输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git config --global user.name <span class="string">"你的GitHub用户名"</span></div><div class="line">git config --global user.email <span class="string">"你的GitHub注册邮箱"</span></div></pre></td></tr></table></figure>
<p>生成ssh密钥文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh-keygen -t rsa -C <span class="string">"你的GitHub注册邮箱"</span></div></pre></td></tr></table></figure>
<p>然后连续按三个回车即可，这里默认不需要设置密码。我们根据终端中提示的地址找到生成的 .ssh 文件夹，该文件夹为隐藏文件夹，可以在终端中用 cd 指令进入，我们需要的Key就存放在该文件夹下面的 id_rsa.pub 文件中，我们可以用指令<code>cat id_rsa.pub</code>打印出该文件中的内容，然后复制全部内容（以ssh-rsa开头）。</p>
<p>打开<a href="https://github.com/settings/keys" target="_blank" rel="external">GitHub_Settings_keys</a>页面，点击 new SSH Key 按钮：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_ujfyFj_3.jpeg" width="700"></p>
<p>其中Title可以随意填，因为我们可能会用不同的电脑连接GitHub，比如家里的电脑，公司的电脑等等，所以后面可能会添加多个Key，可以把Title取为Home或者School等方便区分。把刚复制的内容粘贴进去，点击Add SSH key。</p>
<p>在终端中检测GitHub公钥是否设置成功，输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh git@github.com</div></pre></td></tr></table></figure>
<p>显示以下内容说明成功：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_0OonVo_4.jpeg" width="700"></p>
<blockquote>
<p>这里之所以设置GitHub密钥原因是，通过非对称加密的公钥与私钥来完成加密，公钥放置在GitHub上，私钥放置在自己的电脑里。GitHub要求每次推送代码都是合法用户，所以每次推送都需要输入账号密码验证推送用户是否是合法用户，为了省去每次输入密码的步骤，采用了ssh，当你推送的时候，git就会匹配你的私钥跟GitHub上面的公钥是否是配对的，若是匹配就认为你是合法用户，则允许推送。这样可以保证每次的推送都是正确合法的。</p>
</blockquote>
<p><br></p>
<h1 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h1><p>hexo基于Node.js，我们需要首先安装Node.js，下载地址<a href="https://nodejs.org/en/download/" target="_blank" rel="external">Download | Node.js</a>，注意安装Node.js会包含环境变量及npm的安装。</p>
<p>检测Node.js是否安装成功，在命令行中输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">node -v</div></pre></td></tr></table></figure>
<p>检查npm是否安装成功，在命令行输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm -v</div></pre></td></tr></table></figure>
<p>若显示版本号， 则表明安装成功。</p>
<p>对于Mac用户，要想安装hexo需要首先安装Xcode，我们可以从AppStore安装Xcode，安装结束后，我们需要安装Command Line Tools，Command LineTools是在Xcode中的一款工具，可以在命令行中运行C程序。打开终端输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xcode-select --install</div></pre></td></tr></table></figure>
<p>安装结束后，再次在终端输入：                                                                                              </p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xcode-select --install</div></pre></td></tr></table></figure>
<p>若显示command line tools are already installed，则表明安装成功。</p>
<p>到此，Hexo所需要的环境已经全部搭建完成。</p>
<p> <br></p>
<h1 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h1><p>我们可以用npm命令安装Hexo，打开终端输入：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install -g hexo-cli</div></pre></td></tr></table></figure>
<p>安装结束后，输入以下命令来初始化一个Hexo博客：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hexo init &lt;folder&gt; </div><div class="line"><span class="built_in">cd</span> &lt;folder&gt;</div><div class="line">npm install</div></pre></td></tr></table></figure>
<p>该命令会在当前文件夹下新建一个名叫folder的文件夹来初始化一个博客，如果命令中没有folder，则在当前文件夹初始化博客。比如我要初始化一个名字叫blog的博客，则需要输入<code>hexo init blog</code>，这样我们就在该目录下新建了一个名为blog的文件夹，接着我们利用命令<code>cd blog</code>进入该文件夹，输入<code>npm install</code>来安装必备的库。</p>
<p>到这里我们的网站就初始化完毕了，下面我们尝试添加一篇新的文章并且在本地服务器上查看。不出意外当前终端定位是在我们刚建立的文件夹下，即刚刚的blog文件夹下，此时在终端输入以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hexo new [layout] &lt;title&gt;  <span class="comment">#等价于 hexo n [layout] &lt;title&gt;</span></div><div class="line">hexo generate  <span class="comment">#等价于 hexo g</span></div><div class="line">hexo server  <span class="comment">#等价于 hexo s</span></div></pre></td></tr></table></figure>
<p>我新建了一篇名为“<em>The first article</em>”的文章。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_1ieo4u_5.jpeg" width="700"></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_Mm7oau_6.jpeg" width="700"></p>
<p>完成后打开浏览器输入地址：</p>
<p>Localhost:4000</p>
<p>刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。</p>
<p>可以进入我们刚刚建立的博客，看到我们刚刚建立的文章，以及Hexo默认自带的一篇文章。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_k0kChT_7.jpeg" width="700"></p>
<p>接下来我们来介绍常用的Hexo命令（详细用法可查看Hexo官方文档<a href="https://hexo.io/docs/commands.html" target="_blank" rel="external">Commands | Hexo</a>）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo init <span class="comment">#初始化博客</span></div></pre></td></tr></table></figure>
<p>命令简写：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hexo n <span class="string">"我的博客"</span> <span class="comment">#  == hexo new "我的博客" ：新建文章</span></div><div class="line">hexo g <span class="comment"># == hexo generate ：生成</span></div><div class="line">hexo s <span class="comment"># == hexo server ：启动服务预览</span></div><div class="line">hexo d <span class="comment"># == hexo deploy ：部署</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hexo server <span class="comment">#Hexo会监视文件变动并自动更新，无须重启服务器</span></div><div class="line">hexo server -s <span class="comment">#静态模式</span></div><div class="line">hexo server -p 5000 <span class="comment">#更改端口</span></div><div class="line">hexo server -i 192.168.1.1 <span class="comment">#自定义 IP</span></div><div class="line">hexo clean <span class="comment">#清除缓存，若是网页正常情况下可以忽略这条命令</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="推送网站"><a href="#推送网站" class="headerlink" title="推送网站"></a>推送网站</h1><p>推送网站需要关联Github和Hexo，官方文档下的关联步骤，输入第一行代码后，需要打开blog根目录下的 <em>_config.yml</em> 文件，按照下面所示要求进行配置。在 Hexo中有两份主要的配置文件，其名称都是 <em>_config.yml</em>。其中，一份位于站点根目录下，主要包含Hexo本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。为了描述方便，在以下说明中，将前者称为站点配置文件，后者称为主题配置文件。我们在这一步只需要修改站点配置文件。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_CCOYpH_8.jpeg" width="700"></p>
<p>其实就是给<code>hexo d</code>这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。</p>
<p>接下来，我们输入三条命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hexo clean</div><div class="line">hexo g</div><div class="line">hexo d</div></pre></td></tr></table></figure>
<p>完成后打开浏览器，在地址栏输入 <strong>用户名</strong>.github.io ，这里的<strong>用户名</strong>为你的GitHub用户名，比如我的就是 <a href="zangbo.github.io">zangbo.github.io</a>，然后你就会发现你的博客已经可以在网络上被访问了。</p>
<p><br></p>
<h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><p>虽然在Internet上可以访问我们的网站，但是网址是GitHub提供的：<strong>xxx.github.io</strong> 而我们想使用我们自己刚买的个性化域名，这就需要绑定，这里以阿里云为例。登陆到阿里云：<a href="https://www.aliyun.com" target="_blank" rel="external">阿里云-为了无法计算的价值</a>，点击 <strong>控制台</strong> 进入管理控制台的域名列表，找到你的个性化域名，进入解析：</p>
<p>点击 <strong>添加解析</strong> 按钮，添加三条记录：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_6oNP5k_9.jpeg"></p>
<p>这里两条A记录是固定的，从GitHub官方文档可以找到，这两条是指向GitHub的地址；一条CNAME地址，记录指向是 <strong>用户名</strong>.github.io ，这里的<strong>用户名</strong>是你自己的用户名。</p>
<p>接着登陆GitHub，进入之前创建的仓库，点击 <strong>Settings</strong>，拉到最后，找到 <strong>Custom domain</strong>，填入自己申请的个性域名，点击 <strong>Save</strong>：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_nNgXBn_10.jpeg" width="500"></p>
<p>完成后打开浏览器，在地址栏输入你的个性化域名，就可以直接进入你自己搭建的网站。这一步操作相当于在GitHub仓库目录添加一个CNAME文件，文件中的内容为你刚输入的domain，当GithubPage服务器接收到访问请求时，就知道对应的是这个工程了。</p>
<p><br></p>
<h1 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h1><p>我们刚得到的为Hexo的默认主题 <strong>landscape</strong>，Hexo官网提供了很多可供选择的主题：<a href="https://hexo.io/themes/" target="_blank" rel="external">Themes | Hexo</a>，目前比较流行的是 <strong>Next</strong>，<strong>Casper</strong>，<strong>Uno</strong>，<strong>Modernist</strong>，<strong>yilia</strong> 等等，每个人喜欢的风格不同，最终我选择了一款叫做 <strong>yelee</strong> 的主题，是作者在 <strong>yilia</strong> 的基础上改动来的。以下是几个主题的展示demo：</p>
<p><a href="http://notes.iissnan.com" target="_blank" rel="external">Next</a></p>
<p><a href="https://demo.ghost.io" target="_blank" rel="external">Casper</a></p>
<p><a href="http://daleanthony.com" target="_blank" rel="external">Uno</a></p>
<p><a href="https://orderedlist.com/modernist/" target="_blank" rel="external">Modernist</a></p>
<p><a href="http://litten.me" target="_blank" rel="external">yilia</a></p>
<p><a href="http://moxfive.xyz" target="_blank" rel="external">yelee</a></p>
<p>更换主题之需要打开根目录下的站点配置文件 _config.yml，修改其中的theme的值，例如改为next主题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">theme: next</div></pre></td></tr></table></figure>
<p>具体的主题配置需要修改各自主题下的主题配置文件 _config.yml，大家可以去各主题的GitHub上查看教程，还是比较详细的。</p>
<p><br></p>
<h1 id="更新文章"><a href="#更新文章" class="headerlink" title="更新文章"></a>更新文章</h1><p>更新文章可以打开终端进入博客根目录，输入指令<code>hexo n &quot;文章名&quot;</code>来实现，然后打开根目录下的source/_posts文件夹，可以发现我们新建的 <strong>文章名.md</strong> 文件，然后就可以打开它用Markdown语法来写文章了，具体的Markdown语法教程可以参看这篇：<a href="http://www.appinn.com/markdown/#code" target="_blank" rel="external">Markdown语法说明(简体中文版) </a>。另外补充几点上面没有的：</p>
<ol>
<li>图像居中实现方法：<code>&lt;center&gt;图像&lt;/center&gt;</code></li>
<li>字体加粗可以前后各加两个星号<em>来实现： **</em>需要加粗的文字<em>**</em></li>
<li>换行标签<code>&lt;br /&gt;</code></li>
</ol>
<p>大部分常用用法在上面教程都可以找到。这里注意，在本地修改网站或者更新文章时，可以用指令<code>hexo s</code>在本地服务器看修改效果，打开浏览器输入<code>localhost:4000</code>，每次修改后网站会实时的发生变动。但是修改后要用指令<code>hexo g</code>和<code>hexo d</code>部署到GitHub上，此时的CNAME文件和README.md文件往往会消失，需要重新创建。为了方便起见可以打开网站根目录的source文件夹，把CNAME文件和README.md文件存放在里面。注意CNAME文件没有扩展名。</p>
<p>此时还有个问题，如果把README.md文件放在source文件夹中，执行<code>hexo g</code>指令时会被渲染，解决办法是只要在博客根目录下的配置文件_config.yml中配置一下”skip_render”选项，将不需要渲染的文件名称加入的其选项下就行了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">skip_render: README.md</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="寻找图床"><a href="#寻找图床" class="headerlink" title="寻找图床"></a>寻找图床</h1><p>当文章中有图片时，若是少量图片，可以直接把图片存放在source文件夹中，但图片会占据大量的存储的空间，加载的时候相对缓慢 ，这时考虑把博文里的图片上传到某一网站，然后获得外部链接，使用Markdown语法： <strong>![图片信息](外部链接)</strong> 完成图片的插入，这种网站就被成为图床。这里推荐<a href="https://www.qiniu.com" target="_blank" rel="external">七牛云</a>，新人有10G的免费流量，对于个人博客来说是足够了，超出后的的费用也不贵，也是学生党可以负担得起的。</p>
<p><br></p>
<h1 id="功能扩展"><a href="#功能扩展" class="headerlink" title="功能扩展"></a>功能扩展</h1><h2 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h2><p>打开文件：</p>
<p><code>根目录/node_modules/hexo-generator-index/lib/generator.js</code></p>
<p>替换为以下代码并保存：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="meta">'use strict'</span>;</div><div class="line"></div><div class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">'hexo-pagination'</span>);</div><div class="line"></div><div class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">locals</span>)</span>&#123;</div><div class="line">  <span class="keyword">var</span> config = <span class="keyword">this</span>.config;</div><div class="line">  <span class="keyword">var</span> posts = locals.posts;</div><div class="line"></div><div class="line">    posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</div><div class="line">        <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123; <span class="comment">// 两篇文章top都有定义</span></div><div class="line">            <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date; <span class="comment">// 若top值一样则按照文章日期降序排</span></div><div class="line">            <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top; <span class="comment">// 否则按照top值降序排</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></div><div class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date; <span class="comment">// 都没定义按照文章日期降序排</span></div><div class="line"></div><div class="line">    &#125;);</div><div class="line"></div><div class="line">  <span class="keyword">var</span> paginationDir = config.pagination_dir || <span class="string">'page'</span>;</div><div class="line"></div><div class="line">  <span class="keyword">return</span> pagination(<span class="string">''</span>, posts, &#123;</div><div class="line">    <span class="attr">perPage</span>: config.index_generator.per_page,</div><div class="line">    <span class="attr">layout</span>: [<span class="string">'index'</span>, <span class="string">'archive'</span>],</div><div class="line">    <span class="attr">format</span>: paginationDir + <span class="string">'/%d/'</span>,</div><div class="line">    <span class="attr">data</span>: &#123;</div><div class="line">      <span class="attr">__index</span>: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">  &#125;);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>在写文章时，只需要设置top参数为true即可。</p>
<p><br></p>
<h2 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h2><p>Hexo允许使用MathJax来渲染LaTeX数学公式，首先安装自动部署MathJax的插件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-math --save</div><div class="line">hexo math install</div></pre></td></tr></table></figure>
<p>接着把主题配置文件下的mathjax选项设置为true：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mathjax: true</div></pre></td></tr></table></figure>
<p>如果直接在新建的博文中使用LaTeX格式的公式会出现一些问题，原因是hexo先用marked.<a href="http://lib.csdn.net/base/javascript" target="_blank" rel="external">js</a>渲染，然后再交给MathJax渲染。在marked.js渲染的时候下划线<code>_</code>是被escape掉并且换成了<code>&lt;em&gt;</code>标签，即斜体字，另外LaTeX中的<code>\\</code>也会被转义成一个<code>\</code>，这样会导致MathJax渲染时不认为它是一个换行符了。</p>
<p>我们只要不让marked.js去转义<code>\\</code>,<code>\{</code>,<code>\}</code>在MathJax中有特殊用途的字符就行了。<br>具体修改方式，用编辑器打开marked.js（在<code>./node_modules/marked/lib/</code>中）</p>
<p><strong>Step 1:</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</div></pre></td></tr></table></figure>
<p>替换成</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()# +\-.!_&gt;])/</span>,</div></pre></td></tr></table></figure>
<p>这一步是在原基础上取消了对<code>\\</code>,<code>\{</code>,<code>\}</code>的转义(escape)</p>
<p><strong>Step 2:</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">em: <span class="regexp">/^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</div></pre></td></tr></table></figure>
<p>替换成</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">em:<span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</div></pre></td></tr></table></figure>
<p>这样一来MathJax就能与marked.js共存了。</p>
<p>在Markdown中输入数学公式：</p>
<ul>
<li>行内：<code>$数学公式$</code></li>
<li>行间：<code>$$数学公式$$</code></li>
</ul>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>当初也是看到很多人有自己的网站，所以才萌生了这个想法，希望这篇文章能给大家一点启发，如果后面有更多的心得，也会另发文总结下来。另外推荐一个我比较喜欢的个人博客：<a href="http://www.dandyweng.com" target="_blank" rel="external">翁天信 · Dandy Weng</a></p>
<p>有任何问题欢迎留言或者发邮件。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>知乎<a href="https://www.zhihu.com/people/wurun/answers" target="_blank" rel="external">吴润</a>的专栏文章在Windows的环境的配置教程：<a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="external">GitHub+Hexo 搭建个人网站详细教程 - 知乎专栏</a></li>
<li>GitHub Pages 官方使用说明：<a href="https://pages.github.com" target="_blank" rel="external">GitHub Pages</a></li>
<li>GitHub 官方文档，如何绑定个人域名：<a href="https://help.github.com/articles/using-a-custom-domain-with-github-pages/" target="_blank" rel="external">Using a custom domain with GitHub Pages</a></li>
<li>Hexo 官方文档：<a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="external">文档 | Hexo</a></li>
<li><a href="http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/" target="_blank" rel="external">解决Hexo置顶问题 | Netcan_Space</a></li>
<li><a href="http://blog.csdn.net/emptyset110/article/details/50123231" target="_blank" rel="external">搭建一个支持LaTEX的hexo博客 - Platonic Time - 博客频道 - CSDN.NET</a></li>
</ul>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 教程 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 个人网站 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
