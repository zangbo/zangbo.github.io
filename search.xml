<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（六）：tf.train.ExponentialMovingAverage]]></title>
      <url>/2017/07/01/TensorFlow%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了TensorFlow训练模型过程中对参数的<strong>滑动平均(moving average)</strong>的计算，在测试数据上评估模型性能时用这些平均值总会提升预测结果表现，用到的类主要为<code>tf.train.ExponentialMovingAverage</code>。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="MovingAverage"><a href="#MovingAverage" class="headerlink" title="MovingAverage"></a>MovingAverage</h1><p>常规的滑动平均的计算方法十分简单，对于一个给定的数列，首先设定一个固定的值k，然后分别计算第1项到第k项，第2项到第k+1项，第3项到第k+2项的平均值，依次类推。</p>
<p>以<code>1、2、3、4、5</code>共5个数为例，window为3，计算过程为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(1+2+3)/3=2</div><div class="line">(2+3+4)/3=3</div><div class="line">(3+4+5)/3=4</div></pre></td></tr></table></figure>
<p>下图很好的反映了原始数据和滑动平均之间的关系，其中绿线为原始数据，红线为MovingAverage：</p>
<ul>
<li>当window为3:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_TyB8hT_MovingAverage1.jpeg" alt="window=3" width="700"></p>
<ul>
<li>当window为10:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_UWRLDA_MovingAverage2.jpeg" alt="window=10" width="700"></p>
<p>可以发现当我们使用滑动平均时，会十分有效的提升模型在测试数据上的<strong>健壮性(robustness)</strong>。</p>
<p><br></p>
<h1 id="ExponentialMovingAverage"><a href="#ExponentialMovingAverage" class="headerlink" title="ExponentialMovingAverage"></a>ExponentialMovingAverage</h1><p>在TensorFlow中，我们计算的是<strong>指数滑动平均(ExponentialMovingAverage)</strong>，我们通过使用一个<strong>指数衰减(exponential decay)</strong>来维持着变量的滑动平均。</p>
<p>当我们训练一个模型时，计算训练参数的滑动平均经常是十分有利的，当我们用这些平均后的参数来评估模型时有时会得到比使用常规的训练参数好很多的结果。</p>
<p>我们用一个<code>apply()</code>函数返回一个<code>ops</code>来添加变量的一个副本同时得到原变量的滑动平均，它在我们训练模型的时候使用。该<code>ops</code>得到原变量的滑动平均始终是在每一次训练迭代结束后。</p>
<p><code>average()</code>和<code>average_name()</code>函数返回影子变量和它们的名字，它们在我们对测试数据进行模型评估时使用，它们用参数的滑动平均值来代替最终的训练值来对模型进行评估。它们也可以在我们从一个<code>checkpoint file</code>继续开始训练模型时使用。</p>
<p>滑动平均值用一个指数衰减来计算，当我们创建<code>ExponentialMovingAverage</code>对象时会把该<code>decay</code>值输入进去。影子变量的初始化值和原始变量初始化值相同。每个影子变量计算滑动平均值的公式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">shadow_variable = decay * shadow_variable + (1 - decay) * variable</div></pre></td></tr></table></figure>
<p>通常我们定义<code>decay</code>时会让它尽可能接近于1.0，一般来说我们会让它为0.999、0.9999等。</p>
<p>如下是我们训练一个模型时的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create variables.</span></div><div class="line">var0 = tf.Variable(...)</div><div class="line">var1 = tf.Variable(...)</div><div class="line"><span class="comment"># ... use the variables to build a training model...</span></div><div class="line">...</div><div class="line"><span class="comment"># Create an op that applies the optimizer.  This is what we usually</span></div><div class="line"><span class="comment"># would use as a training op.</span></div><div class="line">opt_op = opt.minimize(my_loss, [var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an ExponentialMovingAverage object</span></div><div class="line">ema = tf.train.ExponentialMovingAverage(decay=<span class="number">0.9999</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create the shadow variables, and add ops to maintain moving averages</span></div><div class="line"><span class="comment"># of var0 and var1.</span></div><div class="line">maintain_averages_op = ema.apply([var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an op that will update the moving averages after each training</span></div><div class="line"><span class="comment"># step.  This is what we will use in place of the usual training op.</span></div><div class="line"><span class="keyword">with</span> tf.control_dependencies([opt_op]):</div><div class="line">    training_op = tf.group(maintain_averages_op)</div><div class="line"></div><div class="line"><span class="comment"># ...train the model by running training_op...</span></div></pre></td></tr></table></figure>
<p>当我们使用滑动平均来预测时，有两种用法：</p>
<ol>
<li>用影子变量代替原始变量，使用<code>average()</code>函数来返回给定变量的影子变量。</li>
<li>通过使用影子变量的<code>name</code>来载入<code>checkpoint files</code>，我们在这里使用<code>average_name()</code>函数。对于这种用法有如下例子：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a Saver that loads variables from their saved shadow values.</span></div><div class="line">shadow_var0_name = ema.average_name(var0)</div><div class="line">shadow_var1_name = ema.average_name(var1)</div><div class="line">saver = tf.train.Saver(&#123;shadow_var0_name: var0, shadow_var1_name: var1&#125;)</div><div class="line">saver.restore(...checkpoint filename...)</div><div class="line"><span class="comment"># var0 and var1 now hold the moving average values</span></div></pre></td></tr></table></figure>
<p>详情可以查看<code>tf.train.Saver</code>，下面介绍<code>ExponentialMovingAverage</code>的相关函数。</p>
<p><br></p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul>
<li><code>__init__</code></li>
<li><code>apply</code></li>
<li><code>average</code></li>
<li><code>average_name</code></li>
<li><code>variables_to_restore</code></li>
</ul>
<p><br></p>
<h2 id="init"><a href="#init" class="headerlink" title="__init__"></a>__init__</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">__init__(</div><div class="line">    decay,</div><div class="line">    num_updates=<span class="keyword">None</span>,</div><div class="line">    zero_debias=<span class="keyword">False</span>,</div><div class="line">    name=<span class="string">'ExponentialMovingAverage'</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>创建一个<code>ExponentialMovingAverage</code>对象。</p>
<ol>
<li><p><code>decay</code>一般取值接近于1.0。</p>
</li>
<li><p><code>num_updates</code>允许<code>dacay</code>值动态的变化，在训练开端<code>dacay</code>速率较低，这使得滑动均值更快，如果有值的话，实际<code>decay</code>速率为以下公式：</p>
<p><code>min(decay, (1 + num_updates) / (10 + num_updates))</code></p>
</li>
<li><p><code>name</code>将会给<code>apply()</code>中的<code>ops</code>添加一个额外的前置名字。</p>
</li>
</ol>
<p><br></p>
<h2 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apply(var_list=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>该函数维护变量的滑动平均，返回一个<code>op</code>来更新所有影子变量。<code>var_list</code>必须是一个变量或者<code>Tensor</code>对象的列表，这个函数创造<code>var_list</code>中所有变量的副本，对于变量副本，初始化值和原变量初始化值相同。变量类型必须是<code>float</code>相关的类型。</p>
<p><code>apply()</code>函数对于不同的<code>var_list</code>可以被调用多次。</p>
<p><br></p>
<h2 id="average"><a href="#average" class="headerlink" title="average"></a>average</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>的滑动平均影子变量，返回类型为<code>Variable</code>。前提是该<code>var</code>使用了<code>apply()</code>函数来维护。</p>
<p><br></p>
<h2 id="average-name"><a href="#average-name" class="headerlink" title="average_name"></a>average_name</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average_name(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>变量的滑动平均影子变量的<code>name</code>。该函数一个典型的应用是在训练过程中计算原始变量的滑动平均，并且在测试时根据影子变量的<code>name</code>恢复出原始变量。</p>
<p>为了恢复原始变量，我们必须知道影子变量的<code>name</code>，影子变量的<code>name</code>和原始变量可以在训练阶段利用<code>Saver()</code>对象来保存，操作为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.train.Saver(&#123;ema.average_name(var): var&#125;)</div></pre></td></tr></table></figure>
<p><code>average_name()</code>函数在<code>apply()</code>函数调用之前或之后都可以使用。</p>
<p><br></p>
<h2 id="variables-to-restore"><a href="#variables-to-restore" class="headerlink" title="variables_to_restore"></a>variables_to_restore</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">variables_to_restore(moving_avg_variables=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>返回一个从<code>restore_name</code>到<code>Variables</code>的映射，如果一个变量有滑动平均值，那么就用该滑动平均影子变量的<code>name</code>来作为<code>restore name</code>，否则，就用原始变量的<code>name</code>。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">variables_to_restore = ema.variables_to_restore()</div><div class="line">saver = tf.train.Saver(variables_to_restore)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage" target="_blank" rel="external">tf.train.ExponentialMovingAverage  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/u014365862/article/details/54380313" target="_blank" rel="external">MovingAverage-滑动平均 - 小鹏的专栏 - 博客频道 - CSDN.NET</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（五）：常用函数和模型的保存与恢复]]></title>
      <url>/2017/06/30/TensorFlow%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文学习了TensorFlow的一些常用的基础函数，主要为以下几种：</p>
<ul>
<li>tf.group</li>
<li>tf.Graph.control_dependencies</li>
<li>tf.train.Saver</li>
</ul>
<p>同时介绍了训练过程中的模型保存和恢复方法，主要用到<code>tf.train.Saver</code>类。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a>tf.group</h1><p><code>tf.group(inputs)</code>创建一个<code>op</code>把多个<code>ops</code>给组合起来，该<code>op</code>无输出。当该<code>op</code>结束操作，其中的所有<code>ops</code>都会结束。</p>
<p>其中<code>inputs</code>为空或者很多<code>tensors</code>。</p>
<p><br></p>
<h1 id="tf-Graph-control-dependencies"><a href="#tf-Graph-control-dependencies" class="headerlink" title="tf.Graph.control_dependencies"></a>tf.Graph.control_dependencies</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">control_dependencies(control_inputs)</div></pre></td></tr></table></figure>
<p>参数<code>control_inputs</code>是一个包含<code>op</code>或者<code>tensor</code>的列表，该列表内的对象必须在控制区域内的<code>ops</code>之前执行。可以为<code>None</code>来清空控制依赖。</p>
<p>通常用<code>with</code>操作来定义一个区域，在该区域下所有的<code>ops</code>都要在<code>control_inputs</code>执行结束后才能执行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</div><div class="line">  <span class="comment"># `d` and `e` will only run after `a`, `b`, and `c` have executed.</span></div><div class="line">  d = ...</div><div class="line">  e = ...</div></pre></td></tr></table></figure>
<p>多次用<code>with</code>调用该函数会得到叠加的依赖，区域内的<code>ops</code>将会在以上所有层次的<code>control_inputs</code>运行结束后才能得到运行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">    <span class="comment"># Ops constructed here run after `a`, `b`, `c`, and `d`.</span></div></pre></td></tr></table></figure>
<p>我们可以用<code>None</code>来清空控制所有依赖。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies(<span class="keyword">None</span>):</div><div class="line">    <span class="comment"># Ops constructed here run normally, not waiting for either `a` or `b`.</span></div><div class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">      <span class="comment"># Ops constructed here run after `c` and `d`, also not waiting</span></div><div class="line">      <span class="comment"># for either `a` or `b`.</span></div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p>控制依赖起作用的区域内只有<code>ops</code>会被执行，仅仅把一个节点放在该区域是不起作用的。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># WRONG</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  t = tf.matmul(tensor, tensor)</div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created outside the context, so no control</span></div><div class="line">    <span class="comment"># dependency will be added.</span></div><div class="line">    <span class="keyword">return</span> t</div><div class="line"></div><div class="line"><span class="comment"># RIGHT</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created in the context, so a control dependency</span></div><div class="line">    <span class="comment"># will be added.</span></div><div class="line">    <span class="keyword">return</span> tf.matmul(tensor, tensor)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="tf-train-Saver"><a href="#tf-train-Saver" class="headerlink" title="tf.train.Saver"></a>tf.train.Saver</h1><p>在训练过程中我们可能会想到每隔一段训练保存一次模型，一方面为了防止过拟合，另一方面如果训练过程被意外打断还可以从某个保存点继续开始训练。之前已经学习过<code>Variable</code>的概念，今天来学习下如何用<code>tf.train.Saver</code>来保存和恢复一个模型。关于<code>tf.train.Saver</code>更详细的内容请参考官方文档。</p>
<p><code>Saver</code>类添加<code>ops</code>来保存变量到<em>checkpoints</em>或者从<em>checkpoints</em>中恢复变量，它同时提供了一些函数方法来运行这些<code>ops</code>。<em>checkpoints</em>是一种二进制文件，它把<code>variable name</code>和<code>tensor</code>值联系起来，默认的为<code>tf.Variable.name</code>。使用<em>checkpoints</em>最好的方法就是用<code>Saver</code>来载入它。</p>
<p><code>Saver</code>可以自动的给<em>checkpoints</em>文件命名，这使得我们在不同的训练阶段可以保存不同的<em>checkpoints</em>。例如我们可以用训练迭代次数来给<em>checkpoints</em>命名，为了防止训练阶段磁盘空间占用量过多，我们还可以选择只保存最近N个文件，或者每N个小时保存一次文件。</p>
<h2 id="保存变量"><a href="#保存变量" class="headerlink" title="保存变量"></a>保存变量</h2><p>用<code>tf.train.Saver()</code>创建一个<code>Saver</code>对象来控制模型中所有的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add an op to initialize the variables.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, initialize the variables, do some work, save the</span></div><div class="line"><span class="comment"># variables to disk.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  sess.run(init_op)</div><div class="line">  <span class="comment"># Do some work with the model.</span></div><div class="line">  <span class="comment"># ..</span></div><div class="line">  <span class="comment"># Save the variables to disk.</span></div><div class="line">  save_path = saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model saved in file: %s"</span> % save_path)</div></pre></td></tr></table></figure>
<p>为了自动给<em>checkpoints</em>文件命名我们可以传入一个<code>global_step</code>值给<code>save()</code>函数。例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">saver.save(sess, 'my-model', global_step=0) ==&gt; filename: 'my-model-0'</div><div class="line">...</div><div class="line">saver.save(sess, 'my-model', global_step=1000) ==&gt; filename: 'my-model-1000'</div></pre></td></tr></table></figure>
<h2 id="恢复变量"><a href="#恢复变量" class="headerlink" title="恢复变量"></a>恢复变量</h2><p>我们用同一个<code>Saver</code>对象来恢复变量，注意当我们从一个文件恢复变量时我们没必要提前对变量初始化。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and</span></div><div class="line"><span class="comment"># do some work with the model.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Restore variables from disk.</span></div><div class="line">  saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model restored."</span>)</div><div class="line">  <span class="comment"># Do some work with the model</span></div><div class="line">  <span class="comment"># ...</span></div></pre></td></tr></table></figure>
<h2 id="保存和恢复部分变量"><a href="#保存和恢复部分变量" class="headerlink" title="保存和恢复部分变量"></a>保存和恢复部分变量</h2><p>如果我们没有给<code>tf.train.Saver()</code>任何参数，它默认保存所有变量，每个变量都和它们的<code>name</code>联系起来。</p>
<p>有时候我们想要在<em>checkpoint</em>文件中给某个变量定义一个具体的<code>name</code>，例如我们想把一个变量取名为<code>&quot;weights&quot;</code>，我们想恢复它的值到一个新的名字叫<code>&quot;param&quot;</code>的变量中。</p>
<p>有时候我们想保存和恢复模型的某一组变量，例如我们训练了一个5层的神经网络，但我们想训练一个新的6层的神经网络，并且从5层的神经网络中恢复参数到新的6层模型的前5层中。</p>
<p>我们可以传入一个变量列表到<code>tf.train.Saver()</code>中，变量在<code>checkpoint</code>文件中的<code>name</code>就是<code>op</code>的<code>name</code>。我们也可以很简单的把<code>names</code>和变量组织成一个Python字典传入<code>tf.train.Saver()</code>中来保存下来，<code>keys</code>是我们使用的<code>names</code>，<code>values</code>是我们的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(..., name=<span class="string">'v1'</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">'v2'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Pass the variables as a dict:</span></div><div class="line">saver = tf.train.Saver(&#123;<span class="string">'v1'</span>: v1, <span class="string">'v2'</span>: v2&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Or pass them as a list.</span></div><div class="line">saver = tf.train.Saver([v1, v2])</div><div class="line"><span class="comment"># Passing a list is equivalent to passing a dict with the variable op names</span></div><div class="line"><span class="comment"># as keys:</span></div><div class="line">saver = tf.train.Saver(&#123;v.op.name: v <span class="keyword">for</span> v <span class="keyword">in</span> [v1, v2]&#125;)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ol>
<li>如果我们想保存不同模型的几组变量，我们可以创建很多个<code>saver</code>对象。而且相同的变量可以存储在不同的<code>saver</code>对象中，它们的值只有在<code>restore()</code>函数执行后才改变。</li>
<li>如果我们想恢复一组变量到模型中，我们必须事先初始化其他变量。</li>
</ol>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="external">tf.train.Saver  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" rel="external">Variables: Creation, Initialization, Saving, and Loading  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（四）：TensorBoard可视化]]></title>
      <url>/2017/06/27/%20TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文对TensorFlow的重要可视化工具TensorBoard进行学习。</p>
<p>当我们训练庞大且复杂的神经网络时，为了便于我们理解和调试TensorFlow程序，可视化工具显得尤为重要。而TensorFlow有一个很方便的可视化工具名为TensorBoard，我们可以利用它来可视化我们的<code>graph</code>以及训练的细节。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<hr>
<p>TensorBoard的界面如下图所示：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627153131_zidI6p_tensorboard.jpeg" alt="MNIST TensorBoard" width="700"></p>
<p>界面主要提供以下几种不同的数据可视化类型：</p>
<ul>
<li>SCALARS</li>
<li>IMAGES</li>
<li>AUDIO</li>
<li>GRAPHS</li>
<li>DISTRIBUTIONS</li>
<li>HISTOGRAMS</li>
<li>EMBEDDINGS</li>
<li>TEXT</li>
</ul>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在TensorFlow运行过程中，我们可以通过<code>summary operations</code>操作来保存它的运行日志，TensorBoard通过读取这些运行日志来实现可视化功能。下面是使用TensorBoard的生命周期：</p>
<ol>
<li><p>创建一个<code>TensorFlow graph</code>，用<code>summary operations</code>来保存我们需要了解的节点的信息。</p>
</li>
<li><p>我们需要整合所有节点的信息，如果一个一个提取太耗费精力和时间，我们可以选择<code>tf.summary.merge_all()</code>函数来把所有的<code>op</code>节点整合成一个单一节点，然后通过运行该节点来提取出所有<code>summary op</code>的信息。</p>
</li>
<li><p>用<code>tf.summary.FileWriter()</code>函数把提取出的所有的<code>summary op</code>信息存入磁盘中。我们同时可以用该函数把<code>graph</code>的信息存入磁盘，用来可视化神经网络的结构。</p>
</li>
<li><p>在训练过程中用<code>add_summary</code>命令把训练信息存入磁盘，训练结束后记得把<code>FileWriter</code>对象<code>close</code>。</p>
</li>
<li><p>打开终端，键入以下命令运行TensorBoard：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure>
<p>得到提示信息<code>“Starting TensorBoard...”</code>则运行成功，我们可以打开浏览器，在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板。</p>
<p><br></p>
</li>
</ol>
<h1 id="Summary-Operations"><a href="#Summary-Operations" class="headerlink" title="Summary Operations"></a>Summary Operations</h1><p>我们利用该操作来从TensorFlow运行过程中获得信息，然后在TensorBoard上展示出来，<code>summary ops</code>同样是<code>ops</code>的一种，它们和<code>tf.matmul</code>或者<code>tf.nn.relu</code>等操作一样。这意味着它们和其他<code>ops</code>相同，包含在一个<code>graph</code>中，读入一个<code>tensors</code>并且输出一个<code>tensors</code>。但是它们也有不同的地方，<code>summary ops</code>输出的向量还包含着连续的<code>protobufs</code>信息，它们可以被保存到磁盘上并且被TensorBoard读取进行可视化。</p>
<p>常见的<code>summary ops</code>如下：</p>
<ul>
<li>tf.summary.scalar</li>
<li>tf.summary.image</li>
<li>tf.summary.audio</li>
<li>tf.summary.text</li>
<li>tf.summary.histogram</li>
</ul>
<p><br></p>
<p>我们在这里主要介绍三种：</p>
<h2 id="Scalar"><a href="#Scalar" class="headerlink" title="Scalar"></a>Scalar</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, accuracy)</div></pre></td></tr></table></figure>
<p><code>Scalar</code>存储一些非向量的单一数值，通常会随着迭代次数变化，例如<code>accuracy</code>或者<code>cross_entropy</code>。我们可以在TensorBoard的<code>SCALARS</code>面板看到该类数据的详细信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_XxfLyL_scalar.jpeg" alt="SCALARS" width="700"></p>
<h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.image(<span class="string">'input'</span>, x_image, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<p>我们可以输出包含图像的<code>summary protobufs</code>，图像信息保存在<code>tensor</code>中，并且必须是4-D的形式<code>[batch_size, height, width, channels]</code>，其中，<code>channels</code>可以是以下形式：</p>
<ul>
<li>1: <code>tensor</code> is interpreted as Grayscale.</li>
<li>3: <code>tensor</code> is interpreted as RGB.</li>
<li>4: <code>tensor</code> is interpreted as RGBA.</li>
</ul>
<p>我们可以在TensorBoard的<code>IMAGES</code>面板看到图片信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_YAlrJU_images.jpeg" alt="IMAGES" width="700"></p>
<h2 id="Histogram"><a href="#Histogram" class="headerlink" title="Histogram"></a>Histogram</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.histogram(<span class="string">"weights"</span>, w)</div></pre></td></tr></table></figure>
<p>很多时候我们想要知道某些向量的分布，比如参数<code>weight</code>、<code>bias</code>或者某一层的输出向量，以及它们随着时间的变化规律，这时可以采用Histogram。</p>
<p>在TensorBoard中有两种显示向量分布的方式，分别是面板中的<code>DISTRIBUTIONS</code>和<code>HISTOGRAMS</code>，如下两图所示。</p>
<p><strong>DISTRIBUTIONS：</strong></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_w23F61_distribution.jpeg" alt="DISTRIBUTIONS" width="700"></p>
<p><strong>HISTOGRAMS：</strong></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_uvshkp_histogram.jpeg" alt="HISTOGRAMS" width="700"></p>
<h2 id="Merge-all"><a href="#Merge-all" class="headerlink" title="Merge_all"></a>Merge_all</h2><p>当我们定义好所有ops，我们可以用以下函数来把所有的<code>op</code>节点整合成一个单一节点，然后通过运行该节点来提取出所有<code>summary op</code>的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum = tf.summary.merge_all()</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h1><p>当我们训练无比庞大且复杂的神经网络时，我们一般会希望对整个网络的结构进行可视化，本文开头的图片展示的便是对MNIST数据集一个简单的五层神经网络的可视化。在提取了所有<code>summary_op</code>信息后，我们可以用以下函数把它们写进给定的磁盘目录下，在此同时可以把<code>graph</code>的信息一起写进去，这样我们便可以在TensorBoard的<code>GRAPHS</code>下对整个神经网络的结构进行可视化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">file_writer = tf.summary.FileWriter(&apos;/path/to/logs&apos;, sess.graph)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="Name-scoping-and-nodes"><a href="#Name-scoping-and-nodes" class="headerlink" title="Name scoping and nodes"></a>Name scoping and nodes</h2><p>然而直接保存得到的结构往往无比晦涩难懂，这时我们就可以考虑用前一个笔记中学到的<code>name_scope</code>对变量和<code>ops</code>进行分层。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'layer2'</span>):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'weight'</span>):</div><div class="line">        W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'bias'</span>):</div><div class="line">        b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'conv'</span>):</div><div class="line">        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'pool'</span>):</div><div class="line">        h_pool2 = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<p>这样我们就可以得到本文开端处的图片中展示的结构图，通常默认只显示最高层，双击每一个模块或者单机模块右上方的<code>+</code>，可以进一步展开，我们可以得到进一步详细的结构信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180059_C4hmAd_graph.jpeg" alt="GRAPHS" width="700"></p>
<p>TensorBoard的<code>graph</code>有两种依赖，<strong>数据依赖</strong>和<strong>控制依赖</strong>。数据依赖展示了<code>tensors</code>在两个<code>ops</code>之间的流动以及流动方向；控制依赖运用了虚线来表示。TensorBoard显示<code>graph</code>有两个区域，<strong>主区域</strong>和<strong>辅助区域</strong>，为了便于观察我们可以把一些高层次的依赖项多的节点从主区域移出到辅助区域，只需要在节点右键点击<code>Remove from main graph</code>即可，如上图所示我们把<code>train</code>移到辅助区域。</p>
<p><br></p>
<h2 id="Runtime-statistics"><a href="#Runtime-statistics" class="headerlink" title="Runtime statistics"></a>Runtime statistics</h2><p>通常来说搜集运行时间的统计数据对我们是很有帮助的，例如总的内存使用，总的计算时间，以及节点的<code>tensor</code>形状。我们可以在<code>GRAPHS</code>的侧边栏处通过选择<code>Compute time</code>和<code>Memory</code>来看每个节点处响应的信息，颜色越深表明相应的值越大。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170629223648_L7I2IB_Run metadata graph.jpeg" alt="Run metadata graph" width="700"></p>
<p>如下是一段代码示例，我们在第99、199、299……1999处保存<code>Runtime</code>统计数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000</span>):</div><div class="line">    <span class="comment">#...</span></div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:  <span class="comment"># Record execution statistics</span></div><div class="line">        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</div><div class="line">        run_metadata = tf.RunMetadata()</div><div class="line">        summary, _ = sess.run([merged, train_step],</div><div class="line">                              feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;,</div><div class="line">                              options=run_options,</div><div class="line">                              run_metadata=run_metadata)</div><div class="line">        train_writer.add_run_metadata(run_metadata, <span class="string">'step%d'</span> % i)</div><div class="line">        train_writer.add_summary(summary, i)</div><div class="line">        print(<span class="string">'Adding run metadata for'</span>, i)</div><div class="line">    <span class="comment">#...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="运行-TensorBoard"><a href="#运行-TensorBoard" class="headerlink" title="运行 TensorBoard"></a>运行 TensorBoard</h1><p>打开终端，键入以下命令运行TensorBoard：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure>
<p>这里的路径是我们用<code>tf.summary.FileWriter()</code>写入的路径。这里注意我们用<code>FileWriter</code>写入时可能会分为<code>train</code>和<code>test</code>两个路径，这时的TensorBoard路径应该为它们两个的上层路径，例如：</p>
<p><strong>.py文件:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_writer = tf.summary.FileWriter(<span class="string">'/tmp/tensorflow/mnist/my_example/train'</span>, sess.graph)</div><div class="line">test_writer = tf.summary.FileWriter(<span class="string">'/tmp/tensorflow/mnist/my_example/test'</span>)</div></pre></td></tr></table></figure>
<p><strong>终端命令：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=<span class="string">'/tmp/tensorflow/mnist/my_example/'</span></div></pre></td></tr></table></figure>
<p>如果我们想比较不同的网络模型的运行情况，例如我们想改超参数，想知道哪个值能获得更高的准确率，TensorBoard允许我们打开不同的模型保存的<code>log</code>，当TensorBoard打开一个路径时，它会遍历此路径下包括子目录下的所有的事件文件，每次它进入一个子目录，它都会载入它作为一个新的<code>run</code>，前段界面也会通过这个路径来组织数据。例如如下有一个已经组织好的TensorBoard <code>log</code>目录，有两个<code>runs</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/some/path/mnist_experiments/run1/events.out.tfevents.1456525581.name</div><div class="line">/some/path/mnist_experiments/run2/events.out.tfevents.1456525385.name</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=/some/path/mnist_experiments</div></pre></td></tr></table></figure>
<p>然后打开浏览器，在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板即可看到可视化结果。</p>
<p>如下是一个不同模型结构训练结果可视化的例子（代码在下面）：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630113432_HnboU8_more model.jpeg" alt="Different models" width="700"></p>
<p><br></p>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><p>我在MNIST上进行了简单的实现，为了更好的调试超参数，我实验了不同的卷积层、全连接层和是否开启<code>dropout</code>，完整代码可以在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/TensorFlow/TensorBoard" target="_blank" rel="external">GitHub</a>上下载，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Copyright 2017 Zangbo. All Rights Reserved.</span></div><div class="line"><span class="comment"># A simple MNIST classifier which displays summaries in TensorBoard.</span></div><div class="line"><span class="comment"># This is an unimpressive MNIST model, but it is a good example of using</span></div><div class="line"><span class="comment"># tf.name_scope to make a graph legible in the TensorBoard graph explorer, and of</span></div><div class="line"><span class="comment"># naming summary tags so that they are grouped meaningfully in TensorBoard.</span></div><div class="line"><span class="comment"># ==============================================================================</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment">### MNIST datasets ###</span></div><div class="line">LOGDIR = <span class="string">'/tmp/mnist_tutorial/'</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(train_dir=LOGDIR + <span class="string">'data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_layer</span><span class="params">(input, size_in, size_out, name=<span class="string">"conv"</span>)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">    w = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, size_in, size_out], stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div><div class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[size_out]), name=<span class="string">"B"</span>)</div><div class="line">    conv = tf.nn.conv2d(input, w, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line">    act = tf.nn.relu(conv + b)</div><div class="line">    tf.summary.histogram(<span class="string">"weights"</span>, w)</div><div class="line">    tf.summary.histogram(<span class="string">"biases"</span>, b)</div><div class="line">    tf.summary.histogram(<span class="string">"activations"</span>, act)</div><div class="line">    <span class="keyword">return</span> tf.nn.max_pool(act, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_layer</span><span class="params">(input, size_in, size_out, name=<span class="string">"fc"</span>)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">    w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div><div class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[size_out]), name=<span class="string">"B"</span>)</div><div class="line">    act = tf.nn.relu(tf.matmul(input, w) + b)</div><div class="line">    tf.summary.histogram(<span class="string">"weights"</span>, w)</div><div class="line">    tf.summary.histogram(<span class="string">"biases"</span>, b)</div><div class="line">    tf.summary.histogram(<span class="string">"activations"</span>, act)</div><div class="line">    <span class="keyword">return</span> act</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnist_model</span><span class="params">(learning_rate, use_two_conv, use_two_fc, use_dropout, hparam)</span>:</span></div><div class="line">  tf.reset_default_graph()</div><div class="line">  sess = tf.Session()</div><div class="line"></div><div class="line">  <span class="comment"># Setup placeholders, and reshape the data</span></div><div class="line">  x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">"x"</span>)</div><div class="line">  x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">  tf.summary.image(<span class="string">'input'</span>, x_image, <span class="number">3</span>)</div><div class="line">  y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">"labels"</span>)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> use_two_conv:</div><div class="line">    conv1 = conv_layer(x_image, <span class="number">1</span>, <span class="number">32</span>, <span class="string">"conv1"</span>)</div><div class="line">    conv_out = conv_layer(conv1, <span class="number">32</span>, <span class="number">64</span>, <span class="string">"conv2"</span>)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    conv1 = conv_layer(x_image, <span class="number">1</span>, <span class="number">64</span>, <span class="string">"conv"</span>)</div><div class="line">    conv_out = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">  flattened = tf.reshape(conv_out, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">if</span> use_two_fc:</div><div class="line">    fc1 = fc_layer(flattened, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>, <span class="string">"fc1"</span>)</div><div class="line">    <span class="keyword">if</span> use_dropout:</div><div class="line">      <span class="keyword">with</span> tf.name_scope(<span class="string">'dropout'</span>):</div><div class="line">        keep_prob = tf.constant(<span class="number">0.5</span>, name=<span class="string">'keep_prob'</span>)</div><div class="line">        fc1_dropout = tf.nn.dropout(fc1, keep_prob)</div><div class="line">        logits = fc_layer(fc1_dropout, <span class="number">1024</span>, <span class="number">10</span>, <span class="string">"fc2"</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      logits = fc_layer(fc1, <span class="number">1024</span>, <span class="number">10</span>, <span class="string">"fc2"</span>)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    logits = fc_layer(flattened, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">10</span>, <span class="string">"fc"</span>)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"xent"</span>):</div><div class="line">    xent = tf.reduce_mean(</div><div class="line">        tf.nn.softmax_cross_entropy_with_logits(</div><div class="line">            logits=logits, labels=y), name=<span class="string">"xent"</span>)</div><div class="line">    tf.summary.scalar(<span class="string">"xent"</span>, xent)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"train"</span>):</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"accuracy"</span>):</div><div class="line">    correct_prediction = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">    tf.summary.scalar(<span class="string">"accuracy"</span>, accuracy)</div><div class="line"></div><div class="line">  summ = tf.summary.merge_all()</div><div class="line"></div><div class="line">  sess.run(tf.global_variables_initializer())</div><div class="line">  writer = tf.summary.FileWriter(LOGDIR + hparam)</div><div class="line">  writer.add_graph(sess.graph)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2001</span>):</div><div class="line">    batch = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">      [train_accuracy, s] = sess.run([accuracy, summ], feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;)</div><div class="line">      writer.add_summary(s, i)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:  <span class="comment"># Record execution statistics</span></div><div class="line">      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</div><div class="line">      run_metadata = tf.RunMetadata()</div><div class="line">      _, s = sess.run([train_step, summ],</div><div class="line">                      feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;,</div><div class="line">                      options=run_options,</div><div class="line">                      run_metadata=run_metadata)</div><div class="line">      writer.add_run_metadata(run_metadata, <span class="string">'step%03d'</span> % i) <span class="comment">#add_run_metadata(run_metadata, tag, global_step=None)</span></div><div class="line">      writer.add_summary(s, i)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      sess.run(train_step, feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;)</div><div class="line">  writer.close()</div><div class="line">  sess.close()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_hparam_string</span><span class="params">(learning_rate, use_two_fc, use_two_conv, use_dropout)</span>:</span></div><div class="line">  conv_param = <span class="string">"conv=2"</span> <span class="keyword">if</span> use_two_conv <span class="keyword">else</span> <span class="string">"conv=1"</span></div><div class="line">  fc_param = <span class="string">"fc=2"</span> <span class="keyword">if</span> use_two_fc <span class="keyword">else</span> <span class="string">"fc=1"</span></div><div class="line">  dropout_param = <span class="string">"dropout"</span> <span class="keyword">if</span> use_dropout <span class="keyword">else</span> <span class="string">"no_dropout"</span></div><div class="line">  <span class="keyword">return</span> <span class="string">"lr_%.0E,%s,%s,%s"</span> % (learning_rate, conv_param, fc_param,dropout_param)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(LOGDIR):</div><div class="line">    tf.gfile.DeleteRecursively(LOGDIR)</div><div class="line">  tf.gfile.MakeDirs(LOGDIR)</div><div class="line">  <span class="comment">#Add other param to try different learning rate</span></div><div class="line">  <span class="keyword">for</span> learning_rate <span class="keyword">in</span> [<span class="number">1E-4</span>]:</div><div class="line">    <span class="comment"># Try different model architectures</span></div><div class="line">    <span class="keyword">for</span> use_two_fc <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">      <span class="keyword">for</span> use_two_conv <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">        <span class="keyword">for</span> use_dropout <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">          <span class="comment"># Construct a hyperparameter string for each one (example: "lr_1E-4,fc=2,conv=2,dropout)</span></div><div class="line">          hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv, use_dropout)</div><div class="line">          print(<span class="string">'Starting run for %s'</span> % hparam)</div><div class="line"></div><div class="line">          <span class="comment"># Actually run with the new settings</span></div><div class="line">          mnist_model(learning_rate, use_two_fc, use_two_conv, use_dropout, hparam)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  main()</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>以下链接均来自于Tensorflow官方网站，且均需要翻墙。</p>
<ul>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/tensorboard/README.md" target="_blank" rel="external">tensorflow/README.md at r1.2 · tensorflow/tensorflow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/summary" target="_blank" rel="external">Module: tf.summary  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard: Visualizing Learning  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/get_started/graph_viz" target="_blank" rel="external">TensorBoard: Graph Visualization  |  TensorFlow</a></li>
<li><a href="https://www.youtube.com/watch?v=eBbEDRsCmv4" target="_blank" rel="external">TensorBoard实践介绍（2017年TensorFlow开发大会） - YouTube</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 可视化 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（三）：Name_Scope和变量共享]]></title>
      <url>/2017/06/24/Tensorflow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文学习了TensorFlow中的四个基本函数以及<strong>变量共享(Sharing Variables)</strong>机制：</p>
<ul>
<li><code>tf.Variable()</code></li>
<li><code>tf.get_variable()</code></li>
<li><code>tf.name_scope()</code></li>
<li><code>tf.variable_scope()</code></li>
</ul>
<p>其中<code>tf.get_variable()</code>和<code>tf.variable_scope()</code>共同构成了Tensorflow的<strong>变量共享</strong>机制。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Variable-和get-variable"><a href="#Variable-和get-variable" class="headerlink" title="Variable()和get_variable()"></a>Variable()和get_variable()</h1><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable()"></a>Variable()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</div></pre></td></tr></table></figure>
<p>函数创建一个变量，该变量的<code>type</code>和<code>shape</code>由初始化的值而定，可以用<code>assign</code>函数改变变量的<code>shape</code>。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create two variables.</span></div><div class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),</div><div class="line">                      name=<span class="string">"weights"</span>)</div><div class="line">biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=<span class="string">"biases"</span>)</div><div class="line">...</div><div class="line"><span class="comment"># Add an op to initialize the variables.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Later, when launching the model</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Run the init operation.</span></div><div class="line">  sess.run(init_op)</div><div class="line">  ...</div><div class="line">  <span class="comment"># Use the model</span></div><div class="line">  ...</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="get-variable"><a href="#get-variable" class="headerlink" title="get_variable()"></a>get_variable()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = tf.get_variable(name, shape, dtype, initializer)</div></pre></td></tr></table></figure>
<p>函数创建一个新的变量或者返回一个已经存在的变量，它是<code>variable_scope</code>的重要组成部分。</p>
<ol>
<li><p>创建一个新的变量，这时默认<code>tf.get_variable_scope().reuse == False</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/v:0"</span></div></pre></td></tr></table></figure>
</li>
<li><p>为了重新使用(reuse)已经存在的变量，这时改变<code>tf.get_variable_scope().reuse == True</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>注意：</strong></p>
<ol>
<li>该函数必须指定<code>name</code>，它会在当前的<code>variable_scope</code>进行<code>reuse</code>检查。</li>
<li>该函数必须指定<code>shape</code>。</li>
<li>该函数定义时可以不进行初始化， 这样会使用<code>variable_scope</code>的默认初始化，如果这两者都没有，那么就使用<code>glorot_uniform_initializer</code>，即<code>xavier</code>进行初始化。</li>
</ol>
<p>一个基本的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])  <span class="comment"># v.name == "foo/v:0"</span></div><div class="line">    w = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])  <span class="comment"># w.name == "foo/w:0"</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>)  <span class="comment"># v.name == "foo/v:0"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ul>
<li><code>tf.Variable()</code><ol>
<li>可以不定义<code>name</code>。</li>
<li>检测到<code>name</code>冲突，系统会自己处理，不会报错。</li>
<li>每次都创建新的变量，<code>reuse=True</code>对它无影响。</li>
<li>需要初始化。</li>
</ol>
</li>
<li><code>tf.get_variable()</code><ol>
<li>必须定义<code>name</code>。</li>
<li>检测到<code>name</code>冲突，系统不会处理，会报错。</li>
<li><code>reuse=True</code>对它有影响，此时如果已经创建了变量对象，则把对象返回，如果没有，就创建一个新的。</li>
<li>可以不用初始化。</li>
<li>共享变量时使用。</li>
</ol>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>):</div><div class="line">    w1 = tf.get_variable(<span class="string">"w1"</span>, shape=[])</div><div class="line">    w2 = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"w2"</span>)</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    w1_p = tf.get_variable(<span class="string">"w1"</span>, shape=[])</div><div class="line">    w2_p = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w2"</span>)</div><div class="line"></div><div class="line">print(w1 <span class="keyword">is</span> w1_p, w2 <span class="keyword">is</span> w2_p)</div><div class="line"></div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#True  False</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="name-scope-和variable-scope"><a href="#name-scope-和variable-scope" class="headerlink" title="name_scope()和variable_scope()"></a>name_scope()和variable_scope()</h1><h2 id="name-scope"><a href="#name-scope" class="headerlink" title="name_scope()"></a>name_scope()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">name_scope(name)</div></pre></td></tr></table></figure>
<p>TensorFlow 可以有数以千计的节点，如此多而难以一下全部看到，甚至无法使用标准图表工具来展示。为简单起见，我们为<code>op</code>名划定范围，并且把该信息用于在图表中的节点上定义一个层级。默认情况下， 只有顶层节点会显示。一个<code>graph</code>是无数<code>name_scope</code>堆叠而来，用命令<code>with name_scope(...):</code>可以添加一个新的<code>name_scope</code>。</p>
<ul>
<li>如果输入一个字符串，那么会建立一个以该字符串命名的<code>name_scope</code>，在此下面定义的所有<code>op</code>均属于该<code>name_scope</code>。如果该字符串已经被定义，则会把命名改为<code>字符串_*</code>，<code>*</code>代表从1开始的阿拉伯数字。例如已经存在一个名为<code>&quot;a&quot;</code>的<code>name_scope</code>，再定义一个名为<code>&quot;a&quot;</code>的<code>name_scope</code>时，会自动把该<code>name_scope</code>改名为<code>&quot;a_1&quot;</code>，数字会依次累加。</li>
<li>如果用<code>with name_scope(...) as scope:</code>指令，那么可以利用该<code>name_scope</code>通过命令<code>with name_scope(scope):</code>重新进入已经建立好的<code>name_scope</code>下定义<code>op</code>。</li>
<li>如果<code>with name_scope(&quot;&quot;)</code>参数为空，则会重置当前<code>name_scope</code>为顶层空<code>name_scope</code>。</li>
</ul>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">  c = tf.constant(<span class="number">5.0</span>, name=<span class="string">"c"</span>)</div><div class="line">  <span class="keyword">assert</span> c.op.name == <span class="string">"c"</span></div><div class="line">  c_1 = tf.constant(<span class="number">6.0</span>, name=<span class="string">"c"</span>)</div><div class="line">  <span class="keyword">assert</span> c_1.op.name == <span class="string">"c_1"</span></div><div class="line"></div><div class="line">  <span class="comment"># Creates a scope called "nested"</span></div><div class="line">  <span class="keyword">with</span> g.name_scope(<span class="string">"nested"</span>) <span class="keyword">as</span> scope:</div><div class="line">    nested_c = tf.constant(<span class="number">10.0</span>, name=<span class="string">"c"</span>)</div><div class="line">    <span class="keyword">assert</span> nested_c.op.name == <span class="string">"nested/c"</span></div><div class="line"></div><div class="line">    <span class="comment"># Creates a nested scope called "inner".</span></div><div class="line">    <span class="keyword">with</span> g.name_scope(<span class="string">"inner"</span>):</div><div class="line">      nested_inner_c = tf.constant(<span class="number">20.0</span>, name=<span class="string">"c"</span>)</div><div class="line">      <span class="keyword">assert</span> nested_inner_c.op.name == <span class="string">"nested/inner/c"</span></div><div class="line"></div><div class="line">    <span class="comment"># Create a nested scope called "inner_1".</span></div><div class="line">    <span class="keyword">with</span> g.name_scope(<span class="string">"inner"</span>):</div><div class="line">      nested_inner_1_c = tf.constant(<span class="number">30.0</span>, name=<span class="string">"c"</span>)</div><div class="line">      <span class="keyword">assert</span> nested_inner_1_c.op.name == <span class="string">"nested/inner_1/c"</span></div><div class="line"></div><div class="line">      <span class="comment"># Treats `scope` as an absolute name_scope, and</span></div><div class="line">      <span class="comment"># switches to the "nested/" scope.</span></div><div class="line">      <span class="keyword">with</span> g.name_scope(scope):</div><div class="line">        nested_d = tf.constant(<span class="number">40.0</span>, name=<span class="string">"d"</span>)</div><div class="line">        <span class="keyword">assert</span> nested_d.op.name == <span class="string">"nested/d"</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> g.name_scope(<span class="string">""</span>):</div><div class="line">          e = tf.constant(<span class="number">50.0</span>, name=<span class="string">"e"</span>)</div><div class="line">          <span class="keyword">assert</span> e.op.name == <span class="string">"e"</span></div></pre></td></tr></table></figure>
<p><code>with g.name_scope(...) as scope:</code>中的<code>scope</code>存放了该<code>scope</code>的名字，可以在之后的<code>op</code>中直接使用该<code>scope</code>来给<code>op</code>命名。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">  <span class="keyword">with</span> g.name_scope(<span class="string">'my_layer'</span>) <span class="keyword">as</span> scope:</div><div class="line">    output = tf.constant(<span class="number">1</span>, name=scope)</div><div class="line"><span class="keyword">assert</span> output.op.name == <span class="string">"my_layer"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="variable-scope"><a href="#variable-scope" class="headerlink" title="variable_scope()"></a>variable_scope()</h2><p><code>variable_scope</code>函数将会在变量名前面加个前缀，同时标注<code>reuse</code>。利用缩进形式会分成有层次的<code>variable_scope</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">        v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/bar/v:0"</span></div></pre></td></tr></table></figure>
<p>我们可以通过<code>tf.get_variable_scope()</code>来得到当前的<code>variable_scope</code>，同时可以利用<code>tf.get_variable_scope().reuse_variables():</code>把当前的<code>variable_scope</code>的<code>reuse</code>改成<code>True</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">    tf.get_variable_scope().reuse_variables()</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div></pre></td></tr></table></figure>
<p>当一个<code>variable_scope</code>的<code>reuse</code>默认为<code>False</code>，但被改成了<code>True</code>以后，是无法再改回<code>False</code>的，而且如果一个<code>variable_scope</code>的<code>reuse</code>被改成了<code>True</code>，它下属的所有<code>variable_scope</code>的<code>reues</code>就都是<code>True</code>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"root"</span>):</div><div class="line">    <span class="comment"># At start, the scope is not reusing.</span></div><div class="line">    <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">        <span class="comment"># Opened a sub-scope, still not reusing.</span></div><div class="line">        <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">        <span class="comment"># Explicitly opened a reusing scope.</span></div><div class="line">        <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">True</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">            <span class="comment"># Now sub-scope inherits the reuse flag.</span></div><div class="line">            <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">True</span></div><div class="line">    <span class="comment"># Exited the reusing scope, back to a non-reusing one.</span></div><div class="line">    <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div></pre></td></tr></table></figure>
<p>我们也可以直接用<code>variable_scope</code>来代替参数名字进入该<code>variable_scope</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>) <span class="keyword">as</span> foo_scope:</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(foo_scope):</div><div class="line">    w = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(foo_scope, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">    w1 = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div><div class="line"><span class="keyword">assert</span> w1 <span class="keyword">is</span> w</div></pre></td></tr></table></figure>
<p>当我们用之前存在的<code>variable_scope</code>进入一个<code>variable_scope</code>时，我们会跳出当前的<code>variable_scope</code>而进入一个完全不同的<code>variable_scope</code>，哪怕它在某个<code>variable_scope</code>的缩进下面，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>) <span class="keyword">as</span> foo_scope:</div><div class="line">    <span class="keyword">assert</span> foo_scope.name == <span class="string">"foo"</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"baz"</span>) <span class="keyword">as</span> other_scope:</div><div class="line">        <span class="keyword">assert</span> other_scope.name == <span class="string">"bar/baz"</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(foo_scope) <span class="keyword">as</span> foo_scope2:</div><div class="line">            <span class="keyword">assert</span> foo_scope2.name == <span class="string">"foo"</span>  <span class="comment"># Not changed.</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="区别-1"><a href="#区别-1" class="headerlink" title="区别"></a>区别</h2><p><code>name_scope</code>是给<code>op_name</code>加前缀，<code>variable_scope</code>是给<code>get_variable()</code>创建的变量的名字加前缀。</p>
<p><code>name_scope</code>可以在<code>variable_scope</code>中打开，但是<code>name_scope</code>仅仅会影响<code>op</code>的<code>name</code>，而不会影响<code>variable</code>的<code>name</code>，但是<code>variable_scope</code>的会影响<code>op</code>的<code>name</code>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"bar"</span>):</div><div class="line">        v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">        x = <span class="number">1.0</span> + v</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/v:0"</span></div><div class="line"><span class="keyword">assert</span> x.op.name == <span class="string">"foo/bar/add"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Sharing-Variables"><a href="#Sharing-Variables" class="headerlink" title="Sharing Variables"></a>Sharing Variables</h1><p>Tensorflow的<strong>变量共享(Sharing Variables)</strong>机制主要是由<code>tf.get_variable()</code>和<code>tf.variable_scope()</code>构成的。</p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>设想一下，假如我们有一个神经网络，它只有两层卷积层组成，而且两层卷积层的输入输出尺寸相同并且卷积核尺寸也相同，如果我们用<code>tf.Variable()</code>来定义这样一个模型，代码大概是下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></div><div class="line">    conv1_weights = tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>]),</div><div class="line">        name=<span class="string">"conv1_weights"</span>)</div><div class="line">    conv1_biases = tf.Variable(tf.zeros([<span class="number">32</span>]), name=<span class="string">"conv1_biases"</span>)</div><div class="line">    conv1 = tf.nn.conv2d(input_images, conv1_weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    relu1 = tf.nn.relu(conv1 + conv1_biases)</div><div class="line"></div><div class="line">    conv2_weights = tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>]),</div><div class="line">        name=<span class="string">"conv2_weights"</span>)</div><div class="line">    conv2_biases = tf.Variable(tf.zeros([<span class="number">32</span>]), name=<span class="string">"conv2_biases"</span>)</div><div class="line">    conv2 = tf.nn.conv2d(relu1, conv2_weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    <span class="keyword">return</span> tf.nn.relu(conv2 + conv2_biases)</div></pre></td></tr></table></figure>
<p>其中有四个不同的变量：<code>conv1_weights</code>、<code>conv1_biases</code>、<code>conv2_weights</code>、<code>conv2_biases</code>，那么问题出现了，当我们反复使用这样一个模型时，每次都要开辟新的存储空间给四个变量，例如我们有两张不同的图片<code>image1</code>和<code>image2</code>，我们先后把它们送入该模型中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># First call creates one set of 4 variables.</span></div><div class="line">result1 = my_image_filter(image1)</div><div class="line"><span class="comment"># Another set of 4 variables is created in the second call.</span></div><div class="line">result2 = my_image_filter(image2)</div></pre></td></tr></table></figure>
<p>总共创建了8个不同的变量。极大的浪费了资源，如果图片数量增多，模型变得复杂，那么耗费的资源是巨大的。因此为了解决这个问题，Tensorflow出现了变量共享的机制。</p>
<p><br></p>
<h2 id="变量共享方法"><a href="#变量共享方法" class="headerlink" title="变量共享方法"></a>变量共享方法</h2><p>下面，我们用<strong>变量共享</strong>来实现上面的模型：</p>
<p>首先利用<code>tf.get_variable()</code>定义一个实现卷积操作的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></div><div class="line">    <span class="comment"># Create variable named "weights".</span></div><div class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape,</div><div class="line">        initializer=tf.random_normal_initializer())</div><div class="line">    <span class="comment"># Create variable named "biases".</span></div><div class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape,</div><div class="line">        initializer=tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    conv = tf.nn.conv2d(input, weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</div></pre></td></tr></table></figure>
<p>然后利用<code>tf.variable_scope()</code>来创建两个在不同<code>variable_scope</code>的卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</div><div class="line">        <span class="comment"># Variables created here will be named "conv1/weights", "conv1/biases".</span></div><div class="line">        relu1 = conv_relu(input_images, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</div><div class="line">        <span class="comment"># Variables created here will be named "conv2/weights", "conv2/biases".</span></div><div class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>如果我们直接调用<code>my_image_filter</code>两次会发生什么呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">result1 = my_image_filter(image1)</div><div class="line">result2 = my_image_filter(image2)</div><div class="line"><span class="comment"># ValueError(... conv1/weights already exists ...)</span></div></pre></td></tr></table></figure>
<p>程序报错，<code>tf.get_variable()</code>会检查变量是否已经存在并且是否共享。如果想共享它们，可以通过设置<code>reuse_variables()</code>来实现，它会把当前的<code>reuse</code>改为<code>True</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"image_filters"</span>) <span class="keyword">as</span> scope:</div><div class="line">    result1 = my_image_filter(image1)</div><div class="line">    scope.reuse_variables()</div><div class="line">    result2 = my_image_filter(image2)</div></pre></td></tr></table></figure>
<p>这样不管我们处理多少张图片，都只会创建四个变量，极大的节省了资源。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>以下链接均来自于Tensorflow官方文档，且均需要翻墙。</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="external">tf.Variable  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/get_variable" target="_blank" rel="external">tf.get_variable  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/name_scope" target="_blank" rel="external">tf.name_scope  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope" target="_blank" rel="external">tf.Graph  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" target="_blank" rel="external">tf.variable_scope  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/variable_scope" target="_blank" rel="external">Sharing Variables  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（四）：获取某地历史天气数据]]></title>
      <url>/2017/06/22/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了爬虫的一个简单实战应用，因为前不久做了个竞赛需要用到历史天气，所以就写了个简单的爬虫程序，利用Requests库和Beautiful Soup库爬取某地的历史天气记录。</p>
<p>中间的数据整合方便起见调用了numpy库和pandas库，关于两者的详细使用方法大家可以去查阅相关资料。</p>
<p>所有代码基于python3.5版本。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>最近参加了天池的一个大数据竞赛，因为需要用到江苏省扬中市的历史天气信息，所以就写了个简单的爬虫程序来爬取。有很多网站都提供了历史天气，这里我们选择的网站是：<a href="http://lishi.tianqi.com/yangzhong/index.html" target="_blank" rel="external">扬中市历史天气-历史天气网</a>。</p>
<p><br></p>
<h1 id="分析网站"><a href="#分析网站" class="headerlink" title="分析网站"></a>分析网站</h1><ol>
<li><p>打开上述网站，往下滑动，可以看到有一栏记录了该地每个月的天气信息，右键打开审查元素，查看该信息所在的标签，发现每个月的天气信息都在标签<code>&lt;div class=&#39;tqtongji1&#39;&gt;</code>下面的<code>&lt;a&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_p0q980_2.jpeg" alt="历史月份信息"></center>

<p><br></p>
</li>
<li><p>用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;tqtongji1&#39;).find_all(&#39;a&#39;)</code>命令得到所有月份的信息，提取出其中的链接，这里只提取最近30个月的信息。</p>
<p><br></p>
</li>
<li><p>打开其中一个月份链接，右键审查元素，发现该月份每天的天气信息都在标签<code>&lt;div class=&#39;tqtongji2&#39;&gt;</code>下面的<code>&lt;li&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_XtWKBE_3.jpeg" alt="部分天气信息"></center>

<p><br></p>
</li>
<li><p>用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;tqtongji2&#39;).find_all(&#39;li&#39;)</code>命令得到该月份每天的部分天气信息并保存在<code>list</code>中，同时用<code>find(&#39;div&#39;, class_=&#39;tqtongji2&#39;).find_all(&#39;a&#39;)</code>命令提取出其中的链接。</p>
<p><br></p>
</li>
<li><p>打开其中一天的链接，右键审查元素，发现该天的其他天气信息都在标签<code>&lt;div class=&#39;history_sh&#39;&gt;</code>下面的<code>&lt;span&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_XNW30r_4.jpeg" alt="其他天气信息"></center>

<p><br></p>
</li>
<li><p>用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;history_sh&#39;).find_all(&#39;span&#39;)</code>命令提取出该天的其他天气信息并保存在<code>list</code>中。</p>
<p><br></p>
</li>
<li><p>利用numpy数组和pandas的DataFrame结构对提取出的数据进行整合，最后输出为CSV文件。</p>
</li>
</ol>
<p><br></p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">	</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">History_weather</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_url</span><span class="params">(self, url)</span>:</span></div><div class="line">        all_weather_index = self.get_all_weather_index(url)</div><div class="line">        all_weather_index = all_weather_index[:<span class="number">30</span>]</div><div class="line">        print(<span class="string">"Get all weather index!"</span>)</div><div class="line">        result_weather = self.get_all_weather(all_weather_index)</div><div class="line">        result_weather.to_csv(<span class="string">'all_weather.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">        print(<span class="string">'Save all weather success!'</span>)</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_soup</span><span class="params">(self, url)</span>:</span></div><div class="line">        headers = &#123;</div><div class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 \</span></div><div class="line">            (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8'&#125;</div><div class="line">        html = requests.get(url, headers)</div><div class="line">        soup = BeautifulSoup(html.text, <span class="string">'lxml'</span>)</div><div class="line">        <span class="keyword">return</span> soup</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_all_weather_index</span><span class="params">(self, url)</span>:</span></div><div class="line">        all_weather_soup = self.get_soup(url)</div><div class="line">        all_weather_index = all_weather_soup.find(</div><div class="line">            <span class="string">'div'</span>, class_=<span class="string">'tqtongji1'</span>).find_all(<span class="string">'a'</span>)</div><div class="line">        <span class="keyword">return</span> all_weather_index</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_all_weather</span><span class="params">(self, all_weather_index)</span>:</span></div><div class="line">        all_month_weather = list()</div><div class="line">        day_weather = list()</div><div class="line">        	</div><div class="line">        <span class="keyword">for</span> weather <span class="keyword">in</span> all_weather_index:</div><div class="line">            month_url = weather[<span class="string">'href'</span>]</div><div class="line">            month_name = weather.get_text()</div><div class="line">            month_weather_soup = self.get_soup(month_url)</div><div class="line">            month_weather = month_weather_soup.find(</div><div class="line">                <span class="string">'div'</span>, class_=<span class="string">'tqtongji2'</span>).find_all(<span class="string">'li'</span>)</div><div class="line">            day_weather_url = month_weather_soup.find(</div><div class="line">                <span class="string">'div'</span>, class_=<span class="string">'tqtongji2'</span>).find_all(<span class="string">'a'</span>)</div><div class="line">            <span class="keyword">for</span> day <span class="keyword">in</span> day_weather_url:</div><div class="line">                day_url = day[<span class="string">'href'</span>]</div><div class="line">                day_soup = self.get_soup(day_url)</div><div class="line">                day_text = day_soup.find(</div><div class="line">                    <span class="string">'div'</span>, class_=<span class="string">'history_sh'</span>).find_all(<span class="string">'span'</span>)</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> day_text:</div><div class="line">                    day_weather.append(i.get_text())</div><div class="line">            weather_list = list()</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> month_weather:</div><div class="line">                weather_list.append(i.get_text())</div><div class="line">            weather_list = weather_list[<span class="number">6</span>:]</div><div class="line">            all_month_weather = all_month_weather + weather_list</div><div class="line">            print(<span class="string">"Get weather :"</span> + month_name)</div><div class="line">            	</div><div class="line">        day_weather = np.array(day_weather).reshape(<span class="number">-1</span>, <span class="number">8</span>)</div><div class="line">        day_weather = DataFrame(day_weather, columns=[</div><div class="line">            <span class="string">'Ultraviolet'</span>, <span class="string">'Dress'</span>, <span class="string">'Travel'</span>, <span class="string">'Comfort_level'</span>,</div><div class="line">            <span class="string">'Morning_exercise'</span>, <span class="string">'Car_wash'</span>, <span class="string">'Drying_index'</span>, <span class="string">'Breath_allergy'</span>])</div><div class="line">        all_month_weather = np.array(all_month_weather).reshape(<span class="number">-1</span>, <span class="number">6</span>)</div><div class="line">        all_month_weather = DataFrame(all_month_weather, columns=[</div><div class="line">            <span class="string">'Date'</span>, <span class="string">'Max_temp'</span>, <span class="string">'Min_temp'</span>, </div><div class="line">            <span class="string">'Whether'</span>, <span class="string">'Wind_direction'</span>, <span class="string">'Wind_power'</span>])</div><div class="line">        print(all_month_weather.shape)</div><div class="line">        print(day_weather.shape)</div><div class="line">        result_weather = pd.merge(</div><div class="line">            all_month_weather, day_weather, left_index=<span class="keyword">True</span>, right_index=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> result_weather</div><div class="line">	</div><div class="line">history_weather = History_weather()</div><div class="line">begin_url = <span class="string">'http://lishi.tianqi.com/yangzhong/index.html'</span></div><div class="line">history_weather.begin_url(begin_url)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>打开<code>all_weather.csv</code>文件：</p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Max_temp</th>
<th>Min_temp</th>
<th>Whether</th>
<th>…</th>
<th>Breath_allergy</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017-04-01</td>
<td>18</td>
<td>8</td>
<td>晴</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>2017-04-02</td>
<td>22</td>
<td>10</td>
<td>晴</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>2017-04-03</td>
<td>24</td>
<td>12</td>
<td>多云</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>2014-11-30</td>
<td>10</td>
<td>2</td>
<td>多云转晴</td>
<td>…</td>
<td>极不易发</td>
</tr>
</tbody>
</table>
<p>共得到910天、14项天气特征数据。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>以上只是简单的调用Requests和Beautiful Soup来进行爬取，是一个非常简单的爬虫实现，后面我们会学习更加高阶的爬虫方法。</p>
<p>爬虫源代码可以在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/Web%20Crawler/history%20weather" target="_blank" rel="external">Github</a>上下载。</p>
<p><br></p>
</the></excerpt>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（三）：BeautifulSoup基础]]></title>
      <url>/2017/06/15/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Beautiful Soup库以及一些基础用法，并主要介绍了<code>find_all()</code>和<code>find()</code>    两个搜索方法。利用Beautiful Soup我们可以很方便的从页面中提取信息，该库兼容python2和3版本。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Beautiful-Soup介绍"><a href="#Beautiful-Soup介绍" class="headerlink" title="Beautiful Soup介绍"></a>Beautiful Soup介绍</h1><p>我们之前已经用Requests获得了需要的页面，接下来需要从页面中提取出我们想要的信息，我们可以选择用正则表达式，但对于很多新手来说正则表达式使用的不熟练会产生很多不必要的麻烦。而对于很多任务来说，有一个工具即高效又便捷，它就是Beautiful Soup。</p>
<p>Beautiful Soup是一个可以从HTML或XML文件中提取数据的Python库，它能够通过你喜欢的转换器实现惯用的文档导航，查找，修改文档的方式。因为BeautifulSoup用法太多，本文仅介绍在爬虫中常用的功能，如果需要了解更多，请查阅官方文档。</p>
<p>文章中代码输出结果用#表示。</p>
<p><br></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>下面一段HTML代码将作为我们的例子反复使用：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">html_doc = """</div><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">title</span>&gt;</span>The Dormouse's story<span class="tag">&lt;/<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span><span class="tag">&lt;<span class="name">b</span>&gt;</span>The Dormouse's story<span class="tag">&lt;/<span class="name">b</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"story"</span>&gt;</span>Once upon a time there were three little sisters; and their names were</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/elsie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link1"</span>&gt;</span>Elsie<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/lacie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link2"</span>&gt;</span>Lacie<span class="tag">&lt;/<span class="name">a</span>&gt;</span> and</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/tillie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link3"</span>&gt;</span>Tillie<span class="tag">&lt;/<span class="name">a</span>&gt;</span>;</div><div class="line">and they lived at the bottom of a well.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"story"</span>&gt;</span>...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">"""</div></pre></td></tr></table></figure>
<p>使用BeautifulSoup解析这段代码，能够得到一个<code>BeautifulSoup</code>的对象，并能按照标准的缩进格式的结构输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc)</div><div class="line">	</div><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;title&gt;</span></div><div class="line"><span class="comment">#    The Dormouse's story</span></div><div class="line"><span class="comment">#   &lt;/title&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="title"&gt;</span></div><div class="line"><span class="comment">#    &lt;b&gt;</span></div><div class="line"><span class="comment">#     The Dormouse's story</span></div><div class="line"><span class="comment">#    &lt;/b&gt;</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;</span></div><div class="line"><span class="comment">#     Elsie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ,</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Lacie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    and</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/tillie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Tillie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ; and they lived at the bottom of a well.</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    ...</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="安装-Beautiful-Soup"><a href="#安装-Beautiful-Soup" class="headerlink" title="安装 Beautiful Soup"></a>安装 Beautiful Soup</h1><p>Beautiful Soup有3和4两个版本，3版本目前已经停止开发，建议使用4版本。</p>
<p>我们可以很方便的使用pip进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install beautifulsoup4</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="安装解析器"><a href="#安装解析器" class="headerlink" title="安装解析器"></a>安装解析器</h1><p>Beautiful Soup支持Python标准库中的HTML解析器，还支持一些第三方的解析器，下表列出了主要的解析器，以及它们的优缺点：</p>
<table>
<thead>
<tr>
<th>解析器</th>
<th>使用方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python标准库</td>
<td><code>BeautifulSoup(markup, &quot;html.parser&quot;)</code></td>
<td>Python的内置标准库执行速度适中文档容错能力强</td>
<td>Python 2.7.3 or 3.2.2前的版本中文档容错能力差</td>
</tr>
<tr>
<td>lxml HTML 解析器</td>
<td><code>BeautifulSoup(markup, &quot;lxml&quot;)</code></td>
<td>速度快文档容错能力强</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>lxml XML 解析器</td>
<td><code>BeautifulSoup(markup, [&quot;lxml&quot;, &quot;xml&quot;])``BeautifulSoup(markup, &quot;xml&quot;)</code></td>
<td>速度快唯一支持XML的解析器</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>html5lib</td>
<td><code>BeautifulSoup(markup, &quot;html5lib&quot;)</code></td>
<td>最好的容错性以浏览器的方式解析文档生成HTML5格式的文档</td>
<td>速度慢不依赖外部扩展</td>
</tr>
</tbody>
</table>
<p>这里推荐使用lxml作为解析器，因为它的效率更高。</p>
<p>可以用pip来安装lxml：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install lxml</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="使用Beautiful-Soup"><a href="#使用Beautiful-Soup" class="headerlink" title="使用Beautiful Soup"></a>使用Beautiful Soup</h1><p>将一段文档传入BeautifulSoup 的构造方法，就能得到一个文档的对象，我们可以制定解析器。例如，我们将开头的例子传入BeautifulSoup，并使用lxml解析器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure>
<p>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种： <code>Tag</code> , <code>NavigableString</code> , <code>BeautifulSoup</code> , <code>Comment</code> 。</p>
<p><br></p>
<p>我们重点讲两个搜索方法：<code>find()</code>和<code>find_all()</code>，我们用这两种方法可以根据HTML文档的标签快速找到我们需要的内容。</p>
<p>以开头提到的文档作为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">html_doc = <span class="string">"""</span></div><div class="line">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</div><div class="line">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</div><div class="line">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</div><div class="line">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</div><div class="line">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</div><div class="line">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</div><div class="line">and they lived at the bottom of a well.&lt;/p&gt;</div><div class="line">&lt;p class="story"&gt;...&lt;/p&gt;</div><div class="line">"""</div><div class="line">	</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc,<span class="string">'lxml'</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="find-all"><a href="#find-all" class="headerlink" title="find_all()"></a>find_all()</h2><p><strong>find_all(name, attrs, recursive, text, kwargs)</strong></p>
<h3 id="name-参数"><a href="#name-参数" class="headerlink" title="name 参数"></a>name 参数</h3><p><code>name</code>参数可以查找所有名字为 <code>name</code> 的tag：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div></pre></td></tr></table></figure>
<p><code>name</code>参数的值可以是字符串、正则表达式、列表、方法或是 <code>True</code></p>
<p>例如：</p>
<p>在<code>find_all()</code>中传入一个<strong>字符串</strong>参数，Beautiful Soup会查找与字符串完整匹配的内容，下面的例子用于查找文档中所有的<code>&lt;b&gt;</code>标签:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'b'</span>)</div><div class="line"><span class="comment"># [&lt;b&gt;The Dormouse's story&lt;/b&gt;]</span></div></pre></td></tr></table></figure>
<p>如果传入<strong>列表</strong>参数，Beautiful Soup会将与列表中任一元素匹配的内容返回。下面代码找到文档中所有<code>&lt;a&gt;</code>标签和<code>&lt;b&gt;</code>标签:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all([<span class="string">"a"</span>, <span class="string">"b"</span>])</div><div class="line"><span class="comment"># [&lt;b&gt;The Dormouse's story&lt;/b&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><code>find_all()</code>方法搜索当前tag的所有tag子节点，并判断是否符合过滤器的条件。</p>
<p><br></p>
<h3 id="keyword-参数"><a href="#keyword-参数" class="headerlink" title="keyword 参数"></a>keyword 参数</h3><ol>
<li><p>如果一个指定名字的参数不是搜索内置的参数名，搜索时会把该参数当作指定名字tag的属性来搜索，如果包含一个名字为 <code>id</code> 的参数，Beautiful Soup会搜索每个tag的”id”属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(id=<span class="string">'link2'</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><br></p>
</li>
<li><p>如果传入 <code>href</code> 参数，Beautiful Soup会搜索每个tag的”href”属性:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(href=re.compile(<span class="string">"elsie"</span>))</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><br></p>
</li>
<li><p>利用<code>true</code>参数在文档树中查找所有包含 <code>id</code> 属性的tag，无论 <code>id</code> 的值是什么:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.find_all(id=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
</li>
</ol>
<p>搜索指定名字的属性时可以使用的参数值包括字符串、正则表达式、列表或是 <code>True</code></p>
<p><br></p>
<h3 id="按CSS搜索"><a href="#按CSS搜索" class="headerlink" title="按CSS搜索"></a>按CSS搜索</h3><p>按照CSS类名搜索tag的功能非常实用，但标识CSS类名的关键字 <code>class</code> 在Python中是保留字，使用 <code>class</code> 做参数会导致语法错误。这里我们可以通过 <code>class_</code> 参数搜索有指定CSS类名的tag:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>, class_=<span class="string">"sister"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><code>class_</code> 参数同样接受不同类型的过滤器：字符串、正则表达式、方法或 <code>True</code> 。</p>
<p><br></p>
<h3 id="text-参数"><a href="#text-参数" class="headerlink" title="text 参数"></a>text 参数</h3><p>通过 <code>text</code> 参数可以搜搜文档中的字符串内容。与 <code>name</code> 参数的可选值一样，<code>text</code> 参数接受字符串、正则表达式、方法或 <code>True</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">soup.find_all(text=<span class="string">"Elsie"</span>)</div><div class="line"><span class="comment"># [u'Elsie']</span></div><div class="line">	</div><div class="line">soup.find_all(text=[<span class="string">"Tillie"</span>, <span class="string">"Elsie"</span>, <span class="string">"Lacie"</span>])</div><div class="line"><span class="comment"># [u'Elsie', u'Lacie', u'Tillie']</span></div><div class="line">	</div><div class="line">soup.find_all(text=re.compile(<span class="string">"Dormouse"</span>))</div><div class="line"><span class="comment"># [u"The Dormouse's story", u"The Dormouse's story"]</span></div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_the_only_string_within_a_tag</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="string">""</span>Return <span class="keyword">True</span> <span class="keyword">if</span> this string <span class="keyword">is</span> the only child of its parent tag.<span class="string">""</span></div><div class="line">    <span class="keyword">return</span> (s == s.parent.string)</div><div class="line">	</div><div class="line">soup.find_all(text=is_the_only_string_within_a_tag)</div><div class="line"><span class="comment"># [u"The Dormouse's story", u"The Dormouse's story", u'Elsie', u'Lacie', u'Tillie', u'...']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h3><p>下面两行代码是等价的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>)</div><div class="line">soup(<span class="string">"a"</span>)</div></pre></td></tr></table></figure>
<p>这两行也是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.title.find_all(text=<span class="keyword">True</span>)</div><div class="line">soup.title(text=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="find"><a href="#find" class="headerlink" title="find()"></a>find()</h2><p><strong>find(name, attrs, recursive, text, kwargs)</strong></p>
<p><code>find_all()</code> 方法将返回文档中符合条件的所有tag，尽管有时候我们只想得到一个结果.比如文档中只有一个<code>&lt;body&gt;</code>标签，那么使用 <code>find_all()</code>方法来查找<code>&lt;body&gt;</code>标签就不太合适， 使用 <code>find_all</code> 方法并设置 <code>limit=1</code> 参数不如直接使用 <code>find()</code> 方法。下面两行代码是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'title'</span>, limit=<span class="number">1</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div><div class="line">	</div><div class="line">soup.find(<span class="string">'title'</span>)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure>
<p>唯一的区别是 <code>find_all()</code> 方法的返回结果是值包含一个元素的列表,而 <code>find()</code> 方法直接返回结果.</p>
<p><code>find_all()</code> 方法没有找到目标是返回空列表， <code>find()</code> 方法找不到目标时返回 <code>None</code>。</p>
<p><br></p>
<h3 id="简写-1"><a href="#简写-1" class="headerlink" title="简写"></a>简写</h3><p><code>soup.head.title</code>是利用tag的名字来简写，这个简写的原理就是多次调用当前tag的<code>find()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.head.title</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div><div class="line">	</div><div class="line">soup.find(<span class="string">"head"</span>).find(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="输出文本"><a href="#输出文本" class="headerlink" title="输出文本"></a>输出文本</h1><p>如果只想得到tag中包含的文本内容，那么可以采用<code>get_text()</code>方法，这个方法获取到tag中包含的所有文版内容，包括子孙tag中的内容，并将结果作为Unicode字符串返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">'&lt;a href="http://example.com/"&gt;\nI linked to &lt;i&gt;example.com&lt;/i&gt;\n&lt;/a&gt;'</span></div><div class="line">soup = BeautifulSoup(markup)</div><div class="line">	</div><div class="line">soup.get_text()</div><div class="line"><span class="comment"># u'\nI linked to example.com\n'</span></div><div class="line">soup.i.get_text()</div><div class="line"><span class="comment"># u'example.com'</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="关于编码"><a href="#关于编码" class="headerlink" title="关于编码"></a>关于编码</h1><p>使用Beautiful Soup解析后，文档都被转换成了Unicode。</p>
<p>Beautiful Soup用了<strong>编码自动检测</strong>子库来识别当前文档编码并转换成Unicode编码。<code>BeautifulSoup</code>对象的<code>.original_encoding</code>属性记录了自动识别编码的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">"&lt;h1&gt;Sacr\xc3\xa9 bleu!&lt;/h1&gt;"</span></div><div class="line">soup = BeautifulSoup(markup,<span class="string">'lxml'</span>)</div><div class="line">soup.original_encoding</div><div class="line"><span class="comment"># 'utf-8'</span></div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p>如果先用<code>r = requests.get(url)</code>指令获得了html页面，建议先用指令<code>r.encoding</code>判断<code>r.text</code>所用的编码方式和源html是否相同（可以通过查看html中的charset= 来判断源html采用的编码方式），如果不同的话要用<code>r.encoding=</code>进行修改，再用<code>soup = BeautifulSoup(r.text,&#39;lxml&#39;)</code>命令。因为<code>r.text</code>会根据当前的编码方式把html转化为Unicode编码输出，如果html包含汉字，而没有采用<code>utf-8</code>进行编码的话，<code>r.text</code>没有办法把汉字转化为相应的Unicode编码，再送进Beautiful Soup中也就无法得到汉字信息了。</p>
<p>也可以直接用<code>soup = BeautifulSoup(r.content,&#39;lxml&#39;)</code>指令，因为<code>r.content</code>返回的是bytes型原始编码信息，而Beautiful Soup可以用<strong>编码自动检测</strong>子库来识别当前文档编码并转换成Unicode编码，但获取文本信息时不建议这样使用，一般来说图像等多媒体内容用content更多一些。</p>
<p><br></p>
<p>而通过Beautiful Soup<strong>输出文档</strong>时，不管输入文档是什么编码方式，输出编码均为UTF-8编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">b'''</span></div><div class="line">&lt;html&gt;</div><div class="line">  &lt;head&gt;</div><div class="line">    &lt;meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" /&gt;</div><div class="line">  &lt;/head&gt;</div><div class="line">  &lt;body&gt;</div><div class="line">    &lt;p&gt;Sacr\xe9 bleu!&lt;/p&gt;</div><div class="line">  &lt;/body&gt;</div><div class="line">&lt;/html&gt;</div><div class="line">'''</div><div class="line">	</div><div class="line">soup = BeautifulSoup(markup)</div><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;meta content="text/html; charset=utf-8" http-equiv="Content-type" /&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p&gt;</span></div><div class="line"><span class="comment">#    Sacré bleu!</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure>
<p>如果不想用UTF-8编码输出，可以将编码方式传入<code>prettify()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(soup.prettify(<span class="string">"latin-1"</span>))</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;meta content="text/html; charset=latin-1" http-equiv="Content-type" /&gt;</span></div><div class="line"><span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>关于Beautiful Soup还有很多其他用法，这里只是重点介绍了运用最多的两个搜索方法<code>find_all()</code>和<code>find()</code>，如果想了解更多的使用方法，请参考官方文档。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>官方文档：<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html#" target="_blank" rel="external">Beautiful Soup Documentation — Beautiful Soup 4.4.0 documentation</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（二）：Requests基础]]></title>
      <url>/2017/06/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Requests库以及一些基础用法，基于python3.5版本<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Requests介绍"><a href="#Requests介绍" class="headerlink" title="Requests介绍"></a>Requests介绍</h1><p>Requests是一个基于urllib3的HTTP库，简单优美，支持python2.6–3.5的各版本，具有无比强大的功能，完全满足今日web的需求。在爬虫学习过程中，我们用它向服务器发送请求，以获取我们需要的信息。</p>
<p><br></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>只需要在终端中运行以下代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install requests</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>我们先来看一个具体的实例来感受下Request：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'http://zangbo.me'</span>, auth=(<span class="string">'user'</span>, <span class="string">'pass'</span>))</div><div class="line">r.status_code</div><div class="line">r.headers[<span class="string">'content-type'</span>]</div><div class="line">r.encoding</div><div class="line">r.text</div></pre></td></tr></table></figure>
<p>以下是输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">200</span></div><div class="line"><span class="string">'text/html; charset=utf-8'</span></div><div class="line"><span class="string">'utf-8'</span></div><div class="line"><span class="string">'&lt;!DOCTYPE html&gt;'</span></div></pre></td></tr></table></figure>
<p>以上代码我们请求了本站的地址，然后依次打印出状态码，请求头，编码方式和相应内容。</p>
<p>是不是功能强大，下面，我们正式开始学习Requests。</p>
<p><br></p>
<h1 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h1><p>首先导入Requests模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div></pre></td></tr></table></figure>
<p>Requests包含各种不同的请求方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>)</div><div class="line">r = requests.put(<span class="string">"http://httpbin.org/put"</span>)</div><div class="line">r = requests.delete(<span class="string">"http://httpbin.org/delete"</span>)</div><div class="line">r = requests.head(<span class="string">"http://httpbin.org/get"</span>)</div><div class="line">r = requests.options(<span class="string">"http://httpbin.org/get"</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get请求</h2><p>我们用的最多的是get请求，用来获取某个网页：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</div></pre></td></tr></table></figure>
<p>现在，我们有一个名叫r的Response对象，它包含了所有关于该页面的信息，我们可以从中提取出我们想要的信息。</p>
<p><br></p>
<h2 id="带参数的get请求"><a href="#带参数的get请求" class="headerlink" title="带参数的get请求"></a>带参数的get请求</h2><p>如果你是手工构建 URL，那么数据会以键/值对的形式置于 URL 中，跟在一个问号的后面，我们可以用字典的形式传递URL参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</div><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=payload)</div><div class="line">print(r.url)</div></pre></td></tr></table></figure>
<p>可以得到输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http://httpbin.org/get?key2=value2&amp;key1=value1</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h2><p>如果我们想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。只需简单地传递一个字典给 data 参数，该字典在发出请求时会自动编码为表单形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=payload)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  ...</div><div class="line">  <span class="string">"form"</span>: &#123;</div><div class="line">    <span class="string">"key2"</span>: <span class="string">"value2"</span>,</div><div class="line">    <span class="string">"key1"</span>: <span class="string">"value1"</span></div><div class="line">  &#125;,</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="获取响应内容"><a href="#获取响应内容" class="headerlink" title="获取响应内容"></a>获取响应内容</h1><h2 id="响应状态码"><a href="#响应状态码" class="headerlink" title="响应状态码"></a>响应状态码</h2><p>我们可以检测发送请求是否成功获得想要的数据，通过检测响应状态码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</div><div class="line">r.status_code</div></pre></td></tr></table></figure>
<p>若输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">200</div></pre></td></tr></table></figure>
<p>表明获取成功。</p>
<p><br></p>
<h2 id="文本内容"><a href="#文本内容" class="headerlink" title="文本内容"></a>文本内容</h2><p>以百度首页为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">"https://www.baidu.com"</span>)</div><div class="line">r.text</div></pre></td></tr></table></figure>
<p>Requests会基于HTTP的头部对响应的编码进行推测，让我们访问<code>r.text</code>时，Requests会使用其推测的文本编码，我们可以用<code>r.encoding</code>改变解码方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">r.encoding</div><div class="line">r.encoding = <span class="string">'ISO-8859-1'</span></div><div class="line">r.encoding</div></pre></td></tr></table></figure>
<p>以上代码输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="string">'utf-8'</span></div><div class="line"><span class="string">'ISO-8859-1'</span></div></pre></td></tr></table></figure>
<p>如果你改变了编码，每当你访问 <code>r.text</code> ，Request 都将会使用 <code>r.encoding</code> 的新值。当不确定时，可以使用 <code>r.content</code> 来找到编码，然后设置 <code>r.encoding</code> 为相应的编码。这样就能使用正确的编码解析 <code>r.text</code> 了。关于<code>r.content</code>下面会介绍。</p>
<p><br></p>
<h2 id="二进制内容"><a href="#二进制内容" class="headerlink" title="二进制内容"></a>二进制内容</h2><p>对于非文本请求，例如图片等多媒体内容，可以以字节的方式访问：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r.content</div></pre></td></tr></table></figure>
<p>这里注意两者的区别：</p>
<ul>
<li>r.text返回的是Unicode型的数据。如果是读取文本，可以通过r.text。</li>
<li>r.content返回的是bytes型也就是二进制的数据。如果读取图片，文件，可以通过r.content来实现。</li>
</ul>
<p>例如我们下载并保存一张图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">jpg_url = <span class="string">'http://mm.howkuai.com/wp-content/uploads/2017a/04/13/01.jpg'</span><span class="comment">#一张美女图</span></div><div class="line">content = requests.get(jpg_url).content</div><div class="line"><span class="keyword">with</span> open(<span class="string">'image.jpg'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f: <span class="comment">#写入多媒体文件必须要 b 这个参数！</span></div><div class="line">    f.write(content)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="JSON内容"><a href="#JSON内容" class="headerlink" title="JSON内容"></a>JSON内容</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'https://github.com/timeline.json'</span>)</div><div class="line">r.json()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="响应头"><a href="#响应头" class="headerlink" title="响应头"></a>响应头</h2><p>通过命令可以查看服务器响应头，以一个Python字典形式展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'http://zangbo.me'</span>)</div><div class="line">r.headers</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">	<span class="string">'Server'</span>: <span class="string">'GitHub.com'</span>, </div><div class="line">	<span class="string">'Date'</span>: <span class="string">'Thu, 15 Jun 2017 04:04:47 GMT'</span>, </div><div class="line">	<span class="string">'Content-Type'</span>: <span class="string">'text/html; charset=utf-8'</span>, </div><div class="line">	<span class="string">'Transfer-Encoding'</span>: <span class="string">'chunked'</span>, </div><div class="line">	<span class="string">'Last-Modified'</span>: <span class="string">'Wed, 14 Jun 2017 15:41:23 GMT'</span>, </div><div class="line">	<span class="string">'Access-Control-Allow-Origin'</span>: <span class="string">'*'</span>, </div><div class="line">	<span class="string">'Expires'</span>: <span class="string">'Thu, 15 Jun 2017 04:14:47 GMT'</span>, </div><div class="line">	<span class="string">'Cache-Control'</span>: <span class="string">'max-age=600'</span>, </div><div class="line">	<span class="string">'Content-Encoding'</span>: <span class="string">'gzip'</span>, </div><div class="line">	<span class="string">'X-GitHub-Request-Id'</span>: <span class="string">'E1D6:0B0F:15941BE:1E637E3:5942075F'</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="获取响应内容总结"><a href="#获取响应内容总结" class="headerlink" title="获取响应内容总结"></a>获取响应内容总结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">r.status_code<span class="comment">#获取状态码</span></div><div class="line">r.text<span class="comment">#文本内容</span></div><div class="line">r.encoding<span class="comment">#查看编码</span></div><div class="line">r.encoding = <span class="string">'ISO-8859-1'</span><span class="comment">#改变编码</span></div><div class="line">r.content<span class="comment">#二进制内容</span></div><div class="line">r.json()<span class="comment">#JSON内容</span></div><div class="line">r.raw<span class="comment">#原始内容</span></div><div class="line">r.headers<span class="comment">#响应头</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="定制请求头"><a href="#定制请求头" class="headerlink" title="定制请求头"></a>定制请求头</h1><p>有些网站具有反爬虫机制，我们有时候需要把爬虫伪装成浏览器来访问网页，因此我们需要为请求添加HTTP头部，只需要简单地传递一个<code>dict</code>给<code>headers</code>参数即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">url = <span class="string">"https://www.baidu.com"</span></div><div class="line">headers = &#123;<span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8'</span>&#125;</div><div class="line">r = requests.get(url,headers=headers)</div></pre></td></tr></table></figure>
<p>注：如何查看自己浏览器的userAgent</p>
<p>打开浏览器的控制台，输入：javascript:alert(navigator.userAgent)</p>
<p><br></p>
<h1 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h1><p>我们可以告诉 requests 在经过以 <code>timeout</code> 参数设定的秒数时间之后停止等待响应：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">requests.get(<span class="string">'http://github.com'</span>, timeout=<span class="number">0.001</span>)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p><code>timeout</code> 仅对连接过程有效，与响应体的下载无关。 <code>timeout</code> 并不是整个下载响应的时间限制，而是如果服务器在 <code>timeout</code> 秒内没有应答，将会引发一个异常（更精确地说，是在 <code>timeout</code> 秒内没有从基础套接字上接收到任何字节的数据时）</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>本文基于Requests官方文档：</p>
<ul>
<li>英文版：<a href="http://docs.python-requests.org/en/master/" target="_blank" rel="external">Requests: HTTP for Humans</a></li>
<li>中文版：<a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">Requests: 让HTTP服务人类</a></li>
</ul>
<p>本文仅介绍基础用法，更高阶以及更详细的内容（Cookie、重定向与请求历史、错误与异常等）请参考官方文档。</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫笔记（一）：爬虫基础知识]]></title>
      <url>/2017/06/13/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本系列记录了爬虫学习的历程，本文整理了学习爬虫需要掌握的基础背景知识，包括HTML、URL在内的基础概念以及爬虫的工作原理。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h1><p>HTML中文全称“超文本标记语言”。超文本指页面内可以包含图片、链接，甚至音乐、程序等非文字元素。</p>
<p>超文本标记语言包括Head部分和Body部分，其中Head部分提供关于网页的信息，Body部分提供网页的具体内容。</p>
<ul>
<li><code>&lt;head&gt;&lt;/head&gt;；</code>这2个标记符分别表示头部信息的开始和结尾。头部中包含的标记是页面的标题、序言、说明等内容，它本身不作为内容来显示，但影响网页显示的效果。</li>
<li><code>&lt;body&gt;&lt;/body&gt;；</code>网页中显示的实际内容均包含在这2个正文标记符之间。正文标记符又称为实体标记。</li>
</ul>
<p><br></p>
<h1 id="URI"><a href="#URI" class="headerlink" title="URI"></a>URI</h1><p>统一资源标识符(Uniform Resource Identifier)，是一个用于标识某一互联网资源名称的字符串。Web上可用的每种资源：HTML文档、图像、视频片段、程序等，由一个URI进行定位。</p>
<p>URI由三部分组成：</p>
<ol>
<li>访问资源的命名机制</li>
<li>存放资源的主机名</li>
<li>资源自身的名称，由路径表示，着重强调于资源。</li>
</ol>
<p><br></p>
<h1 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h1><p>统一资源定位符(Uniform Resource Location)，是URI的一个子集，URI确定一个资源，URL不仅确定一个资源，还告诉你它在哪里。互联网上每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。</p>
<p>URL一般由三部组成：</p>
<ol>
<li>协议(或称为服务方式)</li>
<li>存有该资源的服务器名称或者主机IP地址(有时也包括端口号)</li>
<li>主机资源的具体地址。如目录和文件名等。</li>
</ol>
<p>以下是一个例子：</p>
<p><code>https://tianchi.aliyun.com/competition/information.htm?raceId=231602</code></p>
<p>协议://授权/路径?查询</p>
<ul>
<li>第一部分和第二部分用“://”符号隔开，</li>
<li>第二部分和第三部分用“/”符号隔开。</li>
<li>第一部分和第二部分是不可缺少的，第三部分有时可以省略。</li>
</ul>
<p>URI表示请求服务器的路径，定义这么一个资源，而URL同时说明要如何访问这个资源（通过http://）</p>
<p><br></p>
<h1 id="浏览网页的过程和爬虫原理"><a href="#浏览网页的过程和爬虫原理" class="headerlink" title="浏览网页的过程和爬虫原理"></a>浏览网页的过程和爬虫原理</h1><p>浏览器作为一个客户端，输入地址后，会向服务器端发送一次请求(Request)，服务器正常的话会返回一个响应(Response)，响应的内容是请求打开的页面内容。浏览器收到相应后，对信息进行相应处理，页面就显示在我们面前了。</p>
<p>爬虫的流程相似，我们通过http库向目标站点发送一个Request(里面包含URL)，如果服务器响应，会得到一个Response，我们在利用正则表达式等工具对它进行解析，最后保存数据。</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 数据挖掘 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[编码相关知识]]></title>
      <url>/2017/06/13/%E7%BC%96%E7%A0%81%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文整理了一些常见的编码，它们的起源、区分以及在Python 3中的使用，因为平时被各种不同的编码所困扰，索性就整理出来。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="编码的起源"><a href="#编码的起源" class="headerlink" title="编码的起源"></a>编码的起源</h1><p>在计算机中，所有的数据在存储和运算时都要使用二进制数表示（因为计算机用高电平和低电平分别表示1和0），例如像a、b、c、d这样的52个字母（包括大写）、以及0、1等数字还有一些常用的符号（例如*、#、@等）在计算机中存储时也要使用二进制数来表示，而具体用哪些二进制数字表示哪个符号，当然每个人都可以约定自己的一套（这就叫编码），而大家如果要想互相通信而不造成混乱，那么大家就必须使用相同的编码规则。</p>
<p><br></p>
<h1 id="ASCII"><a href="#ASCII" class="headerlink" title="ASCII"></a>ASCII</h1><p>为了有一套统一的编码规则，美国国家标准协会就出台了ASCII编码，统一规定了上述常用符号用哪些二进制数来表示，后来被国际标准化组织（ISO）定位了国际标准。</p>
<p>标准ASCII码使用指定的7位二进制数组合来表示128种可能的字符，而由于电脑开发初期就建立了8位元位元组，即1byte=8bits，因此如果使用一个位元组来保存字元，则可以存储256个码，于是就诞生了扩展ASCII码。</p>
<p><br></p>
<h1 id="ISO8859-1"><a href="#ISO8859-1" class="headerlink" title="ISO8859-1"></a>ISO8859-1</h1><p>之前说过，标准ASCII是针对英语设计的，当处理带有音调标号（形如汉语的拼音）的亚洲文字时就会出现问题。因此，创建出了一些包括255个字符的由ASCII扩展的字符集。</p>
<p>其中一种扩展的8位字符集是ISO 8859-1 Latin 1，也简称为ISOLatin-1。它把位于128-255之间的字符用于拉丁字母表中特殊语言字符的编码，也因此而得名。ISOLatin-1向下兼容ASCII。</p>
<p><br></p>
<h1 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h1><p>然而欧洲语言不是地球上的唯一语言，亚洲和非洲语言并不能被8位字符集所支持。仅汉语字母表就有80000以上个字符。但是把汉语、日语和越南语的一些相似的字符结合起来，在不同的语言里，使不同的字符代表不同的字，这样只用2个字节就可以编码地球上几乎所有地区的文字。因此，创建了Unicode编码，它通过增加一个高字节对ISO Latin-1字符集进行扩展，当这些高字节位为0时，低字节就是ISO Latin-1字符。最初的Unicode标准UCS-2是使用两个字节表示一个字符，后来有人嫌少使用4个字节表示一个字符，即UCS-4，不过我们用的最多的还是UCS-2。</p>
<p>总的来说，Unicode是计算机科学领域里的一项业界标准，包括字符集、编码方案等。Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。</p>
<p><br></p>
<h1 id="UTF"><a href="#UTF" class="headerlink" title="UTF"></a>UTF</h1><p>对可以用ASCII表示的字符使用Unicode并不高效，因为Unicode比ASCII占用大一倍的空间，而对ASCII来说高字节的0对他毫无用处。为了解决这个问题，就出现了通用转换格式，即UTF（Unicode Transformation Format）。</p>
<center><img src="http://i1.buimg.com/597140/576550baa99295f6.png" alt="Markdown"></center>

<p>换句话说，在计算机<strong>内存</strong>中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，是由UTF负责的。最简单的一种形式是直接用UCS的码位来保存，这就是UTF-16，比如，”汉”直接使用\x6C\x49保存(UTF-16-BE)，或是倒过来使用\x49\x6C保存(UTF-16-LE)。但用着用着美国人觉得自己吃了大亏，以前英文字母只需要一个字节就能保存了，现在大锅饭一吃变成了两个字节，空间消耗大了一倍……于是UTF-8横空出世。</p>
<p><strong>UTF-8</strong>是一种可变长度字符编码。它用1到6个字节编码Unicode字符。用在网页上可以统一页面显示中文简体繁体及其它语言（如英文，日文，韩文）。</p>
<p>此外，常见的UTF格式还有：UTF-7、UTF-7.5、UTF-8、UTF-16、UTF-32</p>
<p><br></p>
<h2 id="Python-3中的编码"><a href="#Python-3中的编码" class="headerlink" title="Python 3中的编码"></a>Python 3中的编码</h2><p>在python 3版本中，字符串是以Unicode编码的，也就是说，python 3字符串支持中英或其他各种语言。此外，我们可以利用<code>.encode()</code>函数来把Unicode字符串进行编码，编码后的字符串前面会用b表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">u = <span class="string">'汉'</span></div><div class="line">print(u)</div><div class="line">s = u.encode(<span class="string">'UTF-8'</span>)</div><div class="line">print(s)</div><div class="line">d = s.decode(<span class="string">'UTF-8'</span>)</div><div class="line">print(d)</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">'汉'</span></div><div class="line">b<span class="string">'\xe6\xb1\x89'</span></div><div class="line"><span class="string">'汉'</span></div></pre></td></tr></table></figure>
<p>关于python 3中读取文件：</p>
<blockquote>
<p>文件总是存储为编码好的数据，因此，为了使用文件中读取的文本数据，必须首先将其解码为一个Unicode字符串。python 3中，文本正常情况下会自动为你解码，所以打开或读取文件会得到一个Unicode字符串。使用的解码方式取决系统，在Mac OS 或者 大多数Linux系统中，首选编码是UTF-8，但Windows不一定。可以使用locale.getpreferredencoding()方法得到系统的默认解码方式。</p>
</blockquote>
<p>注意：python 2和python 3在编码方面具有较大差别，具体可以查阅相关资料，因为我平时只用python 3，所以这里就不提供了。</p>
<p><br></p>
<h1 id="GB2312"><a href="#GB2312" class="headerlink" title="GB2312"></a>GB2312</h1><p>汉字编码字符集，GB2312编码适用于汉字处理、汉字通信等系统之间的信息交换，通行于中国大陆；新加坡等地也采用此编码。中国大陆几乎所有的中文系统和国际化的软件都支持GB 2312。</p>
<p><br></p>
<h1 id="GBK"><a href="#GBK" class="headerlink" title="GBK"></a>GBK</h1><p>GBK全称《汉字内码扩展规范》，向下与 GB 2312 编码兼容，使用了双字节编码方案，共23940个码位，共收录了21003个汉字。</p>
<p><br></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在计算机<strong>内存</strong>中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，是由UTF负责的，所以在很多网页的头文件可以看到编码方式utf-8，这表明该网页使用utf-8编码的。</p>
<p>另外在编程语言的使用过程中，切记关注编码方式是很重要的一件事。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://www.cnblogs.com/284628487a/p/5584714.html" target="_blank" rel="external">Python3 字符编码 - Coder25 - 博客园</a></li>
<li>百度百科</li>
</ul>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 计算机基础 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[配置GithubPage二级域名]]></title>
      <url>/2017/06/12/%E9%85%8D%E7%BD%AEGithubPage%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>因为女友生日做了两个网站，就思考该如何把它们放在GithubPage下，并且设置成二级域名，查阅了一堆资料终于明白GithubPage二级域名该如何配置，于是整理在下面。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Github-Page种类"><a href="#Github-Page种类" class="headerlink" title="Github Page种类"></a>Github Page种类</h1><ol>
<li><p><code>UserPage</code>: 用户的整个站点，这个是最初github支持的类型，创建一个形如username.github.com的项目就可以，用户想要在github上托管网站，这个是必须要建的，而且命名方式固定。</p>
</li>
<li><p><code>ProjectPage</code>: 用户创建出来的其他网站项目也可以托管在github上，首先新建一个项目，然后建立一个名叫<code>gh-pages</code>的branch，这个branch里的文件就是page的站点文件。</p>
<p><br></p>
</li>
</ol>
<h1 id="UserPage默认域名"><a href="#UserPage默认域名" class="headerlink" title="UserPage默认域名"></a>UserPage默认域名</h1><p>用户站点的默认域名是<code>username.github.io</code>，比如我的站点就是<code>zangbo.github.io</code></p>
<p><br></p>
<h1 id="ProjectPage默认域名"><a href="#ProjectPage默认域名" class="headerlink" title="ProjectPage默认域名"></a>ProjectPage默认域名</h1><p>其他网站项目的默认域名，是使用UserPage域名加上二级目录实现的，比如我有个项目叫<code>mylove</code>，那么该项目的站点就是访问 <code>zangbo.github.io/mylove</code></p>
<p><br></p>
<h1 id="UserPage自定义域名"><a href="#UserPage自定义域名" class="headerlink" title="UserPage自定义域名"></a>UserPage自定义域名</h1><p>我有自己的域名，如何绑定到UserPage? 比如用<code>www.zangbo.me</code>替代<code>zangbo.github.io</code>，它是使用CNAME技术来实现的。</p>
<p>具体步骤可以参考我前些日子写的文章：《Mac环境利用GitHub和Hexo搭建个人博客》，除了添加<code>CNAME</code>指向还应添加<code>A</code>指向。</p>
<blockquote>
<p>CNAME指向之后，当浏览器访问<code>www.zangbo.me</code>的时候浏览器就知道<code>实际上</code>是访问<code>zangbo.github.io</code><br>添加CNAME 文件之后，当GithubPage服务器接收到访问<code>www.zangbo.me</code>的http请求，就知道，对应的是这个工程了。</p>
</blockquote>
<p><br></p>
<h1 id="ProjectPage自定义域名"><a href="#ProjectPage自定义域名" class="headerlink" title="ProjectPage自定义域名"></a>ProjectPage自定义域名</h1><p>比如用<code>mylove.zangbo.me</code>替代<code>zangbo.github.io/mylove</code></p>
<ol>
<li><p>同样的，去域名注册商那里，做一个<code>CNAME</code>指向，将<code>mylove.zangbo.me</code> 指向 <code>zangbo.github.io</code>，如果以后会有很多二级域名都指过来，操作方式相同。</p>
</li>
<li><p>在<code>zangbo/mylove</code>这个项目(也就是page项目)根目录下建一个<code>CNAME</code>文件，里面填写<code>mylove.zangbo.me</code>，然后提交到仓库;</p>
</li>
<li><p>等几分钟。</p>
<p><br></p>
</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.zhaoxiaodan.com/%E5%85%B6%E4%BB%96/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEGithubPage%E7%9A%84%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D.html" target="_blank" rel="external">如何配置GithubPage的二级域名</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 教程 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 个人网站 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[送给女友的生日礼物]]></title>
      <url>/2017/06/12/%E9%80%81%E5%A5%B3%E5%8F%8B%E7%9A%84%E7%94%9F%E6%97%A5%E7%A4%BC%E7%89%A9/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>女朋友生日来临之际，自己在别人模版基础上修改着做了两个网站当作礼物，并没有兼容手机端，最好在电脑端登录观看效果。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<div id="aplayer0" class="aplayer" style="margin-bottom: 20px;"></div>
		<script>
			new APlayer({
				element: document.getElementById("aplayer0"),
				narrow: false,
				autoplay: false,
				showlrc: 0,
				music: {
					title: "童话镇",
					author: "陈一发儿",
					url: "http://mp3.haoduoge.com/s/2016-12-24/1482568978.mp3",
					pic: "http://p4.music.126.net/tfa811GLreJI_S0h9epqRA==/3394192426154346.jpg?param=130y130",
				}
			});
		</script>
<p><br></p>
<h1 id="我的礼物"><a href="#我的礼物" class="headerlink" title="我的礼物"></a>我的礼物</h1><p>以下是我的两个网站：</p>
<ul>
<li>博莎小站：<a href="http://lisa.zangbo.me" target="_blank" rel="external">http://lisa.zangbo.me</a></li>
<li>送给Lisa的一封信：<a href="http://mylove.zangbo.me" target="_blank" rel="external">http://mylove.zangbo.me</a></li>
</ul>
<p>注：手机端打开会出现排版问题，建议电脑浏览器浏览。</p>
<p><br></p>
<h1 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a>参考网站</h1><ul>
<li><p>第一个网站参考：<a href="http://huyuqian.chengdazhi.com" target="_blank" rel="external">治芊小站</a></p>
<p>作者主页：<a href="http://chengdazhi.com" target="_blank" rel="external">程大治</a></p>
<p>发现这个网站是在知乎问题 <a href="https://www.zhihu.com/question/37804443" target="_blank" rel="external">程序员能给女朋友做什么特别浪漫的礼物？ - 知乎</a> 下，其中不乏很多很好的创意，通过评论得知作者也是在网上找的模版，很容易就搜到了：<a href="http://www.cssmoban.com/cssthemes/6077.shtml" target="_blank" rel="external">简洁瀑布流布局动物世界博客模板</a>，于是直接进行二次加工。</p>
</li>
</ul>
<ul>
<li><p>第二个网站参考：<a href="http://hackerzhou.me/ex_love/" target="_blank" rel="external">Our Love Story</a></p>
<p>作者GitHub：<a href="https://github.com/hackerzhou" target="_blank" rel="external">hackerzhou</a></p>
<p>这个网站在圈子里可以说是很有名了，作者开发于2011年，当年送的人如今已经变成了前女友，不免一阵唏嘘。作者把代码放在GitHub上允许大家自由的进行二次开发，我便拿来进行修改，因为女朋友并非程序员，所以原本的网页中代码生成部分改为了一封信。</p>
</li>
</ul>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>因为妹子是文科生，送之前我也不知道妹子会不会喜欢这种理工男的浪漫，所以这两个网页只能算是礼物的一部分，自然还是要准备其他实质性的东西的。</p>
<p><br></p>
</the></excerpt>]]></content>
      
        
        <tags>
            
            <tag> 个人网站 </tag>
            
            <tag> 前端 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Word2Vec笔记：Negative Sample]]></title>
      <url>/2017/06/06/Word2Vec%E4%B9%8BNegative-Sample/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Word2Vec中对Skip Gram模型的改进——Negative Sample方法。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在skip-gram模型中，我们发现这是个十分庞大的神经网络，如果我们把词向量设置成300，那么在一个具有10000个单词的字典中，隐层和输出层分别具有3百万个参数。执行梯度下降算法时将会十分缓慢，更糟糕的是，我们需要事先在一个数量庞大的数据集上训练，以便于fine-tune参数和避免过拟合。因此对模型的训练将十分困难。</p>
<p>为了解决这个问题，作者提出了三个修改方案：</p>
<ol>
<li>把常见的短语或单词对当成一个单词来对待</li>
<li>降采样高频单词来见少训练样本数量</li>
<li>用一种他们称之为“负采样”的方式来修改优化目标，这将使得每个训练样本只更新一小部分的模型参数。</li>
</ol>
<p>它们均可以使得训练变的可行同时也改善了结果词向量的质量。</p>
<p><br></p>
<h1 id="单词对和短语"><a href="#单词对和短语" class="headerlink" title="单词对和短语"></a>单词对和短语</h1><p>作者指出一个单词对像“Boston Globe”(一个报纸名)，它的含义和“Boston”以及“Globe”都有着很大的区别，因此把“Boston Globe”当作一个单词来对待是很有意义的，它将会得到一个词向量来表示。在Google News数据集上，这个改变可以把模型字典尺寸从一千亿降低为三百万。Google开源了它们训练好得到的词向量，长度是300，共计三百万个。至于如何从单词中区分短语并且缩短字典尺寸，一个方法是可以参考维基百科的词条目录。</p>
<p><br></p>
<h1 id="降采样高频单词"><a href="#降采样高频单词" class="headerlink" title="降采样高频单词"></a>降采样高频单词</h1><p>在之前的例子中，针对一些通用单词例如“the”存在一些问题：</p>
<ol>
<li>当我们看一些单词对的时候，(“fox”，“the”)没有告诉我们过多的关于“fox”的含义的信息，“the”出现在几乎每个单词的前面。</li>
<li>我们拥有太多的关于(“the”，…)的采样，而我们学习一个好的关于“the”的向量不需要这么多的采样。</li>
</ol>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_S9yXV9_1.jpeg" alt=""></center>

<p>Word2Vec采用了一个叫做“降采样”的框架来解决这个问题。对于每个出现在我们训练文件中的单词，它们将会有一定的概率被删除相关的训练数据，概率的高低取决于它们出现的频率。</p>
<p>如果我们有一个尺寸为10的窗口，那么我们将移除一个特定的关于“the”的训练样本：</p>
<ol>
<li>当我们训练其他单词，“the”将不会在任何窗口中出现。</li>
<li>当我们训练“the”作为输入时，窗口尺寸会小于10。</li>
</ol>
<p><br></p>
<h1 id="采样频率"><a href="#采样频率" class="headerlink" title="采样频率"></a>采样频率</h1><p>关于如何计算字典中一个单词被保留下来的概率，有一套C语言的代码实现的公式。</p>
<p>Wi代表单词，z(wi)表示单词在语料库中出现的频率，例如“peanut”在一个拥有10亿单词语料库中出现了1000次，那么z(‘peanut’)=1E-6。</p>
<p>代码中还有一个参数称之为’sample’，它将控制采样发生的力度，默认是0.001，该数值越小则单词越不可能被保存。</p>
<p>P(wi)为保留单词的概率：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_BJPh3y_5.jpeg" alt=""></center>

<p>我们可以画出该公式的图：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_f7DguJ_6.jpeg" alt=""></center>

<p>如果我们使用默认采样值为0.001，那么有以下有趣的点：</p>
<ul>
<li>P=1，当z&lt;=0.0026时，这意味着当单词数量小于0.26%时将会被全部保留下来。</li>
<li>P=0.5，当z=0.00746时，这意味着有50%的概率被保留下来</li>
<li>P=0.033，当z=1时，这意味着如果语料库中全部是同一个单词时，只有3.3%的概率会被保存下来，这种情况当然很荒谬。</li>
</ul>
<p>论文中定义的公式和C语言代码中有所区别，我觉得C语言代码的实现更加权威。</p>
<p><br></p>
<h1 id="负采样-Negative-Sampling"><a href="#负采样-Negative-Sampling" class="headerlink" title="负采样(Negative Sampling)"></a>负采样(Negative Sampling)</h1><p>训练一个神经网络意味着每个样本都要更新所有参数，使用skip-gram模型将会每次都要更新数量庞大的模型，负采样采用的策略是更新每个样本时之更新很小比例的参数，而不是更新所有参数。</p>
<p>我们训练一个单词对时，例如(“fox”，“quick”)，当输入是“fox”时标签是“quick”的one-hot向量，那么也就是说输出标签神经元中只有一个是1，其他的成千上万个都是0。</p>
<p>因此当我们使用负采样时，我们将随机选择很小数量的“负”单词(例如选择5个)来更新权值，在我们的语料库中，一个“负”单词指的是我们希望让输出的标签的神经元为0，我们依然会更新“正”单词的神经元参数(也就是标签中为1的神经元)，论文中提到对于一个小规模的数据集，可以选择5-20个单词，如果是庞大的数据集，可以选择2-5个。</p>
<p>之前我们的输出层是300<em>10000的参数矩阵，但如今我们更新权值时只需要更新(“quick”)所代表的神经元的参数(输出为1)，以及再加上5个随机的输出为0的神经元，总共只需要更新6个神经元的参数，也就是300</em>6=1800个参数，这仅仅是原来输出层参数的0.06%！</p>
<p>注意在隐藏层中，不管是否使用负采样策略，都只有输入单词的权值被更新。</p>
<p><br></p>
<h1 id="选择负采样"><a href="#选择负采样" class="headerlink" title="选择负采样"></a>选择负采样</h1><p>选择一个单词作为负采样和它出现的频率有关，出现频率越高的单词越容易被选为负样本。在word2vec的C语言实现中，我们可以看到一个计算概率的公式：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_OWB2Gx_7.jpeg" alt=""></center>

<p>这种在C语言中实现的方式很有趣。他们有一个100M大小的元素数组（他们称之为unigram表），他们用词表中每个单词的索引多次填充这个表。然后，要实际选择一个负样本，只需生成0到100M之间的随机整数，并选择表中该索引处的单词。由于较高概率单词在表中出现次数较多，因此更有可能选择这些单词。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这里有一些关于Word2Vec的学习资源：</p>
<p><a href="http://mccormickml.com/2016/04/27/word2vec-resources/" target="_blank" rel="external">Word2Vec Resources · Chris McCormick</a></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="external">Word2Vec Tutorial Part 2 - Negative Sampling · Chris McCormick</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 自然语言处理 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Word2Vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Word2Vec笔记：Skip-Gram模型]]></title>
      <url>/2017/06/05/Word2Vec%E4%B9%8Bskip-gram%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Word2Vec的基本概念，同时对Word2Vec的经典模型——The skip gram neural network model进行详细的学习和整理。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="模型任务"><a href="#模型任务" class="headerlink" title="模型任务"></a>模型任务</h1><p>我们将要训练一个简单的神经网络来执行某个确定的任务，它只有一个隐层，但是我们却不去使用该训练好的网络，我们只是来学习隐层的权值，该权值就是我们的词向量(word vectors)。</p>
<p>具体任务如下：在一个句子中间部分选择一个单词作为输入单词，然后网络会告诉我们字典中每个单词在该单词附近的概率，这里的附近范围指的是该输入单词前面5个和后面5个(共10个)单词。比如我们输入单词是“苏联”，那么字典中“联邦”和“俄罗斯”的概率会高于“西瓜”和“袋鼠”</p>
<p>我们训练文件中存在很多成对的单词，我们不停的把一对单词送入神经网络进行训练，神经网络会统计单词出现的频率，比如（“苏联”，“联邦”）出现的次数更多，那么当我们输入“苏联”这个单词时，“联邦”的概率将会更大。如下是一个窗口为2的例子，前面2个后面2个，蓝色的为输入单词。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_S9yXV9_1.jpeg" alt=""></center>

<p><br></p>
<h1 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h1><p>我们不可能直接把一个单词送入神经网络中训练，因此我们需要找到一个方式来代替单词。为了实现这点，我们先从训练文件中建立一个单词字典，假设我们有10000个独立单词。</p>
<p>我们讲吧每个输入单词例如“ants”作为一个one-hot向量，这个向量有10000维，我们将某一维设置为1，其他位置设置为0来表示单词“蚂蚁”。在训练网络时，输入是10000维向量(表示输入单词)，输出标签是与它相关单词对中的另外一个单词的one-hot向量，也是10000维。但是当我们使用该网络时，输出层使用softmax层，输出将变成一个10000维的概率向量(因为字典中有10000个单词)，其中的每一个维度都表示字典中的一个单词在该输入单词附近的概率。下图为使用时的模型：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_EBQYfm_2.jpeg" alt=""></center>

<h1 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h1><p>如果我们用300个单元来表示隐藏层，那么我们学到的词向量就是300维的，因此隐层的参数矩阵是[10000,300]的，300是Google在发行的数据库中使用的长度，这是个超参数，可以根据自己的需求更改。</p>
<p>因此我们的目标实际上是学习这样一个参数矩阵，输出层在我们训练完网络就丢弃不用。那么输入的one-hot向量什么作用呢，因为该向量大部分都是0，只有一个维度是1，因此可以起到选择的作用，如下简单的例子：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_HQ30yx_3.jpeg" alt=""></center>

<p>我们可以看出隐层的输出是输入one-hot向量的一个词向量表示。</p>
<p><br></p>
<h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><p>1*300的词向量”ants”将会被送到输出层，输出层是一个softmax回归分类器，分类器会输出字典中出现过的每一个单词在它附近的概率，这些概率在0-1之间，而且加起来的和为1。特别的，每个输出单元都有个权值向量和词向量相乘，然后用exp()函数得出结果，最后为了使得所有的向量概率加起来为1，还要除以10000个词向量的exp()结果的和。</p>
<p>以下是计算输出神经元输出“car”这个单词的图示：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_XjsDH7_4.jpeg" alt=""></center>

<p>注意即使训练时某个单词在另一个单词附近的概率是100%，但使用时输出改单词的概率也不是100%，这和softmax函数有关。</p>
<p><br></p>
<h1 id="一些直觉"><a href="#一些直觉" class="headerlink" title="一些直觉"></a>一些直觉</h1><p>如果两个单词又非常相似的语义，我们的模型将会对这两个单词的输入来输出非常相似的结果。其中的一个方法是这两个单词的词向量是相似的。因此如果两个单词有相似的语义，那么他们有相似的词向量。</p>
<p>那么什么叫相似的语义呢？比如“intelligent”和“smart”这种有相似意思的单词，或者“engine”和“transmission”这种相关的单词，或者“ant”和“ants”这种具有相同意思的不同词性的单词。</p>
<p><br></p>
<h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>这种模型包含了数量庞大的参数，对于300特征的隐层和10000单词的字典，需要隐层和输出层各3M大小的权值数量，训练一个庞大的数据库是很困难的，因此我们做了一些改进来便于训练，这就是下篇文章提到的Negative Sample。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="external">Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick</a></p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 自然语言处理 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Word2Vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习中几种不同的梯度下降算法]]></title>
      <url>/2017/06/04/%E5%85%B3%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文整理了关于梯度下降算法几个容易混淆的概念，它们都属于最速下降法。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<hr>
<p>之前关于Tensorflow的笔记有一个地方提到了随机梯度下降，官方文档提到说“这种随机抓取一小部分数据训练的方法称为随机梯度下降(SGD)”，但后来我又查了资料，这种方法准确来说应该称为Mini-batch梯度下降，因为标准的SGD是每次只随机取一个样本进行训练的，关于梯度下降算法几个容易混淆的概念我整理了下，它们都属于最速下降法：</p>
<p><br></p>
<ol>
<li><p><strong>批量梯度下降(Batch Gradient Descent，BGD)</strong></p>
<p>每次迭代的梯度方向计算由所有训练样本共同投票决定，即每次的参数更新需要把所有的训练样本都迭代一遍，可以采用线性搜索来确定最优步长，但缺点是参数更新速度太慢，而且如果训练样本数量特别庞大，对显存要求会很高。“Batch”的含义是训练集中所有样本参与每一轮迭代。</p>
</li>
<li><p><strong>随机梯度下降(Stochastic Gradient Descent，SGD)</strong></p>
<p>和BGD是两个极端，SGD每次迭代只选择一个训练样本，计算梯度然后进行参数的更新，直到收敛，这种方法也可用作online learning。这种方法的优点是参数更新速度很快，不需要占用较大显存。但缺点也很明显，因为不同训练样本的差距和噪声的影响，每次参数更新方向未必是正确的优化方向，但实践证明总体趋势一般是朝着最优点方向的。</p>
</li>
<li><p><strong>Mini-batch Gradient Descent</strong></p>
<p>这种方法在前两种方法里取一个折中，每次在训练集中取一部分数据进行迭代，然后更新参数。这是实践中使用最多的一种方法，具体选取的batch大小取决于数据集和显卡的质量，实验证明太大和太小都不好，应该选择一个适中的尺寸来达到训练效果最优。具体怎么选那就是一门玄学了，不过普通研究生平时科研的情况下选择自己显存能承受的最大值就可以了吧。</p>
</li>
</ol>
<p><br></p>
<p>平时也有些地方把SGD和Mini-batch GD统称为SGD，前者是后者的一个特例，TensorFlow的官方文档大概就是这样。这两者虽然也叫最速下降法，但步长一般不采用线搜索来获得，因为参数更新频率太快，线搜索计算比较复杂会消耗大量时间，所以平时都是凭经验指定步长(即学习率)。因为只要下降方向正确，步长在一个可以接受的范围内依然可以迭代到最优值，但学习率如果选的太大则无法下降，选的太小又使得训练过程漫长很难达到最优值，因此关于学习率的选择又是一门玄学。</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 梯度下降算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（二）：MNIST进阶]]></title>
      <url>/2017/06/03/Tensorflow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本系列是对TensorFlow官方文档进行学习的总结，本篇介绍了利用TensorFlow搭建一个卷积神经网络，并用它来对MNIST数据集进行分类。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这两天完成了官方文档第二部分的学习，之前我们用TensorFlow在MNIST数据集上实现了简单的Softmax回归，但准确率只有92%左右。因为这种方法忽略了图片的空间结构信息，而只是把图像展开成一维向量来输入。今天我们使用TensorFlow来搭建一个卷积神经网络，用它来对MNIST数据集进行分类。</p>
<p>接下来我们进行卷积神经网络的学习，我会一行行的解释用到的代码。</p>
<p><br></p>
<h1 id="初始化阶段"><a href="#初始化阶段" class="headerlink" title="初始化阶段"></a>初始化阶段</h1><p>首先是训练数据的准备工作，下面两行代码会下载并引用MNIST数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>然后我们启动一个会话，这里我们没有选择之前使用的<code>tf.Session()</code>，而是采用了交互式编程中更方便的Interactive类，这样我们可以在运行图的时候插入新的图，这在一些交互式环境比如ipython中更便利，我使用的是jupyter notebook，同样是交互式环境。如果没有使用 InteractiveSession ，那么需要在启动Session之前构建好整个计算图，然后才能启动该计算图。具体方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">sess = tf.InteractiveSession()</div></pre></td></tr></table></figure>
<p>接着我们定义两个占位符x和y_，分别用来输入图片数据和标签。它们均为二维向量，第一个维度用None来指代Batch大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</div></pre></td></tr></table></figure>
<p>为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题(dead neurons)。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div></pre></td></tr></table></figure>
<p>下面重点讲一下TensorFlow中的两个函数，它们分别用来做卷积操作和池化操作。</p>
<ul>
<li><p><code>tf.nn.conv2d(x,W,strides,padding)</code></p>
<p>参数有四个：</p>
<p>第一个参数x：4D的tensor，格式为[batch,height,width,channels]。</p>
<p>第二个参数W：核函数，同样是4D的tensor，格式为[height,width,in_channel,out_channel]。</p>
<p>第三个参数strides：步长，也是4D的tensor，格式为[1,stride,stride,1]，一般第一维和第四维都是1，因为很少有对batch和channel进行卷积计算。</p>
<p>第四个参数padding：分“SAME”或者“VALID”两种，前者会进行补0，使得卷积前后的尺寸相同，后者不补0。</p>
</li>
<li><p><code>tf.nn.max_pool(value, ksize, strides, padding)</code></p>
<p>参数是四个，和卷积很类似：</p>
<p>第一个参数value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape</p>
<p>第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1</p>
<p>第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]</p>
<p>第四个参数padding：和卷积类似，可以取’VALID’ 或者’SAME’，返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式。</p>
</li>
</ul>
<p>这里我们的卷积使用1步长(stride size)，补0(padding size)模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling。为了代码更简洁，我们把这部分抽象成一个函数。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="第一层卷积和池化"><a href="#第一层卷积和池化" class="headerlink" title="第一层卷积和池化"></a>第一层卷积和池化</h1><p>下面我们可以实现第一层了，它由一个卷积接一个max pooling完成。卷积在每个5x5的patch(卷积核)中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。而对于每一个输出通道都有一个对应的偏置量。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</div><div class="line">b_conv1 = bias_variable([<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>为了用这一层，我们把x变成一个4d向量，其第2、第3维对应图片的高、宽，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。 然后我们进行卷积和池化操作，卷积后要用ReLU激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="第二层卷积和池化"><a href="#第二层卷积和池化" class="headerlink" title="第二层卷积和池化"></a>第二层卷积和池化</h1><p>第二层中，每个5x5的patch会得到64个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">h_pool2 = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="全连接层fc1"><a href="#全连接层fc1" class="headerlink" title="全连接层fc1"></a>全连接层fc1</h1><p>现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>为了减少过拟合，我们采用dropout。说起dropout，这又是一门玄学了，dropout是指在网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，即随机让一些节点参数不工作。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。实验证明这种方法能有效的减少过拟合。</p>
<p>我们用一个placeholder来代表神经元的输出在dropout中保持不变的概率，这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">keep_prob = tf.placeholder(tf.float32)</div><div class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><p>最后，我们添加一个softmax层，就像前面的单层softmax regression一样。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line">b_fc2 = bias_variable([<span class="number">10</span>])</div><div class="line">y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h1><p>为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，只是我们会用更加复杂的ADAM优化器来做梯度最速下降，在feed_dict中加入额外的参数keep_prob来控制dropout比例。然后每100次迭代输出一次日志。 一共进行了20000次迭代，batch_size是50。实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">cross_entropy = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</div><div class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">sess.run(tf.global_variables_initializer())</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</div><div class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;</div><div class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</div><div class="line">    print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy))</div><div class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line">print(<span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</div><div class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</div></pre></td></tr></table></figure>
<p>我用的jupyter notebook交互式环境。官网给出的训练时间大概是一个半小时，但我只用了16分钟左右就训练完了，可能官网给的是用CPU训练的参考时间。最后我们得到的准确率是99.2%，和官方文档得到的是一样的。这个网络模型是两层卷积两层池化两层全连接，最后一层全连接是softmax层。</p>
<p>至此我们就用TensorFlow从头到尾实现了一个卷积神经网络，中间涉及了很多矩阵运算，且都是4维向量之间的运算。期间用到了部分最优化里的东西，比如最速下降法，但并不深入。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>深度学习到底还是个偏工程应用的学科，对于理论知识涉及面极广但并非很深入，而且很多地方还是玄学。比如dropout、迭代次数、学习率、batch size、正则项系数等等，这些参数全都要靠自己凭借经验调试，能出什么结果全凭运气。</p>
<p>好啦，我们关于卷积神经网络实现MNIST的内容就学习完啦，终于自己实现了一个网络hin开心，下面有时间会继续学习TensorFlow的其他部分。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.tensorflow.org/get_started/mnist/pros" target="_blank" rel="external">Deep MNIST for Experts  |  TensorFlow</a>（需翻墙）</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 笔记（一）：MNIST入门]]></title>
      <url>/2017/06/02/Tensorflow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本系列是对TensorFlow官方文档进行学习的总结，本篇主要关于TensorFlow的基础知识和在MNIST数据集上结合Softmax回归的简单实现。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近开始学习TensorFlow，关于TensorFlow网上有很多学习笔记，但基本上都来源于官方文档，目前已经有翻译好的中文版，因此决定直接从官方文档学起。代码部分我用的是Python3.5版本。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621230301_Z8l9h5_unnamed.jpeg" alt=""></center>

<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>TensorFlow支持Python、C和C++三种语言，但Python的库是最全的，也是使用最广泛的。</p>
<p>TensorFlow用<strong>张量(Tensor)</strong>表示数据，数据一般是多维数组，在Python中为numpy的ndarray对象。<strong>Flow(流)</strong>意味着基于数据流图(Data Flow Graphs)进行数值计算。TensorFlow即为张量从图的一端流动到另一端。</p>
<p>TensorFlow的运算可以用有向图表示，其中<strong>节点</strong>(operation,简称op)代表数学运算，<strong>边</strong>表示节点之间的某种联系，负责传输多维数据(Tensors)。节点可以被分配到多个计算设备上，可以异步和并行的地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。</p>
<p>图在定义的过程中不会被执行，必须在会话(Session)中被启动，会话把op分发到GPU或CPU设备上，同时提供执行方法。因此TensorFlow的程序通常被组织成<strong>构建阶段</strong>和<strong>执行阶段</strong>，在构建阶段，我们把所有op描述成一个图；在执行阶段，我们通过会话执行之前构建好的图。</p>
<p>如下例子是实现一个简单计数器:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">one = tf.constant(<span class="number">1</span>)</div><div class="line">state = tf.Variable(<span class="number">0</span>)</div><div class="line">new_value = tf.add(state, one)</div><div class="line">update = tf.assign(state, new_value)</div><div class="line">sess = tf.Session()</div><div class="line">tf.global_variables_initializer().run()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    print(sess.run(state))</div><div class="line">    sess.run(update)</div></pre></td></tr></table></figure>
<p>以下是输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">0</div><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td></tr></table></figure>
<p>最后记得关闭会话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p>节点(op)的常用定义：</p>
<p>1、<strong>常量(constant)</strong>，一般用作源节点，输出给其他节点做运算。</p>
<p>2、<strong>变量(variable)</strong>，一般用来存储需要更新的参数，需要初始化。</p>
<p>3、<strong>占位符(placeholder)</strong>，先定义好一个没有具体内容的节点，但需要指定数据类型，一般用作输入和输出节点，在执行会话run操作时用feed操作输入数据。</p>
<p>4、表示某种操作，比如：</p>
<p><code>+ - * /</code>，均为矩阵间对应项操作。</p>
<p><code>tf.matmul(a,b)</code>，矩阵乘法，不同于*。</p>
<p><code>tf.assign(a,b)</code>，赋值操作，把b的值赋给a。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621203909_FE7lvU_2.jpeg?imageMogr2/thumbnail/800x" alt=""></center>



<p>上例中，在<strong>构建阶段</strong>，我们定义一个常量one令它为1；定义了一个变量state令初始值为0；定义了一个加法操作<code>tf.add()</code>，它等同于+；定义了一个赋值操作<code>tf.assign()</code>，把更新后的值new_value赋给state。在该阶段所有操作都不会被执行。</p>
<p>在<strong>执行阶段</strong>，我们需要启动一个会话来执行图。我们定义一个会话<code>sess = tf.Session()</code>，通过<code>sess.run()</code>执行操作。如果括号内为常量、变量等，则会取出相应的值，我们称该操作为<strong>Fetch</strong>；如果括号内为某个运算操作，则会执行该操作同时返回结果tensor。在执行阶段的最初我们首先需要初始化所有变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.global_variables_initializer().run()</div></pre></td></tr></table></figure>
<p>在该例中我们通过循环反复执行update节点，并打印出state变量中的值，实现一个简单计数器的操作。</p>
<p>执行阶段结束后需要通过命令<code>sess.close()</code>结束会话，释放空间。也可以使用<code>with tf.Session() as sess:</code>代替<code>sess = tf.Session()</code>来建立会话，这样不需要手动释放空间。</p>
<p>需要注意的是，在执行会话的<code>sess.run()</code>操作之前，任何对op的定义都不会被执行。</p>
<p><br></p>
<h1 id="MNIST机器学习入门"><a href="#MNIST机器学习入门" class="headerlink" title="MNIST机器学习入门"></a>MNIST机器学习入门</h1><p>本章介绍了MNIST数据集和Softmax回归，前者是个入门级的计算机视觉数据集，包含各种手写数字图片，官方文档把MNIST称为机器学习届的“Hello World”。Softmax Regression是一个非常基础的数学模型，在多分类问题中被广泛使用，在斯坦福大学的公开课CS231n中，Softmax回归也被放在最前面来讲，可见其重要性。本章学习用Softmax回归训练一个机器学习模型用于预测MNIST数据集中的数字。</p>
<p>首先下载MNIST数据集，我们只需要用如下两行代码自动下载并且引用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p> 每张图片包含28*28个像素点，我们可以用一个数字矩阵来表示它。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621230309_tE1HOU_11.jpeg?imageView2/2/w/800" alt=""></center>

<p>在该模型中，我们直接把这个数组展开成一个一维向量，也就是忽视它的空间结构，把每张图片都看作一个长度为28*28=784的向量。数据集的标签数据是”one-hot vectors” ，标签是介于0～9之间的数字，每个标签都是长为10的一维向量，只有一个元素为1其他均为0，例如标签9将表示成([0,0,0,0,0,0,0,0,0,0,1]) .</p>
<p>下面介绍下Softmax回归，对于输入的每张图片，我们希望得到每个标签对应的概率，且所有标签的概率加起来应为1，Softmax回归就实现了这样一个功能。</p>
<p>Softmax回归分两步：第一步，我们通过一系列的加权求和，如果某个像素具有很强的证据说明这张图片不属于该类，那么相应的权值为负数，相反如果这个像素拥有有利的证据支持这张图片属于这个类，那么权值是正数。 再加上一个额外的偏置项(bias)，因此对于给定输入x我们可以得到某张图片的得分：<br>$$<br>\text{evidence}_i = \sum_j W_{i,~ j} x_j + b_i<br>$$<br>第二步，我们利用Softmax函数把它转化为概率值：<br>$$<br>y = \text{softmax}(\text{evidence})<br>$$<br>其中，Softmax函数的具体形式为：</p>
<p>$$<br>\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}<br>$$<br>因此我们得到如下形式(假设x为一个长度为3的一维向量，标签数量同样为3)：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621230307_YKyvO7_softmax-regression-vectorequation.jpeg?imageView2/2/w/800" alt=""></center>

<p>简写为：</p>
<p>$$<br>y = \text{softmax}(Wx + b)<br>$$<br>在我们的模型中，训练数据共60000张图片，但由于我们采用随机梯度下降来优化，因此我们x的尺寸为[batch_size,784]，W的尺寸为[784,10]，b的尺寸为[1,10]，最终输出的<code>y</code>尺寸和标签<code>y_</code>尺寸均为[batch_size,10]。具体实现我用的jupyter notebook，过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</div><div class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</div><div class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</div><div class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[<span class="number">1</span>]))</div><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>).minimize(cross_entropy)</div><div class="line">sess = tf.Session()</div><div class="line">tf.global_variables_initializer().run()</div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</div></pre></td></tr></table></figure>
<p>我们首先定义了变量W和b，用来存储模型参数，在一遍遍迭代中不断的对其进行更新，初始化为全零矩阵。然后定义占位符x，先在图中分配好节点，类型为float型，None表示此张量的第一个维度可以是任何长度的，在执行时可以利用feed操作输入任何数量的图片。接着定义了矩阵乘法操作<code>tf.matmul()</code>和加操作+，以及定义了节点y进行softmax计算。</p>
<p>我把数据流图简单画了下：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170621203909_wnrrZf_9.jpeg?imageView2/2/w/800" alt=""></center>

<p>我们已经定义了一个指标来说明一个模型是好的，但在机器学习，我们通常定义指标来表示一个模型是坏的，这个指标称为成本(cost)或损失(loss)，然后尽量最小化这个指标，这两种方式是相同的。</p>
<p>这里我们用到的成本函数是“交叉熵”(cross-entropy) ，它的定义如下：</p>
<p>$$<br>H_{y’}(y) = -\sum_i y’_i \log(y_i)<br>$$<br>y 是我们预测的概率分布, y’ 是实际的分布 </p>
<p>因此需要定义一个新的占位符<code>y_</code>来表示标签数据，来计算交叉熵，我们采用的batch_size为100，这里计算的是100张图片的交叉熵总和。在这里，我们定义一个最优化操作train_step，要求TensorFlow用梯度下降算法(gradient descent algorithm)以0.05的学习速率最小化交叉熵。</p>
<p>到这里，我们程序的构建阶段就完成了，接下来是执行阶段。我们启动一个会话<code>sess = tf.Session()</code>，首先初始化所有变量，然后开始启动训练，这里让模型循环训练迭代1000次，每次随机抓取训练数据中100个数据点，然后用它们替换之前定义的占位符<code>x</code>和<code>y_</code>中的参数(feed操作)。这种随机抓取数据训练的方法称为<strong>随机梯度下降</strong>，可以很好的节省训练开销，同时最大化的学习数据集的总体性能。</p>
<p>训练用的时间比较短，用CUDA只要几秒钟，训练结束后我们需要验证模型的性能，MNIST数据集有10000张测试图片用于评估模型的泛化性能。下面是测试阶段代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</div></pre></td></tr></table></figure>
<p>先介绍下用到的几个主要函数：</p>
<p><code>tf.argmax()</code>函数能给出tensor对象在某一维上的最大值所在的索引值。</p>
<p><code>tf.equal()</code>函数来对比两个tensor是否相同，返回一组布尔值。</p>
<p><code>tf.cast()</code>函数把tensor转化为某个特定类型，这里我们把布尔型转化为浮点数。例如： [True, False, True, True] 会变成 [1,0,1,1] 。</p>
<p><code>tf.reduce_mean()</code>函数用于计算tensor中所有元素的平均值，上例取平均值得到0.75。</p>
<p>在本模型中，我们定义节点correct_prediction来计算输出值和标签值的比较结果，得到一组布尔值；随后定义节点accuracy来执行转化浮点数操作和取平均值操作，最后返回的值即为我们模型最终测试的准确率。在测试时的执行阶段，我们用测试数据对占位符<code>x</code>和<code>y_</code>进行feed操作。</p>
<p>最终的准确率大约为91%，因为忽略了图片的空间结构信息，因此得到的准确率并非特别高，下一章会学习用卷积神经网络进行预测，可以达到99%的准确率。</p>
<p><br></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>TensorFlow相比于Caffe确实是难了一个等级，Caffe的模块化程度较高，给人一种黑箱子的感觉，模型建好是建好了，也能跑出来结果，但却对其中的运行过程毫不知情，就像搭积木。相比之下TensorFlow是一点点的自己去构造整个模型，更偏底层一些，因此真的是越用越喜欢，尤其是自己能独立的把整个模型构建起来后，成就感自然是Caffe不能比的。</p>
<p>因为Python进行复杂运算的效率较低，所以我们通常会使用各种函数库，比如NumPy，会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。但是从外部计算切回Python又是很大的开销，因此TensorFlow使用的方案是先一次性定义好所有需要的操作，然后全部一起放在Python外运行，这也是TensorFlow的程序分为构建阶段和执行阶段的原因。</p>
<p>因为节点可以被分配到多个计算设备上，可以异步和并行的地执行操作，所以我们就可以通过分布式运算极大的提高运行速度，这算是TensorFlow的优点之一吧。</p>
<p>本文部分图片和代码引用自TensorFlow官方文档，手绘数据流图为原创.</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="external">MNIST For ML Beginners  |  TensorFlow</a> （需翻墙）</p>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mac环境利用GitHub和Hexo搭建个人博客]]></title>
      <url>/2017/06/01/Mac%E7%8E%AF%E5%A2%83%E5%88%A9%E7%94%A8GitHub%E5%92%8CHexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了在Mac环境下如何利用GitHub和Hexo搭建个人博客，网上有不少教程已经过时，自己也走了不少弯路。最近折腾了下，决定总结一下具体的流程。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<h1 id="Hexo介绍"><a href="#Hexo介绍" class="headerlink" title="Hexo介绍"></a>Hexo介绍</h1><p>互联网时代很多人选择用个人博客来展示自己，试想一下拥有一个属于自己的域名，同时搭建一个属于自己的博客是多么炫酷的一件事。然而做独立博客在技术上要面对一堆需要解决的问题，于是就催生出一些自动化的搭建和维护博客的工具。目前比较流行的工具有ghost、Jekyll和Hexo，ghost可以生成动态网站，依赖于数据库，对环境的依赖相对较高，操作起来也更复杂一些。只是构建个人独立博客的话，更建议选择后两者。Jekyll和hexo都是用来生成静态网站的工具，对环境依赖少，可移植性更高，两者都可以托管在GitHub上。Jekyll需要安装python、ruby和一些库，而Hexo仅仅依赖于node，于是思虑再三我决定选择Hexo。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_XCXY5G_1.jpeg" alt=""></center>

<p><br></p>
<h1 id="搭建流程"><a href="#搭建流程" class="headerlink" title="搭建流程"></a>搭建流程</h1><ol>
<li>获得域名</li>
<li>利用GitHub创建仓库</li>
<li>安装Git</li>
<li>安装Node.js</li>
<li>安装Hexo</li>
<li>推送网站</li>
<li>绑定域名</li>
<li>配置主题</li>
<li>更新文章</li>
<li>寻找图床</li>
<li>功能扩展</li>
</ol>
<p><br></p>
<h1 id="获得域名"><a href="#获得域名" class="headerlink" title="获得域名"></a>获得域名</h1><p>域名是网站的一个入口，也是别人对网站的第一印象，常见的域名有.com、.cn、.net等后缀，也有io、me、vip等后缀，一个巧妙的域名能给人留下深刻的印象，比如饿了么的域名：<a href="https://www.ele.me/" target="_blank" rel="external">https://www.ele.me/</a> 。要想搭建一个个人博客，给别人留下深刻印象，拥有一个属于自己的域名是必不可少的，而购买域名也是我们整个搭建过程中唯一需要花钱的地方，但花费并不多。</p>
<p>申请域名的地方有很多，以下列举了几个常用的注册商：</p>
<ul>
<li><a href="https://www.aliyun.com" target="_blank" rel="external">阿里云-为了无法计算的价值</a></li>
<li><a href="https://cloud.baidu.com" target="_blank" rel="external">百度云–智能，计算无限可能</a></li>
<li><a href="https://www.qcloud.com" target="_blank" rel="external">腾讯云 - 值得信赖</a></li>
<li><a href="https://www.godaddy.com" target="_blank" rel="external">GoDaddy 全球知名互联网域名注册商</a></li>
</ul>
<p>这里推荐阿里云，域名种类多而且提供隐私保护，购买后需要实名认证，如果不喜欢实名认证的同学可以选择最后一个GoDaddy，但是GoDaddy不提供隐私保护，会把你的邮箱和手机信息暴露出来，导致可能收到很多垃圾邮件和短信，而且GoDaddy提供的域名后缀种类有限，比如我喜欢的.me就没有。因此我选择了阿里云，申请入口<a href="https://wanwang.aliyun.com/domain/" target="_blank" rel="external">域名注册</a>。</p>
<p>阿里云购买.me域名首年只需要13元，支持支付宝付款，实名认证比较简单，只需要提交一张身份证照片即可，经过实名认证审核后，我们就拥有了属于自己的域名。</p>
<p><br></p>
<h1 id="利用GitHub创建仓库"><a href="#利用GitHub创建仓库" class="headerlink" title="利用GitHub创建仓库"></a>利用GitHub创建仓库</h1><p>创建网站不仅需要域名，而且需要网站空间。网站空间也叫虚拟主机，是用来存放网站内容的地方。但是租借虚拟主机需要很大的开销，幸运的是，我们可以把静态网站托管到GitHub上，GitHub是一个面向开源及私有软件项目的托管平台。</p>
<p>首先需要登陆到GitHub：<a href="https://github.com" target="_blank" rel="external">Build software better, together</a>，如果没有GitHub账号，可以使用邮箱注册，然后点击GitHub中的New repository创建新仓库，仓库名为：<strong>用户名</strong>.github.io，这里命名格式为固定的，<strong>用户名</strong>是你的GitHub账号，比如我的仓库名为：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_HZdID1_2.jpeg" alt=""></center>



<p>建好之后，我们就拥有了用于搭建个人博客的网站空间。</p>
<p><br></p>
<h1 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h1><p>Git是一款免费、开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。具体的细节可以看廖雪峰老师的教程：<a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="external">Git教程 - 廖雪峰的官方网站</a>，打开Git的官方网站下载Git安装包：<a href="https://git-scm.com/downloads" target="_blank" rel="external">Git - Downloads</a>，由于我们是Mac OS系统，这里选择相关的下载包，下载后根据提示安装。</p>
<p>安装结束后，打开终端输入git测试是否安装成功，安装成功后，需要把Git和GitHub账号绑定，首先设置user.name和user.email信息，在终端中输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git config --global user.name <span class="string">"你的GitHub用户名"</span></div><div class="line">git config --global user.email <span class="string">"你的GitHub注册邮箱"</span></div></pre></td></tr></table></figure>
<p>生成ssh密钥文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh-keygen -t rsa -C <span class="string">"你的GitHub注册邮箱"</span></div></pre></td></tr></table></figure>
<p>然后连续按三个回车即可，这里默认不需要设置密码。我们根据终端中提示的地址找到生成的 .ssh 文件夹，该文件夹为隐藏文件夹，可以在终端中用 cd 指令进入，我们需要的Key就存放在该文件夹下面的 id_rsa.pub 文件中，我们可以用指令<code>cat id_rsa.pub</code>打印出该文件中的内容，然后复制全部内容（以ssh-rsa开头）。</p>
<p>打开<a href="https://github.com/settings/keys" target="_blank" rel="external">GitHub_Settings_keys</a>页面，点击 new SSH Key 按钮：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_ujfyFj_3.jpeg" alt=""></center>

<p>其中Title可以随意填，因为我们可能会用不同的电脑连接GitHub，比如家里的电脑，公司的电脑等等，所以后面可能会添加多个Key，可以把Title取为Home或者School等方便区分。把刚复制的内容粘贴进去，点击Add SSH key。</p>
<p>在终端中检测GitHub公钥是否设置成功，输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh git@github.com</div></pre></td></tr></table></figure>
<p>显示以下内容说明成功：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_0OonVo_4.jpeg?imageView2/2/w/800" alt=""></center>

<blockquote>
<p>这里之所以设置GitHub密钥原因是，通过非对称加密的公钥与私钥来完成加密，公钥放置在GitHub上，私钥放置在自己的电脑里。GitHub要求每次推送代码都是合法用户，所以每次推送都需要输入账号密码验证推送用户是否是合法用户，为了省去每次输入密码的步骤，采用了ssh，当你推送的时候，git就会匹配你的私钥跟GitHub上面的公钥是否是配对的，若是匹配就认为你是合法用户，则允许推送。这样可以保证每次的推送都是正确合法的。</p>
</blockquote>
<p><br></p>
<h1 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h1><p>hexo基于Node.js，我们需要首先安装Node.js，下载地址<a href="https://nodejs.org/en/download/" target="_blank" rel="external">Download | Node.js</a>，注意安装Node.js会包含环境变量及npm的安装。</p>
<p>检测Node.js是否安装成功，在命令行中输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">node -v</div></pre></td></tr></table></figure>
<p>检查npm是否安装成功，在命令行输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm -v</div></pre></td></tr></table></figure>
<p>若显示版本号， 则表明安装成功。</p>
<p>对于Mac用户，要想安装hexo需要首先安装Xcode，我们可以从AppStore安装Xcode，安装结束后，我们需要安装Command Line Tools，Command LineTools是在Xcode中的一款工具，可以在命令行中运行C程序。打开终端输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xcode-select --install</div></pre></td></tr></table></figure>
<p>安装结束后，再次在终端输入：                                                                                              </p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xcode-select --install</div></pre></td></tr></table></figure>
<p>若显示command line tools are already installed，则表明安装成功。</p>
<p>到此，Hexo所需要的环境已经全部搭建完成。</p>
<p> <br></p>
<h1 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h1><p>我们可以用npm命令安装Hexo，打开终端输入：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install -g hexo-cli</div></pre></td></tr></table></figure>
<p>安装结束后，输入以下命令来初始化一个Hexo博客：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hexo init &lt;folder&gt; </div><div class="line"><span class="built_in">cd</span> &lt;folder&gt;</div><div class="line">npm install</div></pre></td></tr></table></figure>
<p>该命令会在当前文件夹下新建一个名叫folder的文件夹来初始化一个博客，如果命令中没有folder，则在当前文件夹初始化博客。比如我要初始化一个名字叫blog的博客，则需要输入<code>hexo init blog</code>，这样我们就在该目录下新建了一个名为blog的文件夹，接着我们利用命令<code>cd blog</code>进入该文件夹，输入<code>npm install</code>来安装必备的库。</p>
<p>到这里我们的网站就初始化完毕了，下面我们尝试添加一篇新的文章并且在本地服务器上查看。不出意外当前终端定位是在我们刚建立的文件夹下，即刚刚的blog文件夹下，此时在终端输入以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hexo new [layout] &lt;title&gt;  <span class="comment">#等价于 hexo n [layout] &lt;title&gt;</span></div><div class="line">hexo generate  <span class="comment">#等价于 hexo g</span></div><div class="line">hexo server  <span class="comment">#等价于 hexo s</span></div></pre></td></tr></table></figure>
<p>我新建了一篇名为“<em>The first article</em>”的文章。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_1ieo4u_5.jpeg?imageView2/2/w/800" alt=""></center>

<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204124_Mm7oau_6.jpeg?imageView2/2/w/800" alt=""></center>

<p>完成后打开浏览器输入地址：</p>
<p>Localhost:4000</p>
<p>刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。</p>
<p>可以进入我们刚刚建立的博客，看到我们刚刚建立的文章，以及Hexo默认自带的一篇文章。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_k0kChT_7.jpeg?imageView2/2/w/800" alt=""></center>

<p>接下来我们来介绍常用的Hexo命令（详细用法可查看Hexo官方文档<a href="https://hexo.io/docs/commands.html" target="_blank" rel="external">Commands | Hexo</a>）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo init <span class="comment">#初始化博客</span></div></pre></td></tr></table></figure>
<p>命令简写：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hexo n <span class="string">"我的博客"</span> <span class="comment">#  == hexo new "我的博客" ：新建文章</span></div><div class="line">hexo g <span class="comment"># == hexo generate ：生成</span></div><div class="line">hexo s <span class="comment"># == hexo server ：启动服务预览</span></div><div class="line">hexo d <span class="comment"># == hexo deploy ：部署</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hexo server <span class="comment">#Hexo会监视文件变动并自动更新，无须重启服务器</span></div><div class="line">hexo server -s <span class="comment">#静态模式</span></div><div class="line">hexo server -p 5000 <span class="comment">#更改端口</span></div><div class="line">hexo server -i 192.168.1.1 <span class="comment">#自定义 IP</span></div><div class="line">hexo clean <span class="comment">#清除缓存，若是网页正常情况下可以忽略这条命令</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="推送网站"><a href="#推送网站" class="headerlink" title="推送网站"></a>推送网站</h1><p>推送网站需要关联Github和Hexo，官方文档下的关联步骤，输入第一行代码后，需要打开blog根目录下的 <em>_config.yml</em> 文件，按照下面所示要求进行配置。在 Hexo中有两份主要的配置文件，其名称都是 <em>_config.yml</em>。其中，一份位于站点根目录下，主要包含Hexo本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。为了描述方便，在以下说明中，将前者称为站点配置文件，后者称为主题配置文件。我们在这一步只需要修改站点配置文件。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_CCOYpH_8.jpeg" alt=""></center>

<p>其实就是给<code>hexo d</code>这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。</p>
<p>接下来，我们输入三条命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hexo clean</div><div class="line">hexo g</div><div class="line">hexo d</div></pre></td></tr></table></figure>
<p>完成后打开浏览器，在地址栏输入 <strong>用户名</strong>.github.io ，这里的<strong>用户名</strong>为你的GitHub用户名，比如我的就是 <a href="zangbo.github.io">zangbo.github.io</a>，然后你就会发现你的博客已经可以在网络上被访问了。</p>
<p><br></p>
<h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><p>虽然在Internet上可以访问我们的网站，但是网址是GitHub提供的：<strong>xxx.github.io</strong> 而我们想使用我们自己刚买的个性化域名，这就需要绑定，这里以阿里云为例。登陆到阿里云：<a href="https://www.aliyun.com" target="_blank" rel="external">阿里云-为了无法计算的价值</a>，点击 <strong>控制台</strong> 进入管理控制台的域名列表，找到你的个性化域名，进入解析：</p>
<p>点击 <strong>添加解析</strong> 按钮，添加三条记录：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_6oNP5k_9.jpeg" alt=""></center>

<p>这里两条A记录是固定的，从GitHub官方文档可以找到，这两条是指向GitHub的地址；一条CNAME地址，记录指向是 <strong>用户名</strong>.github.io ，这里的<strong>用户名</strong>是你自己的用户名。</p>
<p>接着登陆GitHub，进入之前创建的仓库，点击 <strong>Settings</strong>，拉到最后，找到 <strong>Custom domain</strong>，填入自己申请的个性域名，点击 <strong>Save</strong>：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/个人博客/20170621204125_nNgXBn_10.jpeg" alt=""></center>

<p>完成后打开浏览器，在地址栏输入你的个性化域名，就可以直接进入你自己搭建的网站。这一步操作相当于在GitHub仓库目录添加一个CNAME文件，文件中的内容为你刚输入的domain，当GithubPage服务器接收到访问请求时，就知道对应的是这个工程了。</p>
<p><br></p>
<h1 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h1><p>我们刚得到的为Hexo的默认主题 <strong>landscape</strong>，Hexo官网提供了很多可供选择的主题：<a href="https://hexo.io/themes/" target="_blank" rel="external">Themes | Hexo</a>，目前比较流行的是 <strong>Next</strong>，<strong>Casper</strong>，<strong>Uno</strong>，<strong>Modernist</strong>，<strong>yilia</strong> 等等，每个人喜欢的风格不同，最终我选择了一款叫做 <strong>yelee</strong> 的主题，是作者在 <strong>yilia</strong> 的基础上改动来的。以下是几个主题的展示demo：</p>
<p><a href="http://notes.iissnan.com" target="_blank" rel="external">Next</a></p>
<p><a href="https://demo.ghost.io" target="_blank" rel="external">Casper</a></p>
<p><a href="http://daleanthony.com" target="_blank" rel="external">Uno</a></p>
<p><a href="https://orderedlist.com/modernist/" target="_blank" rel="external">Modernist</a></p>
<p><a href="http://litten.me" target="_blank" rel="external">yilia</a></p>
<p><a href="http://moxfive.xyz" target="_blank" rel="external">yelee</a></p>
<p>更换主题之需要打开根目录下的站点配置文件 _config.yml，修改其中的theme的值，例如改为next主题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">theme: next</div></pre></td></tr></table></figure>
<p>具体的主题配置需要修改各自主题下的主题配置文件 _config.yml，大家可以去各主题的GitHub上查看教程，还是比较详细的。</p>
<p><br></p>
<h1 id="更新文章"><a href="#更新文章" class="headerlink" title="更新文章"></a>更新文章</h1><p>更新文章可以打开终端进入博客根目录，输入指令<code>hexo n &quot;文章名&quot;</code>来实现，然后打开根目录下的source/_posts文件夹，可以发现我们新建的 <strong>文章名.md</strong> 文件，然后就可以打开它用Markdown语法来写文章了，具体的Markdown语法教程可以参看这篇：<a href="http://www.appinn.com/markdown/#code" target="_blank" rel="external">Markdown语法说明(简体中文版) </a>。另外补充几点上面没有的：</p>
<ol>
<li>图像居中实现方法：<code>&lt;center&gt;图像&lt;/center&gt;</code></li>
<li>字体加粗可以前后各加两个星号<em>来实现： **</em>需要加粗的文字<em>**</em></li>
<li>换行标签<code>&lt;br /&gt;</code></li>
</ol>
<p>大部分常用用法在上面教程都可以找到。这里注意，在本地修改网站或者更新文章时，可以用指令<code>hexo s</code>在本地服务器看修改效果，打开浏览器输入<code>localhost:4000</code>，每次修改后网站会实时的发生变动。但是修改后要用指令<code>hexo g</code>和<code>hexo d</code>部署到GitHub上，此时的CNAME文件和README.md文件往往会消失，需要重新创建。为了方便起见可以打开网站根目录的source文件夹，把CNAME文件和README.md文件存放在里面。注意CNAME文件没有扩展名。</p>
<p>此时还有个问题，如果把README.md文件放在source文件夹中，执行<code>hexo g</code>指令时会被渲染，解决办法是只要在博客根目录下的配置文件_config.yml中配置一下”skip_render”选项，将不需要渲染的文件名称加入的其选项下就行了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">skip_render: README.md</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="寻找图床"><a href="#寻找图床" class="headerlink" title="寻找图床"></a>寻找图床</h1><p>当文章中有图片时，若是少量图片，可以直接把图片存放在source文件夹中，但图片会占据大量的存储的空间，加载的时候相对缓慢 ，这时考虑把博文里的图片上传到某一网站，然后获得外部链接，使用Markdown语法： <strong>![图片信息](外部链接)</strong> 完成图片的插入，这种网站就被成为图床。这里推荐<a href="https://www.qiniu.com" target="_blank" rel="external">七牛云</a>，新人有10G的免费流量，对于个人博客来说是足够了，超出后的的费用也不贵，也是学生党可以负担得起的。</p>
<p><br></p>
<h1 id="功能扩展"><a href="#功能扩展" class="headerlink" title="功能扩展"></a>功能扩展</h1><h2 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h2><p>打开文件：</p>
<p><code>根目录/node_modules/hexo-generator-index/lib/generator.js</code></p>
<p>替换为以下代码并保存：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="meta">'use strict'</span>;</div><div class="line"></div><div class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">'hexo-pagination'</span>);</div><div class="line"></div><div class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">locals</span>)</span>&#123;</div><div class="line">  <span class="keyword">var</span> config = <span class="keyword">this</span>.config;</div><div class="line">  <span class="keyword">var</span> posts = locals.posts;</div><div class="line"></div><div class="line">    posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</div><div class="line">        <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123; <span class="comment">// 两篇文章top都有定义</span></div><div class="line">            <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date; <span class="comment">// 若top值一样则按照文章日期降序排</span></div><div class="line">            <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top; <span class="comment">// 否则按照top值降序排</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></div><div class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date; <span class="comment">// 都没定义按照文章日期降序排</span></div><div class="line"></div><div class="line">    &#125;);</div><div class="line"></div><div class="line">  <span class="keyword">var</span> paginationDir = config.pagination_dir || <span class="string">'page'</span>;</div><div class="line"></div><div class="line">  <span class="keyword">return</span> pagination(<span class="string">''</span>, posts, &#123;</div><div class="line">    <span class="attr">perPage</span>: config.index_generator.per_page,</div><div class="line">    <span class="attr">layout</span>: [<span class="string">'index'</span>, <span class="string">'archive'</span>],</div><div class="line">    <span class="attr">format</span>: paginationDir + <span class="string">'/%d/'</span>,</div><div class="line">    <span class="attr">data</span>: &#123;</div><div class="line">      <span class="attr">__index</span>: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">  &#125;);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>在写文章时，只需要设置top参数为true即可。</p>
<p><br></p>
<h2 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h2><p>Hexo允许使用MathJax来渲染LaTeX数学公式，首先安装自动部署MathJax的插件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-math --save</div><div class="line">hexo math install</div></pre></td></tr></table></figure>
<p>接着把主题配置文件下的mathjax选项设置为true：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mathjax: true</div></pre></td></tr></table></figure>
<p>如果直接在新建的博文中使用LaTeX格式的公式会出现一些问题，原因是hexo先用marked.<a href="http://lib.csdn.net/base/javascript" target="_blank" rel="external">js</a>渲染，然后再交给MathJax渲染。在marked.js渲染的时候下划线<code>_</code>是被escape掉并且换成了<code>&lt;em&gt;</code>标签，即斜体字，另外LaTeX中的<code>\\</code>也会被转义成一个<code>\</code>，这样会导致MathJax渲染时不认为它是一个换行符了。</p>
<p>我们只要不让marked.js去转义<code>\\</code>,<code>\{</code>,<code>\}</code>在MathJax中有特殊用途的字符就行了。<br>具体修改方式，用编辑器打开marked.js（在<code>./node_modules/marked/lib/</code>中）</p>
<p><strong>Step 1:</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</div></pre></td></tr></table></figure>
<p>替换成</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()# +\-.!_&gt;])/</span>,</div></pre></td></tr></table></figure>
<p>这一步是在原基础上取消了对<code>\\</code>,<code>\{</code>,<code>\}</code>的转义(escape)</p>
<p><strong>Step 2:</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">em: <span class="regexp">/^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</div></pre></td></tr></table></figure>
<p>替换成</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">em:<span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</div></pre></td></tr></table></figure>
<p>这样一来MathJax就能与marked.js共存了。</p>
<p>在Markdown中输入数学公式：</p>
<ul>
<li>行内：<code>$数学公式$</code></li>
<li>行间：<code>$$数学公式$$</code></li>
</ul>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>当初也是看到很多人有自己的网站，所以才萌生了这个想法，希望这篇文章能给大家一点启发，如果后面有更多的心得，也会另发文总结下来。另外推荐一个我比较喜欢的个人博客：<a href="http://www.dandyweng.com" target="_blank" rel="external">翁天信 · Dandy Weng</a></p>
<p>有任何问题欢迎留言或者发邮件。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>知乎<a href="https://www.zhihu.com/people/wurun/answers" target="_blank" rel="external">吴润</a>的专栏文章在Windows的环境的配置教程：<a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="external">GitHub+Hexo 搭建个人网站详细教程 - 知乎专栏</a></li>
<li>GitHub Pages 官方使用说明：<a href="https://pages.github.com" target="_blank" rel="external">GitHub Pages</a></li>
<li>GitHub 官方文档，如何绑定个人域名：<a href="https://help.github.com/articles/using-a-custom-domain-with-github-pages/" target="_blank" rel="external">Using a custom domain with GitHub Pages</a></li>
<li>Hexo 官方文档：<a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="external">文档 | Hexo</a></li>
<li><a href="http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/" target="_blank" rel="external">解决Hexo置顶问题 | Netcan_Space</a></li>
<li><a href="http://blog.csdn.net/emptyset110/article/details/50123231" target="_blank" rel="external">搭建一个支持LaTEX的hexo博客 - Platonic Time - 博客频道 - CSDN.NET</a></li>
</ul>
<p><br></p>
</the>]]></content>
      
        <categories>
            
            <category> 教程 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 个人网站 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[欢迎来到我的小站]]></title>
      <url>/2017/05/20/%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br><img src="/background/bg-2.jpg" width="520" alt="Welcome"></excerpt></p>
<hr>
<p>本站域名 <code>http://zangbo.me</code>。</p>
<p>一只机器学习菜鸟研究生，平时会分享一些机器学习、深度学习、数据挖掘、计算机视觉、编程语言相关的笔记，也有一些教程以及个人随笔。本人由于学识尚浅，文中难免会有错误，如发现请邮件告知，同时也欢迎与我交流。</p>
<p>本站全部文章均为个人原创，如需转载请注明出处和作者。</p>
<the rest="" of="" contents="" |="" 余下全文=""></the>]]></content>
      
        
    </entry>
    
  
  
</search>
