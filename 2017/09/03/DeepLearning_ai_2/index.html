<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="deeplearning.ai," />





  <link rel="alternate" href="/atom.xml" title="ZangBo's Home" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta name="description" content="最近在学习Andrew Ng（吴恩达）开设的deeplearning.ai课程，本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第一周学习笔记，主要内容是数据集划分，偏差和方差，解决过拟合问题的正则化方法（包括L2正则化和dropout），Data a">
<meta name="keywords" content="deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearning.ai学习笔记（二）">
<meta property="og:url" content="http://zangbo.me/2017/09/03/DeepLearning_ai_2/index.html">
<meta property="og:site_name" content="ZangBo&#39;s Home">
<meta property="og:description" content="最近在学习Andrew Ng（吴恩达）开设的deeplearning.ai课程，本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第一周学习笔记，主要内容是数据集划分，偏差和方差，解决过拟合问题的正则化方法（包括L2正则化和dropout），Data a">
<meta property="og:updated_time" content="2017-09-07T09:33:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepLearning.ai学习笔记（二）">
<meta name="twitter:description" content="最近在学习Andrew Ng（吴恩达）开设的deeplearning.ai课程，本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第一周学习笔记，主要内容是数据集划分，偏差和方差，解决过拟合问题的正则化方法（包括L2正则化和dropout），Data a">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zangbo.me/2017/09/03/DeepLearning_ai_2/"/>





  <title>DeepLearning.ai学习笔记（二） | ZangBo's Home</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZangBo's Home</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">永远年轻 永远热泪盈眶</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zangbo.me/2017/09/03/DeepLearning_ai_2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZangBo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZangBo's Home">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DeepLearning.ai学习笔记（二）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-03T21:21:33+08:00">
                2017-09-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>最近在学习Andrew Ng（吴恩达）开设的deeplearning.ai课程，本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第一周学习笔记，主要内容是数据集划分，偏差和方差，解决过拟合问题的正则化方法（包括L2正则化和dropout），Data augmentation和early stopping，加速训练的方法像归一化输入，Xavier初始化权值方法，以及确保反向传播的梯度检验。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h1><p>数据集一般分为训练数据(trainingdata)、验证数据(development data)和测试数据(test data)。</p>
<p>传统机器学习算法的分割方式一般为60%、20%、20%。</p>
<p>大数据时代不需要特别多的验证数据和测试数据，如果有100万数据，那么可能训练数据有98万，验证数据和测试数据各1万，还可能更少各5000。</p>
<p>验证数据的作用是选择更好的算法，可以理解为确定超参数，测试数据用来无偏估计算法的性能。如果不需要无偏估计算法性能，有时候也可以不需要测试数据，直接用验证数据来当测试数据用，这种情况下有的人也把它称为测试数据，最合适的叫法是<strong>训练-验证集</strong>。</p>
<p>验证集和测试集的分布要相同，即来源要相同，它们两者和训练集有时为了更快捷的获得数据可以分布不同。</p>
<p><br></p>
<h1 id="偏差-amp-方差"><a href="#偏差-amp-方差" class="headerlink" title="偏差&amp;方差"></a>偏差&amp;方差</h1><p>偏差(bias)和方差(variance)，训练集损失一般用来衡量偏差，验证集损失一般用来衡量方差。基础损失(base error)一般表示一个可以达到的最低损失，比如人眼观测的损失。</p>
<p><br></p>
<p>举个例子：假如有个判断是否为猫的图片的数据集，人眼错误率为0。</p>
<p>训练集损失低(1%)，验证集损失高(15%)，过拟合，高方差。</p>
<p>训练集损失高(15%)，验证集损失高(16%)，两者相差不多且明显高于基础损失(0)，欠拟合，高偏差。</p>
<p>训练集损失高(15%)，验证集损失更高(30%)，且明显高于基础损失(0)，高偏差，高方差。</p>
<p>训练集损失低(0.5%)，验证集损失低(1%)，且和基础损失(0)差不多，低偏差，低方差。</p>
<p> <br></p>
<p>在训练一个网络时，首先要做的是观察偏差，先把偏差降到最低。如果偏差过高，那么使用更大规模的神经网络，一般这种做法是有用的。也可以训练更长时间，这方法有时候有用。</p>
<p>一旦偏差降低到可接受程度，再看方差，通过查看验证集损失。若方差高，可以使用更多的数据，一般是有用的。如果不能得到更多数据，可以使用正则化的方法。当然也可以更换神经网络，通过找到合适的神经网络实现一箭双雕的目的。</p>
<p>重复以上过程直到实现低方差和低偏差。</p>
<p> <br></p>
<p>在机器学习初期，通常方差和偏差互相影响。而在深度学习时期，通常使用一个更大规模的网络配合正则化，可以在不影响方差的基础上降低偏差。而增加更多的的数据量，可以在不影响偏差的基础上降低方差。</p>
<p><br></p>
<h1 id="降低方差-amp-减小过拟合"><a href="#降低方差-amp-减小过拟合" class="headerlink" title="降低方差&amp;减小过拟合"></a>降低方差&amp;减小过拟合</h1><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>正则化方法分为多种，用的比较多的是L2正则化和dropout。</p>
<p><br></p>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>使用L2正则化的交叉熵损失：<br>$$<br>J{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{L}\right) + (1-y^{(i)})\log\left(1- a^{L}\right) \large{)} }\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W{k,j}^{[l]2} }_\text{L2 regularization cost}<br>$$<br>正则化用于减少方差，一般用L2正则化而不用L1，因为L1正则化倾向于把W变得很稀疏，会有很多0。L2正则化在代价函数后面加上$\frac{\lambda}{2m} ||W||^2$，因为<code>lambda</code>是python的保留字，所以我们用<code>lambd</code>表示正则化系数，这是个超参数。在训练时，在dW项后面加一个$\frac{\lambda}{m}W$，即正则化项总是倾向于使得W减少，相当于乘了一个小于1的权重衰减系数（weightdecay)。<br>$$<br>\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W<br>$$<br>关于正则化为什么有用，其中一种说法是，当使用正则化时，整个网络会被迫使得一部分参数项变小，进而使得原本复杂的网络变的简单，相当于变相的把复杂的神经网络简单化，也就减小了过拟合的程度。</p>
<p><br></p>
<p>总结：</p>
<ol>
<li>代价函数要添加正则化项</li>
<li>反向传播梯度dW要添加额外的项$\frac{\lambda}{m}W$</li>
<li>正则化使得参数更倾向于取较小的值</li>
</ol>
<p><br></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>dropout本身是一种正则化方法，可以理解为，神经网络不会把高权值放在任何一个神经元节点上，因为每一个节点都有可能失活，于是它会倾向于利用每一个神经元把它们权值压缩的很小，某种意义上和L2正则化相似。</p>
<p>如果某一层输出为a，则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">d= np.random.rand(a.shape[<span class="number">0</span>], a.shape[<span class="number">1</span>]) &lt; keep_prob</div><div class="line">a =np.multiply(a, d)</div><div class="line">a = a / keep_prob <span class="comment">#为了不影响a的期望值，添加这一步，使得测试阶段不需要调整数值范围。</span></div></pre></td></tr></table></figure>
<p>我们只需要在训练阶段使用dropout，在测试阶段不需要使用。</p>
<p>我们可以在参数矩阵W较大的层上使用较小的keep_prob，一般不对输入层使用dropout，一般只在计算机视觉领域使用dropout，因为通常没有足够的数据，dropout仅仅用于解决过拟合问题。</p>
<p>dropout缺点是代价函数J不再被明确定义。梯度下降不容易复查，通常我们会关闭dropout，确保J单调递减，再打开dropout。</p>
<p><br></p>
<p>实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#正向传播：</span></div><div class="line"><span class="comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span></div><div class="line">D1 =np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])</div><div class="line"><span class="comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span></div><div class="line">D1 = D1 &lt; keep_prob  </div><div class="line"><span class="comment"># Step 3: shut down some neurons of A1</span></div><div class="line">A1 = A1 * D1       </div><div class="line"><span class="comment"># Step 4: scalethe value of neurons that haven't been shut down</span></div><div class="line">A1 = A1 / keep_prob                             </div><div class="line">  </div><div class="line"><span class="comment">#反向传播</span></div><div class="line"><span class="comment"># Step 1: Apply mask D1 to shutdown the same neurons as during the forward propagation</span></div><div class="line">dA1 = dA1 * D1              </div><div class="line"><span class="comment"># Step 2: Scale the value ofneurons that haven't been shut down</span></div><div class="line">dA1 = dA1 / keep_prob</div></pre></td></tr></table></figure>
<p><br></p>
<p>总结：</p>
<ol>
<li>dropout是一种正则化方法</li>
<li>只能在训练过程中使用dropout，不能在测试时使用</li>
<li>在前向传播和反向传播中都使用dropout</li>
<li>在dropout时，因为dropout过程会使得输出值变为原本的keep_prob倍，固把输出A除以keep_prob的值，可以使得激活函数的输出得到期盼的值。</li>
</ol>
<p><br></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>不同的正则化方法得到的实验结果：</p>
<table>
<thead>
<tr>
<th>model</th>
<th>train accuracy</th>
<th>test accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN without regularization</td>
<td>95%</td>
<td>91.5%</td>
</tr>
<tr>
<td>3-layer NN with L2-regularization</td>
<td>94%</td>
<td>93%</td>
</tr>
<tr>
<td>3-layer NN with dropout</td>
<td>93%</td>
<td>95%</td>
</tr>
</tbody>
</table>
<p>正则化方法会降低训练准确率，但是会有利于测试集准确率。</p>
<p>结论：</p>
<ol>
<li>正则化有利于避免过拟合</li>
<li>正则化会使得参数倾向于更小的值</li>
<li>L2正则化和Dropout是两种很有效的正则化方法。</li>
</ol>
<p><br></p>
<h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><p>解决过拟合问题，如果不能增加数据量，还可以对图片水平翻转，随机旋转，随机裁剪放大后的图片，称为data augmentation。</p>
<p><br></p>
<h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p>另一个解决过拟合的方法是early stopping。验证集误差不再下降则停止训练。它和L2正则化可以二选一。L2正则化需要更大的计算代价，需要找最优的正则化系数。early stopping缺点是不能独立的解决训练数据最优化和防止过拟合两个问题，我们一般倾向于用独立的方法解决单一问题，所以一般不用early stopping。</p>
<p><br></p>
<h1 id="加速训练"><a href="#加速训练" class="headerlink" title="加速训练"></a>加速训练</h1><h2 id="归一化输入数据"><a href="#归一化输入数据" class="headerlink" title="归一化输入数据"></a>归一化输入数据</h2><p>其中一个加速训练的方法是归一化输入。第一步零均值化，第二步归一化方差。如果用某均值和方差来归一化训练集，那么应该用同样的均值和方差来归一化测试集。我们在两个数据集上要使用同样的数据转换。</p>
<p> 归一化的目的是使得输入数据的各个维度的特征具有相似的范围，这样更有利于梯度下降算法的实施，我们也就可以用相对来说更大的学习率，训练速度就会变的更快。比如如果x1范围是[0, 1000]，x2范围是[0, 1]，优化起来就会变的特别慢。<br><br></p>
<h2 id="Xavier初始化权值"><a href="#Xavier初始化权值" class="headerlink" title="Xavier初始化权值"></a>Xavier初始化权值</h2><p>梯度消失和梯度爆炸，梯度值将伴随着层数呈指数增加或衰减，目前的解决方法之一是使用Xavier方法初始化权值，这也是加快训练速度的技巧之一。</p>
<p>Xavier初始化指的是当激活函数为relu时，<code>W = np.random.randn(…) * np.sqrt(2/n[l-1])</code>，使得W的方差变为2/n，这样最终的输出a依然是可以接受的方差。如果激活函数是tanh，则为<code>W = np.random.randn(…) *np.sqrt(1/n[l-1])</code>，把W方差变为1/n。</p>
<p><br></p>
<p>参数初始化时需要注意的点：</p>
<ol>
<li>不同的初始化方式导致不同的结果，较差的方法会导致梯度爆炸或梯度消失，</li>
<li>随机初始化用来打破对称性，使得不同的隐层神经元学到不同的东西。</li>
<li>不要把权值初始化太大，会减慢训练速度。</li>
<li>He初始化方法对于ReLU网络效果最好，即<code>W = np.random.randn(...) * np.sqrt(2/layers_dims[l-1])</code></li>
</ol>
<table>
<thead>
<tr>
<th>Model</th>
<th>Train accuracy</th>
<th>Problem/Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN with zeros initialization</td>
<td>50%</td>
<td>fails to break symmetry</td>
</tr>
<tr>
<td>3-layer NN with large random initialization</td>
<td>83%</td>
<td>too large weights</td>
</tr>
<tr>
<td>3-layer NN with He initialization</td>
<td>99%</td>
<td>recommended method</td>
</tr>
</tbody>
</table>
<p> <br></p>
<h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>梯度检验用于检查反向传播是否正确实施，做梯度检验时，使用双边误差比单边误差更准确。不要再训练过程中使用梯度检验，它只用来debug。 </p>
<p>梯度检验时，把所有的参数矩阵W转换成一个一维向量，然后把所有的转换后的W一维向量以及一维的b参数连接成一个大的一维向量$\theta$。对于dW和db做同样的对应处理，得到$d\theta$。然后对$J(\theta)$进行双边误差计算(公式4)，得到$d{\theta}approx$，然后计算$d{\theta}approx$和$d\theta$的距离(公式5) 。</p>
<p>$$<br>\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}<br>$$</p>
<p>$$<br>difference = \frac {| grad - gradapprox |_2}{| grad |_2 + | gradapprox |_2 }<br>$$</p>
<p>梯度检验时要加上正则化项，但梯度检验不能和dropout同时使用，进行梯度检验时要关掉dropout。 </p>
<p><br>  </p>
<p>总结：</p>
<ol>
<li>如果$\epsilon=1e-7$，则最后的difference应小于等于1e-7，这样说明梯度下降没问题。</li>
<li>梯度检查速度很慢，因此不要在训练过程中执行梯度检查，只需要在验证梯度下降是否正确时使用。</li>
<li>梯度检查不能再dropout时使用，应先关闭dropout，使用梯度检查确保梯度下降正确，再使用dropout进行训练。</li>
</ol>
<p><br></p>
</the></excerpt>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deeplearning-ai/" rel="tag"># deeplearning.ai</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/03/DeepLearning_ai_1/" rel="next" title="deeplearning.ai学习笔记（一）">
                <i class="fa fa-chevron-left"></i> deeplearning.ai学习笔记（一）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/07/DeepLearning_ai_3/" rel="prev" title="deeplearning">
                deeplearning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="ZangBo" />
          <p class="site-author-name" itemprop="name">ZangBo</p>
           
              <p class="site-description motion-element" itemprop="description">A student from SJTU</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#数据集划分"><span class="nav-number">1.</span> <span class="nav-text">数据集划分</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#偏差-amp-方差"><span class="nav-number">2.</span> <span class="nav-text">偏差&方差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#降低方差-amp-减小过拟合"><span class="nav-number">3.</span> <span class="nav-text">降低方差&减小过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-number">3.1.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#L2正则化"><span class="nav-number">3.1.1.</span> <span class="nav-text">L2正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">3.1.2.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">3.1.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-augmentation"><span class="nav-number">3.2.</span> <span class="nav-text">Data augmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Early-stopping"><span class="nav-number">3.3.</span> <span class="nav-text">Early stopping</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#加速训练"><span class="nav-number">4.</span> <span class="nav-text">加速训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化输入数据"><span class="nav-number">4.1.</span> <span class="nav-text">归一化输入数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Xavier初始化权值"><span class="nav-number">4.2.</span> <span class="nav-text">Xavier初始化权值</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度检验"><span class="nav-number">5.</span> <span class="nav-text">梯度检验</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZangBo</span>
</div>



        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>
