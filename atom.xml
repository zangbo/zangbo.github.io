<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZangBo&#39;s Home</title>
  <subtitle>永远年轻 永远热泪盈眶</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zangbo.me/"/>
  <updated>2017-10-08T02:56:43.000Z</updated>
  <id>http://zangbo.me/</id>
  
  <author>
    <name>ZangBo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>EM算法</title>
    <link href="http://zangbo.me/2017/10/07/EM/"/>
    <id>http://zangbo.me/2017/10/07/EM/</id>
    <published>2017-10-07T08:16:08.000Z</published>
    <updated>2017-10-08T02:56:43.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文介绍了机器学习领域经典算法之一的EM算法，也被称为数据挖掘十大算法之一。EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望（expectation）；M步，求极大（maximization）。所以这一算法称为<strong>期望极大算法（expectation maximization algorithm）</strong>，简称为EM算法。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="预知识"><a href="#预知识" class="headerlink" title="预知识"></a>预知识</h1><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>$D$</td>
<td>已有的数据(data)</td>
</tr>
<tr>
<td>$\theta$</td>
<td>要估计的参数(parameter)</td>
</tr>
<tr>
<td>$p(\theta)$</td>
<td>先验概率(prior)</td>
</tr>
<tr>
<td>$p(\theta\mid{D})$</td>
<td>后验概率(posterior)</td>
</tr>
<tr>
<td>$p(D)$</td>
<td>数据分布(evidence)</td>
</tr>
<tr>
<td>$p(D\mid\theta)$</td>
<td>似然函数(likelihood of θ w.r.t. D)</td>
</tr>
<tr>
<td>$p(y,\theta\mid{D})$</td>
<td>已知数据条件下的y,θ概率</td>
</tr>
</tbody>
</table>
<h2 id="先验概率-amp-后验概率"><a href="#先验概率-amp-后验概率" class="headerlink" title="先验概率&amp;后验概率"></a>先验概率&amp;后验概率</h2><p>先验概率(prior)与后验概率(posterior)简称为<strong>先验</strong>和<strong>后验</strong>。</p>
<p>这两个概念来自于贝叶斯定理，我们肯定是针对同一个事物才有先后之分，如果针对两个事物，先后不是没有意义了么？那这个共同的对象，就是我们的参数$\theta$。后验概率是指掌握了一定量的数据后我们的参数分布是怎么样的，表示为$p(\theta|D)$；那先验就是在没有掌握数据时我们的参数怎么分布。</p>
<p>看到这里，你可能会问：如果连数据都没有，我怎么知道我的参数是怎么分布的？你提出这个问题，就说明你是一个赤裸裸的频率派学家，你需要通过数据来得到你的参数！而这并不是贝叶斯派的考虑，贝叶斯估计最重要的就是那个先验的获得。虽然你这次的一组数据，比如说扔三次硬币产生的序列是（110），但是其实根据我历史的经验来看，一枚硬币正反面其实很有可能是按照均匀分布来的，只不过可能因为你<strong>抛得次数少</strong>了所以产生了不是均匀分布的效果，因此我要考虑以往的经验在里面。</p>
<p>你可能又会问：那你这个均匀分布不就是完全猜来的嘛，你怎么知道我这次是不是一样的硬币呢？没错！就是<strong>“猜来的”</strong>。先验在很多时候完全是假设，然后去验证有的数据是否吻合先验猜想，所以这里的猜很重要。还要注意，先验一定是与数据无关的，你不能看到了数据再做这些猜想，一定是<strong>没有任何数据之前</strong>你就猜了一个参数的先验概率。</p>
<h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p>通常，事件A在事件B（发生）的条件下的概率，与事件B在事件A（发生）的条件下的概率是不一样的；然而，这两者是有确定的关系的，贝叶斯定理就是这种关系的陈述。贝叶斯公式的一个用途在于通过已知的三个概率函数推出第四个。</p>
<p>作为一个普遍的原理，贝叶斯定理对于所有概率的解释是有效的；然而，<strong>频率主义者</strong>和<strong>贝叶斯主义者</strong>对于在应用中，某个随机事件的概率该如何被赋值，有着不同的看法：频率主义者根据随机事件发生的频率，或者总体样本里面的发生的个数来赋值概率；贝叶斯主义者则根据未知的命题来赋值概率。这样的理念导致贝叶斯主义者有更多的机会使用贝叶斯定理。<br>$$<br>P(A|B) = \frac{P(B|A)P(A)}{P(B)}\tag{1}<br>$$<br>在贝叶斯定理中，每个名词都有约定俗成的名称：</p>
<ul>
<li>P(<em>A</em>|<em>B</em>)是已知B发生后A的条件概率，也因为和B有关被称作A的后验概率。</li>
<li>P(<em>B</em>|<em>A</em>)是已知A发生后B的条件概率，也因为和A有关被称作B的后验概率。</li>
<li>P(<em>A</em>)是A的先验概率（或边缘概率），因为它不考虑任何B方面的因素。</li>
<li>P(<em>B</em>)是B的先验概率（或边缘概率），因为它不考虑任何A方面的因素。</li>
</ul>
<p>按这些术语，贝叶斯定理可表述为：<br>$$<br>后验概率 = \frac{相似度\times先验概率}{标准化向量}\tag{2}<br>$$</p>
<p>也就是说，后验概率与先验概率和相似度的乘积成正比。</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>极大似然估计法认为参数是固有的，但是可能由于一些外界噪声的干扰，使数据看起来不是完全由参数决定的。没关系，数学家们觉得，虽然有误差存在，<strong>但只要让在这个数据给定的情况下，找到一个概率最大的参数就可以了</strong>。那问题其实就变成了一个条件概率最大的求解，即求使得$p(\theta|D)$ 最大的参数$\theta$，形式化表达为求解<br>$$<br>\arg\max_{\theta}p(\theta|D)\tag{3}<br>$$</p>
<p>而根据条件概率公式有<br>$$<br>p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}\tag{4}<br>$$</p>
<p>因为我们在极大似然估计中假设是确定的，所以$p(\theta)$就是一个常数。$p(D)$同样是根据已有的数据得到的，也是确定的，或者我们可以把其看作是对整个概率的一个归一化因子。这时候，求解公式(3)就变成了求解<br>$$<br>\arg\max_{\theta}p(D|\theta)\tag{5}<br>$$<br>(5)式中的$p(D|\theta)$就是似然函数，我们要做的就是求一个是似然最大的参数，所以称为极大似然估计。想求解这个问题，需要假设我们的数据是相互独立的。$D=\{y1,y_2,y_3,…,y_n\}$，这时候有<br>$$<br>p(D|\theta)=\prod_{i=1}^{n}p(y_i|\theta)\tag{6}<br>$$<br>一般对(6)式取对数求解对数极大似然，就可以把连乘变成求和，然后求导取极值点就是要求的参数值，不在此赘述。</p>
<h2 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h2><p>若f(x)是区间(a,b)上的凹函数，则对任意的$x_1,x_2,x_3,…,x_n\in(a,b)$，有：<br>$$<br>f(\frac{x_1+x_2+x_3+…+x_n}{n})\le \frac{f(x_1)+f(x_2)+f(x_3)+…+f(x_n)}{n}\tag{7}<br>$$<br>当且仅当$x_1=x_2=x3=…=x_n$时等号成立。</p>
<p>若f(x)是区间(a,b)上的凸函数，则对任意的$x_1,x_2,x_3,…,x_n\in(a,b)$，有：<br>$$<br>f(\frac{x_1+x_2+x_3+…+x_n}{n})\ge \frac{f(x_1)+f(x_2)+f(x_3)+…+f(x_n)}{n}\tag{8}<br>$$<br>当且仅当$x_1=x_2=x3=…=x_n$时等号成立。</p>
<p>其加权形式为，若f(x)是区间(a,b)上的凹函数，则对任意的$x_1,x_2,x_3,…,x_n\in(a,b)$, $\sum_{i=1}^na_i=1$有：<br>$$<br>f(a_1x_1+a_2x_2+a_3x_3+…+a_nx_n)\leq a_1f(x_1)+a_2f(x_2)+a_3f(x_3)+…+a_nf(x_n)\tag{8}<br>$$<br>凸函数同理。</p>
<p><br></p>
<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>概率模型有时既含有观测变量，又含有隐变量或潜在变量，如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单的使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。这里仅讨论极大似然估计法。</p>
<p>在理解EM算法之前，先来举个例子，我们叫它<strong>“三硬币模型”</strong>：现在有A、B、C三个不同的硬币，它们的材料并非均匀，投掷一次正面朝上的概率分别为$\pi、p、q$。在每轮开始前首先掷硬币A，根据结果选择另外两个硬币，如果是正面则选择B来掷，如果是反面则选择C来掷。选择完之后开始掷选择的硬币，如果是正面则记为1，反面则记为0。在每轮开始前都先掷A进行选择，然后记录B或C的投掷结果。独立重复n次试验，这里n=10，得到结果如下：<br>$$<br>1，1，0，1，0，0，1，0，1，1<br>$$<br>假设我们只能观测到掷硬币的结果，不能观测掷硬币的过程，问如何估计三硬币出现的概率，即三硬币模型的参数。</p>
<p>这里的观测变量Y即如上的观测结果，隐变量Z就是每次掷硬币选择的是B还是C，我们无法得知Z的值，因为我们无法观测掷硬币的过程。这种情况下，我们就可以用EM算法来解决这个问题。</p>
<h2 id="感性的理解"><a href="#感性的理解" class="headerlink" title="感性的理解"></a>感性的理解</h2><p>这部分有一篇论文个人感觉写的特别好，强烈推荐大家：</p>
<p><a href="https://www.nature.com/nbt/journal/v26/n8/pdf/nbt1406.pdf" target="_blank" rel="external">What is the expectation maximization algorithm?</a></p>
<blockquote>
<p>Do, C. B., &amp; Batzoglou, S. (2008). What is the expectation maximization algorithm?. Nature biotechnology, 26(8), 897.</p>
</blockquote>
<p>论文中提到的例子，我用python做了简单的实现，代码放在了我的GitHub上，地址：</p>
<p><a href="https://github.com/zangbo/MachineLearning/tree/master/EM" target="_blank" rel="external">https://github.com/zangbo/MachineLearning/tree/master/EM</a></p>
<p>关于论文的中文解析可以参考这篇博文：</p>
<p><a href="http://www.jianshu.com/p/1121509ac1dc" target="_blank" rel="external">如何感性的理解EM算法? - 简书</a></p>
<h2 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h2><p>这里我们令观测数据为Y。我们面对一个概率模型，用极大似然估计法来做的话就是要极大化观测数据Y关于参数$\theta$的对数似然函数$L(\theta)=logP(Y|\theta)$。然而在存在隐变量Z的情况下，完整数据除了Y之外还包括Z，这时我们把上面的对数似然函数就变成了：<br>$$<br>L(\theta)=logP(Y|\theta)=log\sum_ZP(Y,Z|\theta)\tag{9}<br>$$<br>极大化该似然函数是比较困难的，因为其中包含了隐变量Z，属于未观测数据。这时候我们想，我们能不能找出来一个式子，使得$L(\theta)$总能大于或等于该式子，那么如果我们能求出该式子的极大值，是不是就可以进而得出$L(\theta)$的极大值呢？我们首先对如上式子做一个等价变换：<br>$$<br>log\sum_ZP(Y,Z|\theta)=log\sum_ZG(Z)\frac{P(Y,Z|\theta)}{G(Z)}\tag{10}<br>$$<br>这里的G函数我们可以认为是只和Z有关的概率函数。接下来要用到Jensen不等式，上面已经介绍过Jensen不等式，这里我们用的是$log\sum_j\lambda_jy_j\ge\sum_j\lambda_jlogy_j$，其中$\lambda_j\ge0$，$\sum_j\lambda_j=1$。根据Jensen不等式，(10)式可以写成：<br>$$<br>log\sum_ZG(Z)\frac{P(Y,Z|\theta)}{G(Z)}\ge \sum_ZG(Z)log\frac{P(Y,Z|\theta)}{G(Z)} \tag{11}<br>$$<br>这里G需要满足的条件是$\sum_ZG(Z)=1$，接着我们想等号成立的条件是什么，也就是左式的下限是什么。使得等号成立，则必须满足$\frac{P(Y,Z|\theta)}{G(Z)}=c$，其中c为常数。那么同时满足这两个条件的G，我们可以构造它为：<br>$$<br>\begin{split}<br>G(Z)=&amp;\frac{P(Y,Z|\theta^{(i)})}{\sum_ZP(Y,Z|\theta^{(i)})}\\<br>       =&amp;\frac{P(Y,Z|\theta)}{P(Y|\theta^{(i)})}\\<br>       =&amp;P(Z|Y,\theta^{(i)})<br> \end{split}<br> \tag{12}<br>$$<br>G(Z)的数学含义可以看出是在给定观测数据Y和当前参数估计$\theta^{(i)}$下隐变量数据Z的条件概率分布，代入(11)式得：<br>$$<br>\sum_ZG(Z)log\frac{P(Y,Z|\theta)}{G(Z)}=\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})} \tag{13}<br>$$<br>此时只需要极大化右式即可，省去对$\theta$的极大化而言是常数的项，最终转化为求下式：<br>$$<br>\begin{split}<br>&amp;\arg\mathop{\max} \limits_{\theta}\lgroup\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})} \rgroup \\<br>=&amp;\arg\mathop{\max} \limits_{\theta}\lgroup \sum_ZP(Z|Y,\theta^{(i)})logP(Y,Z|\theta) \rgroup \\<br>=&amp;\arg\mathop{\max} \limits_{\theta}\lgroup \sum_ZG(Z)logP(Y,Z|\theta) \rgroup\\<br>=&amp;\arg\mathop{\max} \limits_{\theta}\lgroup Q(\theta,\theta^{(i)}) \rgroup<br>\end{split}<br>\tag{14}<br>$$<br>可以看出，$Q(\theta,\theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y,\theta^{(i)}]$，即最终转化为求完全数据的对数似然函数$logP(Y,Z|\theta)$关于在给定观测数据Y和当前参数$\theta^{(i)}$下对未观测数据Z的条件概率分布$P(Z|Y,\theta^{(i)})$的期望并且求其极大化。</p>
<p>这就完成了EM算法的一次迭代。由此可以得出EM算法的步骤：</p>
<ol>
<li><p>选择参数的初值$\theta^{(0)}$，开始迭代；</p>
</li>
<li><p>E步：记$\theta^{(i)}$为第i次迭代参数$\theta$的估计值，在第i+1次迭代的E步，计算$G(Z)=P(Z|Y,\theta^{(i)})$，并代入(14)式得到<br>$$<br>Q(\theta,\theta^{(i)})=\sum_ZG(Z)logP(Y,Z|\theta)\tag{15}<br>$$</p>
</li>
<li><p>M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$，确定第i+1次迭代的参数估计值$\theta^{(i+1)}$<br>$$<br>\theta^{(i+1)}=\arg\mathop{\max} \limits_{\theta}\lgroup Q(\theta,\theta^{(i)}) \rgroup\tag{16}<br>$$</p>
</li>
<li><p>重复第2步和第3步，直到收敛。</p>
</li>
</ol>
<p>由此可以看出，EM算法是<strong>通过不断求解下界的极大化逼近求解对数似然函数极大化</strong>的算法。</p>
<p>下图可以看出EM算法的直观解释：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/EM/20171007230641_2s4o7k_WechatIMG232.jpeg" alt=""></p>
<p>从图可以推断出EM算法不能保证找到全局最优值。事实上，它只能保证找到稳定点。因此在应用中，初值的选择变得非常重要，常用的方法是选择几个不同的初值进行迭代，然后对得到的各个估值加以比较，从中选择最好的。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>《统计学习方法》 -李航</li>
<li><a href="http://blog.csdn.net/liu1194397014/article/details/52766760" target="_blank" rel="external">极大似然估计与贝叶斯估计 - Jim_Liu - CSDN博客</a></li>
<li><a href="https://www.zhihu.com/question/27976634" target="_blank" rel="external">怎么通俗易懂地解释EM算法并且举个例子? - 知乎</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文介绍了机器学习领域经典算法之一的EM算法，也被称为数据挖掘十大算法之一。EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望（expectation）；M步，求极大（maximization）。所以这一算法称为&lt;strong&gt;期望极大算法（expectation maximization algorithm）&lt;/strong&gt;，简称为EM算法。&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘" scheme="http://zangbo.me/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="数据挖掘十大算法" scheme="http://zangbo.me/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8D%81%E5%A4%A7%E7%AE%97%E6%B3%95/"/>
    
      <category term="半监督学习" scheme="http://zangbo.me/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率模型" scheme="http://zangbo.me/tags/%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>用ffmpeg无损分割和合并视频</title>
    <link href="http://zangbo.me/2017/09/27/CutConcatVideo/"/>
    <id>http://zangbo.me/2017/09/27/CutConcatVideo/</id>
    <published>2017-09-27T11:31:12.000Z</published>
    <updated>2017-10-07T08:18:46.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了用ffmpeg无损分割和合并视频的方法。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="分割视频"><a href="#分割视频" class="headerlink" title="分割视频"></a>分割视频</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ffmpeg -ss **START_TIME** -i input.mp4 -vcodec copy -acodec copy -t **DURATION_TIME** output.mp4</div></pre></td></tr></table></figure>
<p>其中 <strong>START_TIME/DURATION_TIME</strong> 分别表示开始时间和持续时间。它们可以写成两种格式：</p>
<ol>
<li>以秒为单位技术：80</li>
<li>时:分:秒： 00:01:20</li>
</ol>
<p><br></p>
<h1 id="合并视频"><a href="#合并视频" class="headerlink" title="合并视频"></a>合并视频</h1><p>首先新建一个文档命名为“list.txt”，把想要合并的视频片段用以下格式写进去：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">file &apos;/path/../video1.mp4&apos;</div><div class="line">file &apos;/path/../video2.mp4&apos;</div><div class="line">file &apos;/path/../video3.mp4&apos;</div></pre></td></tr></table></figure>
<p>其中，引号内为要合并视频的地址。接着打开命令行，输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ffmpeg -f concat -i list.txt -c copy output.mp4</div></pre></td></tr></table></figure>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文记录了用ffmpeg无损分割和合并视频的方法。&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="http://zangbo.me/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="视频处理" scheme="http://zangbo.me/tags/%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai 笔记（五）</title>
    <link href="http://zangbo.me/2017/09/08/DeepLearning_ai_5/"/>
    <id>http://zangbo.me/2017/09/08/DeepLearning_ai_5/</id>
    <published>2017-09-08T06:18:37.000Z</published>
    <updated>2017-09-12T12:49:06.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本笔记是课程《Structuring Machine Learning Projects》的学习笔记，该课程介绍了在进行机器学习项目时需要注意的一些点，包括正交化方法，误差分析，迁移学习，多任务学习和端到端(end to end)系统。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h1><p>正交化方法指的是在修改算法的一部分时不会对另一部分造成影响。其中一个反例是early stopping，当我们使用early stopping来改善验证集误差时，会对训练集误差产生影响，因此我们一般选择其他的避免过拟合的方式，比如正则化方法。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912203529_414qSz_15.jpeg" alt=""></p>
<p>注意：如果在验证集表现好，在测试集表现差，那需要增加验证集的大小。</p>
<p><br></p>
<h1 id="贝叶斯误差"><a href="#贝叶斯误差" class="headerlink" title="贝叶斯误差"></a>贝叶斯误差</h1><p>贝叶斯误差被认为是能达到的极限误差，我们再怎么努力也无法超越。</p>
<p>有时候会把人类水平误差认为是贝叶斯误差，然后把训练集误差和人类水平误差之间的差距称为可避免偏差，把训练集误差和验证集误差之间的差距称为方差，重点减少训练集误差还是验证集误差，取决于可避免偏差和误差谁更大一些。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912204122_g3l9XM_16.jpeg" alt=""></p>
<ol>
<li>A例子表明应重点减少训练误差</li>
<li>B例子表明应重点减少验证误差</li>
</ol>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912204206_aLGNLg_18.jpeg" alt=""></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912204206_8PoLQw_17.jpeg" alt=""></p>
<p><br></p>
<h1 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h1><p>找出验证集中错误分类的样本，对它们分类错误的原因进行分类统计，找出错误最多的原因进行重点改善。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912204339_UjMb9A_19.jpeg" alt=""></p>
<p>验证集和测试集分布必须一样，它俩和训练集分布可以不同。可以先用庞大的数据来做训练数据，例如高清网络猫图，然后用目标图像做验证和测试数据，例如手机上传的猫图。</p>
<p>验证集和测试集误差有很大差距的话，说明对验证集过拟合了，需要增大验证集。</p>
<p>这种情况下，我们可以从训练集再分出来一小部分座位训练-验证集，衡量它的误差。</p>
<ol>
<li>人类水平误差</li>
<li>训练集误差</li>
<li>训练-验证集误差</li>
<li>验证集误差</li>
<li>测试集误差</li>
</ol>
<p><br></p>
<p>1-2之间差距表明可避免偏差bias。</p>
<p>2-3之间差距表明方差variance</p>
<p>3-4之间差距表明数据不匹配差距</p>
<p>4-5之间差距表明验证集过拟合程度</p>
<p><br></p>
<p>做人工误差分析，看看训练集和验证集的样本差别，然后尽量让训练集像验证集。</p>
<p><br></p>
<h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>迁移学习适合目标任务训练数据集较少的情况。</p>
<p>任务A迁移到B上，需要A和B有相同的输入，例如都是图像，或者都是音频。</p>
<p>A的数据量要远大于B。</p>
<p>A的低层次特征可以帮助B。</p>
<p>可以A数据集上先训练好网络，然后把最后一层或者几层参数重新初始化，最后用B数据集进行重新训练。如果B数据集较大也可以把网络所有层都重新训练。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912204339_3NLqmv_20.jpeg" alt=""></p>
<p><br></p>
<h1 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h1><p>例如多标签分类。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912204339_yxUXTh_21.jpeg" alt=""></p>
<p>也可以应对标签有部分缺省的情况，这样在j加的时候只加有标注的类别的损失。</p>
<p>在物体检测领域，自动驾驶领域多任务学习用的更多。大部分时候迁移学习用的更多。</p>
<p>多个任务的底层特征对彼此都有帮助的前提下，可以用多任务学习。</p>
<p>每个任务的数据量相差不大的情况下可以用多任务学习，效果会更好些。</p>
<p>如果能训练一个足够大的神经网络，那么多任务学习的效果不会比训练多个神经网络效果差。</p>
<p><br></p>
<h1 id="端到端系统"><a href="#端到端系统" class="headerlink" title="端到端系统"></a>端到端系统</h1><p>端到端系统可能会需要更庞大的数据量，它省去了人为设计的很多环节，数据量庞大的情况下效果会特别好。</p>
<p>然而如果没有庞大的数据量，可以选择人为的设计，人为的设计系统可以把人类对问题的很多认识直接注入系统中。</p>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本笔记是课程《Structuring Machine Learning Projects》的学习笔记，该课程介绍了在进行机器学习项目时需要注意的一些点，包括正交化方法，误差分析，迁移学习，多任务学习和端到端(end to end)系统。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="deeplearning.ai" scheme="http://zangbo.me/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai 笔记（四）</title>
    <link href="http://zangbo.me/2017/09/06/DeepLearning_ai_4/"/>
    <id>http://zangbo.me/2017/09/06/DeepLearning_ai_4/</id>
    <published>2017-09-06T10:27:37.000Z</published>
    <updated>2017-09-12T12:27:28.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第三周学习笔记，主要内容为超参数的选择策略，Batch normalization(BN层)和softmax函数。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h1><p>超参数选择中，学习率是最重要的。可以采用随机撒点的策略，也可以又粗到细搜索的策略。</p>
<p> 学习率的随机取值，可以采用对数坐标的随机取值：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912201338_hEMvHU_11.jpeg" alt=""></p>
<p>指数加权平均的超参数beta随机取值，可以对1-beta进行随机取值，然后采用对数坐标的方式：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912201338_wwbVkR_10.jpeg" alt=""><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912201338_t5wkc2_9.jpeg" alt=""></p>
<p>在[-3, -1]之间随机取值。</p>
<p>超参数搜索有两个方式：</p>
<ol>
<li>当设备资源有限时，可以采用“熊猫”的方式，只训练一个模型，然后一遍又一遍的逐步改善，逐步调试。</li>
<li>当设备资源充足时，可以采用“鱼子酱”的方式，同时训练多个模型，然后选择其中效果较好的超参数。</li>
</ol>
<p><br></p>
<h1 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h1><p>每次只是针对一个mini-batch来计算。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912201338_23ljBT_12.jpeg" alt=""></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912201338_ulJssJ_13.jpeg" alt=""></p>
<p>训练算法会把隐层神经元的均值和方差设置成某一固定值。一般是对Z进行BN处理。</p>
<p>使用BN时，参数b通常没用，一般效果会被$\beta$取代，可以省去或者设置为0。</p>
<p>$\beta$和$\gamma$并非超参数，而是可以被训练的参数，它们的size等于Z。</p>
<p>BN层减少了后层神经元输入分布的改变程度，使得它们更加稳定。前层改变。迫使后层适应的程度减小了，它减少了前后层的联系程度，使得网络每层都可以自己学习，有助于加速整个网络的学习。</p>
<p>BN层也有轻微的正则化效果，因为每一个Mini-batch的均值和标准差都是带有一定的噪音的，因此会使得隐层神经元不会过分依赖前面任何一个神经元输出。因为噪音比较小，所以是轻微的正则化效果，而应用较大的batch size可以减少正则化效果，因为噪声减少了。可以把BN和dropout同时使用。</p>
<p>把BN当做一个加速的方式，不要当作正则化的方式，正则化是附带的作用。BN还可以有助于训练更深的神经网络。</p>
<p>在测试时，在所有的Mini-batch上对均值和标准差进行指数加权平均，最后用平均后的均值和标准差，结合训练后得到的$\beta$和$\gamma$，来进行测试。</p>
<p><br></p>
<h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912201338_Oq2TL1_14.jpeg" alt=""></p>
<p>softmax梯度反向传播时，dZl = y^ - y，和logistics回归相同，但和logistics回归维度不同。反向传播时的梯度计算如下（y为标签）：<br>$$<br>dZ^{[l]}=\hat{y}-y<br>$$<br><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第三周学习笔记，主要内容为超参数的选择策略，Batch normalization(BN层)和softmax函数。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="deeplearning.ai" scheme="http://zangbo.me/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai 笔记（三）</title>
    <link href="http://zangbo.me/2017/09/05/DeepLearning_ai_3/"/>
    <id>http://zangbo.me/2017/09/05/DeepLearning_ai_3/</id>
    <published>2017-09-05T09:27:37.000Z</published>
    <updated>2017-09-12T12:08:53.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第二周学习笔记，主要内容为三种不同的梯度下降且重点介绍了Mini-batch gradient descent，指数加权平均，指数加权平均中的偏差修正，动量(Momentum)，RMSprop，Adam算法以及神经网络中的局部最优问题。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><ol>
<li>Batch gradient descent：每次训练迭代处理所有的训练样本，速度慢。</li>
<li>Mini-batch gradient descent：每次训练迭代尺寸为Mini-batch size的一部分样本。</li>
<li>stochastic gradient descent：每次训练迭代单个样本，效率低。</li>
</ol>
<p><br></p>
<p>把整个训练集遍历一遍称为一个epoch。</p>
<p>sgd和Mini-batch gd永远不会收敛，只会在最小值附近波动。</p>
<p>一般Mini-batch 大小为64-512，2的次方速度更快，且需要考虑CPU/GPU内存，这是个超参数。</p>
<p> <br></p>
<p>用Mini-batch gd一般分两步：</p>
<ol>
<li>打乱顺序</li>
<li>分成大小为Mini-batch size的数据集列表。</li>
</ol>
<p><br></p>
<h1 id="指数加权移动平均"><a href="#指数加权移动平均" class="headerlink" title="指数加权移动平均"></a>指数加权移动平均</h1><p>指数加权移动平均，也叫指数加权平均，公式如下。若$\beta =0.9$，相当于今天的加权温度等于昨天温度的0.9倍加上今天温度的0.1倍，也就相当于平均了过去10天($\frac{1}{1-0.9}$)的温度作为今天的加权温度。$\beta$越小，曲线越延后，也越平缓。<br>$$<br>v_t = \beta v_{t-1}+(1-\beta)\theta_t<br>$$<br>指数加权平均，如果权值为0.9，则$0.9^{10}$约等于1/e，也就是大概原值的约1/3，小于这个值我们就认为贡献不是很大。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912195258_bdJdyB_2.jpeg" alt=""></p>
<p><br></p>
<h2 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h2><p>一般计算时令$v_0=0$，固在初期计算值比较低，可以用$\frac{v_t}{1-\beta^t}$代替$v_t$，修正初期数值，而后期作用就不是这么大了。也有人不用偏差修正，直接熬过初期的一些计算。</p>
<p><br></p>
<h1 id="动量-Momentum"><a href="#动量-Momentum" class="headerlink" title="动量(Momentum)"></a>动量(Momentum)</h1><p>每次迭代，在当前mini-batch下计算：<br>$$<br>\begin{cases}<br>v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \\<br>W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}<br>\end{cases}<br>$$</p>
<p>$$<br>\begin{cases}<br>v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \\<br>b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}<br>\end{cases}<br>$$</p>
<p>$\beta$是超参数，最常用的是0.9，可以加速梯度下降，加快训练速度。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912195530_K7bia8_4.jpeg" alt=""></p>
<p>从蓝色变为红色，公式加号前面相当于速度，后面的梯度项相当于加速度，相当于增加了一个动量，可以使用更大的学习率。</p>
<p><br></p>
<h1 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h1><p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912195554_tMgYh7_5.jpeg" alt=""></p>
<p>假设水平方向为W参数，垂直方向为b参数。可以看出b方向梯度较大，$db^2$也就更大，$S_{db}$也就更大，那么最后b的更新步长也就更小，因此在垂直方向上更新步长会减小，水平方向相对更大，所以更新曲线从蓝色变为了绿色，加快了学习速度。</p>
<p>可以使用更大的学习率，$\epsilon$一般为了防止分母变为0，一般设置$10^{-8}$。</p>
<p><br> </p>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam算法结合了Momentum和RMSprop。<br>$$<br>\begin{cases}<br>v_{W^{[l]}} = \beta_1 v_{W^{[l]}} + (1 - \beta_1) \frac{\partial J }{ \partial W^{[l]} } \\<br>v^{corrected}_{W^{[l]}} = \frac{v_{W^{[l]}}}{1 - (\beta_1)^t} \\<br>s_{W^{[l]}} = \beta_2 s_{W^{[l]}} + (1 - \beta_2) (\frac{\partial J }{\partial W^{[l]} })^2 \\<br>s^{corrected}_{W^{[l]}} = \frac{s_{W^{[l]}}}{1 - (\beta_2)^t} \\<br>W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{W^{[l]}}}{\sqrt{s^{corrected}_{W^{[l]}}}+\varepsilon}<br>\end{cases}<br>$$<br><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912200303_I5mLOR_6.jpeg" alt=""></p>
<p>超参数设置建议：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912200306_F5SYHx_7.jpeg" alt=""></p>
<p>平时一般只需要把后三者超参数调整成这三个值即可，然后主要调整学习率超参数。</p>
<p><br></p>
<h1 id="局部最优问题"><a href="#局部最优问题" class="headerlink" title="局部最优问题"></a>局部最优问题</h1><p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170912200415_T0cfp3_8.jpeg" alt=""></p>
<p>局部最优问题更像是一个马鞍形的问题。</p>
<p>我们不太可能被困在一个很差的局部最优点上，如果我们训练一个较大的神经网络，存在较多参数，并且成本函数被定义在较高的维度空间。</p>
<p>平稳区域是一个很大的问题，因为在这个区域参数会更新的很慢，这时Momentum、RMSprop和Adam等算法就起到了作用。</p>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第二周学习笔记，主要内容为三种不同的梯度下降且重点介绍了Mini-batch gradient descent，指数加权平均，指数加权平均中的偏差修正，动量(Momentum)，RMSprop，Adam算法以及神经网络中的局部最优问题。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="deeplearning.ai" scheme="http://zangbo.me/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai 笔记（二）</title>
    <link href="http://zangbo.me/2017/09/04/DeepLearning_ai_2/"/>
    <id>http://zangbo.me/2017/09/04/DeepLearning_ai_2/</id>
    <published>2017-09-04T13:21:33.000Z</published>
    <updated>2017-09-12T12:53:33.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第一周学习笔记，主要内容是数据集划分，偏差和方差，解决过拟合问题的正则化方法（包括L2正则化和dropout），Data augmentation和early stopping，加速训练的方法像归一化输入，Xavier初始化权值方法，以及确保反向传播的梯度检验。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h1><p>数据集一般分为训练数据(trainingdata)、验证数据(development data)和测试数据(test data)。</p>
<p>传统机器学习算法的分割方式一般为60%、20%、20%。</p>
<p>大数据时代不需要特别多的验证数据和测试数据，如果有100万数据，那么可能训练数据有98万，验证数据和测试数据各1万，还可能更少各5000。</p>
<p>验证数据的作用是选择更好的算法，可以理解为确定超参数，测试数据用来无偏估计算法的性能。如果不需要无偏估计算法性能，有时候也可以不需要测试数据，直接用验证数据来当测试数据用，这种情况下有的人也把它称为测试数据，最合适的叫法是<strong>训练-验证集</strong>。</p>
<p>验证集和测试集的分布要相同，即来源要相同，它们两者和训练集有时为了更快捷的获得数据可以分布不同。</p>
<p><br></p>
<h1 id="偏差-amp-方差"><a href="#偏差-amp-方差" class="headerlink" title="偏差&amp;方差"></a>偏差&amp;方差</h1><p>偏差(bias)和方差(variance)，训练集损失一般用来衡量偏差，验证集损失一般用来衡量方差。基础损失(base error)一般表示一个可以达到的最低损失，比如人眼观测的损失。</p>
<p><br></p>
<p>举个例子：假如有个判断是否为猫的图片的数据集，人眼错误率为0。</p>
<p>训练集损失低(1%)，验证集损失高(15%)，过拟合，高方差。</p>
<p>训练集损失高(15%)，验证集损失高(16%)，两者相差不多且明显高于基础损失(0)，欠拟合，高偏差。</p>
<p>训练集损失高(15%)，验证集损失更高(30%)，且明显高于基础损失(0)，高偏差，高方差。</p>
<p>训练集损失低(0.5%)，验证集损失低(1%)，且和基础损失(0)差不多，低偏差，低方差。</p>
<p> <br></p>
<p>在训练一个网络时，首先要做的是观察偏差，先把偏差降到最低。如果偏差过高，那么使用更大规模的神经网络，一般这种做法是有用的。也可以训练更长时间，这方法有时候有用。</p>
<p>一旦偏差降低到可接受程度，再看方差，通过查看验证集损失。若方差高，可以使用更多的数据，一般是有用的。如果不能得到更多数据，可以使用正则化的方法。当然也可以更换神经网络，通过找到合适的神经网络实现一箭双雕的目的。</p>
<p>重复以上过程直到实现低方差和低偏差。</p>
<p> <br></p>
<p>在机器学习初期，通常方差和偏差互相影响。而在深度学习时期，通常使用一个更大规模的网络配合正则化，可以在不影响方差的基础上降低偏差。而增加更多的的数据量，可以在不影响偏差的基础上降低方差。</p>
<p><br></p>
<h1 id="降低方差-amp-减小过拟合"><a href="#降低方差-amp-减小过拟合" class="headerlink" title="降低方差&amp;减小过拟合"></a>降低方差&amp;减小过拟合</h1><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>正则化方法分为多种，用的比较多的是L2正则化和dropout。</p>
<p><br></p>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>使用L2正则化的交叉熵损失：<br>$$<br>J{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{L}\right) + (1-y^{(i)})\log\left(1- a^{L}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W{k,j}^{[l]2} }_\text{L2 regularization cost}<br>$$<br>正则化用于减少方差，一般用L2正则化而不用L1，因为L1正则化倾向于把W变得很稀疏，会有很多0。L2正则化在代价函数后面加上$\frac{\lambda}{2m} ||W||^2$，因为<code>lambda</code>是python的保留字，所以我们用<code>lambd</code>表示正则化系数，这是个超参数。在训练时，在dW项后面加一个$\frac{\lambda}{m}W$，即正则化项总是倾向于使得W减少，相当于乘了一个小于1的权重衰减系数（weightdecay)。<br>$$<br>\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W<br>$$<br>关于正则化为什么有用，其中一种说法是，当使用正则化时，整个网络会被迫使得一部分参数项变小，进而使得原本复杂的网络变的简单，相当于变相的把复杂的神经网络简单化，也就减小了过拟合的程度。</p>
<p><br></p>
<p>总结：</p>
<ol>
<li>代价函数要添加正则化项</li>
<li>反向传播梯度dW要添加额外的项$\frac{\lambda}{m}W$</li>
<li>正则化使得参数更倾向于取较小的值</li>
</ol>
<p><br></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>dropout本身是一种正则化方法，可以理解为，神经网络不会把高权值放在任何一个神经元节点上，因为每一个节点都有可能失活，于是它会倾向于利用每一个神经元把它们权值压缩的很小，某种意义上和L2正则化相似。</p>
<p>如果某一层输出为a，则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">d= np.random.rand(a.shape[<span class="number">0</span>], a.shape[<span class="number">1</span>]) &lt; keep_prob</div><div class="line">a =np.multiply(a, d)</div><div class="line">a = a / keep_prob <span class="comment">#为了不影响a的期望值，添加这一步，使得测试阶段不需要调整数值范围。</span></div></pre></td></tr></table></figure>
<p>我们只需要在训练阶段使用dropout，在测试阶段不需要使用。</p>
<p>我们可以在参数矩阵W较大的层上使用较小的keep_prob，一般不对输入层使用dropout，一般只在计算机视觉领域使用dropout，因为通常没有足够的数据，dropout仅仅用于解决过拟合问题。</p>
<p>dropout缺点是代价函数J不再被明确定义。梯度下降不容易复查，通常我们会关闭dropout，确保J单调递减，再打开dropout。</p>
<p><br></p>
<p>实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#正向传播：</span></div><div class="line"><span class="comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span></div><div class="line">D1 =np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])</div><div class="line"><span class="comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span></div><div class="line">D1 = D1 &lt; keep_prob  </div><div class="line"><span class="comment"># Step 3: shut down some neurons of A1</span></div><div class="line">A1 = A1 * D1       </div><div class="line"><span class="comment"># Step 4: scalethe value of neurons that haven't been shut down</span></div><div class="line">A1 = A1 / keep_prob                             </div><div class="line">  </div><div class="line"><span class="comment">#反向传播</span></div><div class="line"><span class="comment"># Step 1: Apply mask D1 to shutdown the same neurons as during the forward propagation</span></div><div class="line">dA1 = dA1 * D1              </div><div class="line"><span class="comment"># Step 2: Scale the value ofneurons that haven't been shut down</span></div><div class="line">dA1 = dA1 / keep_prob</div></pre></td></tr></table></figure>
<p><br></p>
<p>总结：</p>
<ol>
<li>dropout是一种正则化方法</li>
<li>只能在训练过程中使用dropout，不能在测试时使用</li>
<li>在前向传播和反向传播中都使用dropout</li>
<li>在dropout时，因为dropout过程会使得输出值变为原本的keep_prob倍，固把输出A除以keep_prob的值，可以使得激活函数的输出得到期盼的值。</li>
</ol>
<p><br></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>不同的正则化方法得到的实验结果：</p>
<table>
<thead>
<tr>
<th>model</th>
<th>train accuracy</th>
<th>test accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN without regularization</td>
<td>95%</td>
<td>91.5%</td>
</tr>
<tr>
<td>3-layer NN with L2-regularization</td>
<td>94%</td>
<td>93%</td>
</tr>
<tr>
<td>3-layer NN with dropout</td>
<td>93%</td>
<td>95%</td>
</tr>
</tbody>
</table>
<p>正则化方法会降低训练准确率，但是会有利于测试集准确率。</p>
<p>结论：</p>
<ol>
<li>正则化有利于避免过拟合</li>
<li>正则化会使得参数倾向于更小的值</li>
<li>L2正则化和Dropout是两种很有效的正则化方法。</li>
</ol>
<p><br></p>
<h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><p>解决过拟合问题，如果不能增加数据量，还可以对图片水平翻转，随机旋转，随机裁剪放大后的图片，称为data augmentation。</p>
<p><br></p>
<h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p>另一个解决过拟合的方法是early stopping。验证集误差不再下降则停止训练。它和L2正则化可以二选一。L2正则化需要更大的计算代价，需要找最优的正则化系数。early stopping缺点是不能独立的解决训练数据最优化和防止过拟合两个问题，我们一般倾向于用独立的方法解决单一问题，所以一般不用early stopping。</p>
<p><br></p>
<h1 id="加速训练"><a href="#加速训练" class="headerlink" title="加速训练"></a>加速训练</h1><h2 id="归一化输入数据"><a href="#归一化输入数据" class="headerlink" title="归一化输入数据"></a>归一化输入数据</h2><p>其中一个加速训练的方法是归一化输入。第一步零均值化，第二步归一化方差。如果用某均值和方差来归一化训练集，那么应该用同样的均值和方差来归一化测试集。我们在两个数据集上要使用同样的数据转换。</p>
<p> 归一化的目的是使得输入数据的各个维度的特征具有相似的范围，这样更有利于梯度下降算法的实施，我们也就可以用相对来说更大的学习率，训练速度就会变的更快。比如如果x1范围是[0, 1000]，x2范围是[0, 1]，优化起来就会变的特别慢。<br><br></p>
<h2 id="Xavier初始化权值"><a href="#Xavier初始化权值" class="headerlink" title="Xavier初始化权值"></a>Xavier初始化权值</h2><p>梯度消失和梯度爆炸，梯度值将伴随着层数呈指数增加或衰减，目前的解决方法之一是使用Xavier方法初始化权值，这也是加快训练速度的技巧之一。</p>
<p>Xavier初始化指的是当激活函数为relu时，<code>W = np.random.randn(…) * np.sqrt(2/n[l-1])</code>，使得W的方差变为2/n，这样最终的输出a依然是可以接受的方差。如果激活函数是tanh，则为<code>W = np.random.randn(…) *np.sqrt(1/n[l-1])</code>，把W方差变为1/n。</p>
<p><br></p>
<p>参数初始化时需要注意的点：</p>
<ol>
<li>不同的初始化方式导致不同的结果，较差的方法会导致梯度爆炸或梯度消失，</li>
<li>随机初始化用来打破对称性，使得不同的隐层神经元学到不同的东西。</li>
<li>不要把权值初始化太大，会减慢训练速度。</li>
<li>He初始化方法对于ReLU网络效果最好，即<code>W = np.random.randn(...) * np.sqrt(2/layers_dims[l-1])</code></li>
</ol>
<table>
<thead>
<tr>
<th>Model</th>
<th>Train accuracy</th>
<th>Problem/Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN with zeros initialization</td>
<td>50%</td>
<td>fails to break symmetry</td>
</tr>
<tr>
<td>3-layer NN with large random initialization</td>
<td>83%</td>
<td>too large weights</td>
</tr>
<tr>
<td>3-layer NN with He initialization</td>
<td>99%</td>
<td>recommended method</td>
</tr>
</tbody>
</table>
<p> <br></p>
<h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>梯度检验用于检查反向传播是否正确实施，做梯度检验时，使用双边误差比单边误差更准确。不要再训练过程中使用梯度检验，它只用来debug。 </p>
<p>梯度检验时，把所有的参数矩阵W转换成一个一维向量，然后把所有的转换后的W一维向量以及一维的b参数连接成一个大的一维向量$\theta$。对于dW和db做同样的对应处理，得到$d\theta$。然后对$J(\theta)$进行双边误差计算(公式4)，得到$d{\theta}approx$，然后计算$d{\theta}approx$和$d\theta$的距离(公式5) 。</p>
<p>$$<br>\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}<br>$$</p>
<p>$$<br>difference = \frac {| grad - gradapprox |_2}{| grad |_2 + | gradapprox |_2 }<br>$$</p>
<p>梯度检验时要加上正则化项，但梯度检验不能和dropout同时使用，进行梯度检验时要关掉dropout。 </p>
<p><br>  </p>
<p>总结：</p>
<ol>
<li>如果$\epsilon=1e-7$，则最后的difference应小于等于1e-7，这样说明梯度下降没问题。</li>
<li>梯度检查速度很慢，因此不要在训练过程中执行梯度检查，只需要在验证梯度下降是否正确时使用。</li>
<li>梯度检查不能再dropout时使用，应先关闭dropout，使用梯度检查确保梯度下降正确，再使用dropout进行训练。</li>
</ol>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本笔记是课程《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》的第一周学习笔记，主要内容是数据集划分，偏差和方差，解决过拟合问题的正则化方法（包括L2正则化和dropout），Data augmentation和early stopping，加速训练的方法像归一化输入，Xavier初始化权值方法，以及确保反向传播的梯度检验。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="deeplearning.ai" scheme="http://zangbo.me/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai 笔记（一）</title>
    <link href="http://zangbo.me/2017/09/03/DeepLearning_ai_1/"/>
    <id>http://zangbo.me/2017/09/03/DeepLearning_ai_1/</id>
    <published>2017-09-03T04:39:44.000Z</published>
    <updated>2017-09-12T11:22:39.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>最近对Andrew Ng（吴恩达）新开的课程进行了学习，该系列课程共五门课，我首先学习了第一门课《Neural Networks and Deep Learning》，本课程较为基础，适合查缺补漏。观看过程中整理了一系列笔记，以供以后回顾使用。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<ol>
<li>训练样本分为结构化数据和非结构化数据，后者包括语音、图像、自然语言等抽象的数据；前者的每一个特征都有特定含义，如预测房价问题中的住房面积特征等，结构化数据在数据挖掘领域用的更多。</li>
<li>传统机器学习算法相比于深度学习算法，不能很好的处理大数据。</li>
<li>loss function：适用于计算单个样本。</li>
<li>cost function：表示参数总的代价。</li>
<li>logistics regression，损失函数不能使用平方差函数来表示，这是个非凸优化问题，可以使用交叉熵损失，变为凸优化问题。</li>
<li>应避免任何显式的for循环，能用向量计算就用向量，实在无法用向量的，比如迭代次数，可以使用for循环。</li>
<li>尽量避免使用一维数组，用<code>np.random.randn(1, 5)</code>而不是<code>np.random.randn(5)</code>。</li>
<li>学会用<code>assert(a.shape == (1,5))</code>来确保代码正确，减少bug。</li>
<li>训练数据和测试数据X一般以列为单位进行堆积，形成输入矩阵，如果单个样本的维度为n_x，样本数目为m，则X的维度为(n_x, m)。</li>
<li>输出预测值的维度为(1, m)。</li>
<li>前向传播的线性运算<code>Z = WX + b</code>，python代码为<code>Z = np.dot(W, X) + b</code>。</li>
<li>如果当前的层数为l，则当前的参数矩阵W的维度为(n_l, n_l-1)，其中n_l为当前层的神经元个数，n_l-1为前一层的神经元个数。</li>
<li>如果当前层数位l，则当前的偏执矩阵b的维度为(n_l, 1)，在进行线性运算时，b会进行广播(broadcasting)，先把自己复制m次变成维度为(n_l, m)，再进行相加运算。</li>
<li>dW和W的维度相同，db和b维度相同，Z和A维度相同。</li>
<li>前向传播线性运算后会进行激活函数运算，<code>A = g(Z)</code>。</li>
<li>激活函数使用tanh(Z)总比sigmoid(Z)效果要好，因为前者输出均值为0，更便于下一层的学习。输出层如果是二元分类的话，应使用sigmoid(Z)。</li>
<li>除了输出层以外，默认的激活函数使用relu函数。</li>
<li>在relu中，有足够多的隐藏单元使得Z大于0，对大多数训练样本速度还是很快的。</li>
<li>如果激活函数<code>A = g(Z) = tanh(Z)</code>，则<code>g&#39;(Z) = 1 - A^2</code>。</li>
<li>如果激活函数<code>A = g(Z) = sigmoid(Z)</code>, 且损失函数为交叉熵损失，则<code>dZ = A - Y</code>，Y为标签矩阵。</li>
<li>参数矩阵W不能全0初始化，因为对称性会使得多个隐藏单元变的没有意义，每个隐藏单元的输出都相同，表示的函数也相同。应该随机初始化为极小的值，通常代码为<code>np.random.randn(n_l, n_l-1) * 0.01</code>。</li>
<li>参数b可以进行全0初始化，通常代码为<code>np.zeros((n_l, 1))</code>。</li>
<li>以上两个代码中，一个是单括号()，一个是双括号(())，容易混淆。</li>
<li>超参数有很多种：学习率、迭代次数、隐藏层数、隐藏层单元数、激活函数、momentum、mini batch size、正则化系数等。</li>
<li>深度学习中，深层网络比浅层网络更有用，可以从两个角度解释。生物学角度，低层网络提取简单特征如边缘特征，后面每一层网络都是对前一层进行组合，高层网络表示抽象特征。电路学角度，用单隐层来实现多隐层的功能往往需要的神经元数目呈指数级增长。</li>
<li>二元分类，最后一层激活函数为sigmoid函数，损失函数为交叉熵损失的情况下，前向传播和反向传播流程如下：</li>
</ol>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning_ai/20170903132108_afZMXl_1.jpeg" alt=""></p>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;最近对Andrew Ng（吴恩达）新开的课程进行了学习，该系列课程共五门课，我首先学习了第一门课《Neural Networks and Deep Learning》，本课程较为基础，适合查缺补漏。观看过程中整理了一系列笔记，以供以后回顾使用。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="deeplearning.ai" scheme="http://zangbo.me/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>午后随感</title>
    <link href="http://zangbo.me/2017/07/26/Essay_1/"/>
    <id>http://zangbo.me/2017/07/26/Essay_1/</id>
    <published>2017-07-26T08:34:40.000Z</published>
    <updated>2017-07-26T14:09:10.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p><img src="http://orwbuystz.bkt.clouddn.com/随笔/20170726212755_GyZWqy_午后随想.jpeg" width="60%"></p>
<p><br></p>
<p>趴在实验室的桌子上睡的迷迷糊糊，窗外的蚕鸣断断续续的传到耳朵里，有那么几个瞬间我以为自己回到了小时候，那时候一家人还在小县城郊区的小院里居住。同样的盛夏午后，同样的蝉鸣不绝于耳，我迷迷糊糊的醒来，院子里我妈在晒被子，三十多度的高温炙烤着大地，老旧的录音机播放着邓丽君的“恰似你的温柔”，我扑在被子上，放肆的闻着上面独特又好闻的“太阳味”。有人说这味道源自于“螨虫”，后被辟谣说其实是棉花的味道，但这已经不重要了。后来的岁月里无数次我晒被子，无数次的在太阳落山时把头埋在软绵绵的尚有余温的被子中，尽情闻着上面的味道，却都不如当年那个下午那么好闻。后来我才明白，味道并没有变，只是我们的日子再也不似当年那么闲暇而缓慢了。</p>
<p><br></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>回忆起以前的事，总是觉得时间在某个片段过的很快，又会在某些片段定格，以至于有时候我误以为自己的记忆是不连续的。大部分的往事被埋藏在脑海深处，却总有那么几个场景被反复唤起，无意间就会闯入你的眼前，让你忘记自己在做什么，随之而来的是一阵恍惚和发呆。</p>
<p><br></p>
<p>最近喜欢听一些老歌，有时候觉得老歌怎么听都比现在的歌有味道，或许正是因为这些老歌本身就寄托着某些回忆吧。喜欢邓丽君可能是因为我的父母，她的歌声在我还不记事的时候就开始陪伴我了。记不清那是几岁的时候，大概在小学时吧，那时候一家人刚离开小县城来到市区，全家挤在一栋破旧的楼里不足三十平米的小房间里，同样迷迷糊糊的醒来，我妈在不远处炒菜，背影有些模糊，白色的烟顺着天花板飘到窗外，录音机刚好放到“小城故事”。那个傍晚，那一幕，伴随着这首歌，长久的定格在我的记忆中，无论如何都挥之不去。</p>
<p><br></p>
<p>每一个小城都有着千千万万的人，每个人又都有着自己的故事。这首歌总会让我联想到某个江南小镇，青瓦石阶，烟雨迷蒙，一把油纸伞掉落在地上，水滴顺着屋檐留下，发出滴滴答答的声音。也或许是在遥远的天边，某个深山中的小城。这些我都不得而知，从小我在鲁西南广袤的内陆平原地区一个普通的小县城长大，没有经历过江南的雨季，没有见过小桥流水人家，没有广袤的大海，没有连绵的山脉，甚至连个丘陵都看不到。只能从音乐中，从文字中，从电视中捕捉到一丝半点，然后交给想象。直到我比较大的时候才第一次在连云港见到大海，在济南见到山。</p>
<p><br></p>
<p>后来我走过大疆南北很多地方，爬过很多山，看过很多次海，也去过很多有名的江南小镇，却很难找回当年在拥挤的小屋子里听邓丽君的“小城故事”感受的那种意境。我想是因为自己越成长，想象力变的越局限了吧。当年和小伙伴人手一把木剑，能编出一整套武侠电视连续剧，我们把自己编进我们构造的庞大而宏伟的世界中，纵横驰骋，策马扬鞭。</p>
<p><br></p>
<p>我们这代人上小学那会不比如今，大多都是高中才有属于自己的手机，而且大多还是从诺基亚开始。那会没有王者荣耀，只有小霸王游戏机，那会没有微信，大家还生活在用空间写日志的生活中。日子过得很慢很慢，初中毕业时大家还分发回忆录一张张的写，QQ昵称和简介还是火星文。我的第一篇日志写于2007年，如今十年过去了，谁能想到北京奥运会已经是十年前的事了。</p>
<p><br></p>
<p>后来我们家搬家，原来住了好多年的楼房被推平，变成一片废墟，或许这个假期我再回去已经盖起别的楼房了。小时候住过的小院前早已不是大片农田，再也看不到麦子和玉米，到处都是水泥和钢筋的痕迹。现代化渐渐把90年代的一切痕迹抹去，连同90后这个群体也被历史的车轮拉扯着裹挟着长大。每个年代都有着属于自己的记忆，记得很小的时候还看过黑白电视，记得小时候大人还在用BB机，记得我的第一款诺基亚手机，记得我们毕业时在每一个回忆录上认真写着一路顺风。</p>
<p><br></p>
<p>我们的手机早已可以保存任意多条短信，却很少再有发短信的人。这个世界在日新月异的变化，只是在闲暇的时候，我依然还是会想起那些闪闪发光的日子，那些时常闯入梦境的回忆，是使我坚持走下去的动力。</p>
<p><br></p>
<p>——2017.07.26</p>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;&lt;img src=&quot;http://orwbuystz.bkt.clouddn.com/随笔/20170726212755_GyZWqy_午后随想.jpeg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;趴在实验室的桌子上睡的迷迷糊糊，窗外的蚕鸣断断续续的传到耳朵里，有那么几个瞬间我以为自己回到了小时候，那时候一家人还在小县城郊区的小院里居住。同样的盛夏午后，同样的蝉鸣不绝于耳，我迷迷糊糊的醒来，院子里我妈在晒被子，三十多度的高温炙烤着大地，老旧的录音机播放着邓丽君的“恰似你的温柔”，我扑在被子上，放肆的闻着上面独特又好闻的“太阳味”。有人说这味道源自于“螨虫”，后被辟谣说其实是棉花的味道，但这已经不重要了。后来的岁月里无数次我晒被子，无数次的在太阳落山时把头埋在软绵绵的尚有余温的被子中，尽情闻着上面的味道，却都不如当年那个下午那么好闻。后来我才明白，味道并没有变，只是我们的日子再也不似当年那么闲暇而缓慢了。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://zangbo.me/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习基础：全连接层</title>
    <link href="http://zangbo.me/2017/07/26/DeepLearningFc/"/>
    <id>http://zangbo.me/2017/07/26/DeepLearningFc/</id>
    <published>2017-07-26T04:16:43.000Z</published>
    <updated>2017-07-26T07:09:26.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>最近补了一下深度学习的基础知识，对全连接层中数据的前向传播和梯度的反向传播有了更深的认识，同时自己用Python实现了一个三层的全连接网络，并将结果可视化出来。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><p>全连接的神经网络也叫<strong>多层感知机(MLP)</strong>，前后两层的任意两个神经元都相连，同层的神经元之间没有连接关系，通过如下线性组合后送入一个激活函数进行非线性运算。<br>$$<br>Zi = \sum_j W{i,~ j} x_j + b_i<br>$$<br>常见的激活函数有<code>Sigmoid</code>、<code>ReLU</code>、双曲正切等，本次试验要实现一个二分类的神经网络，因此采用<code>Sigmoid</code>函数。<br>$$<br>Sigmoid=\frac {1}{1+e^{-z}}<br>$$<br><code>Sigmoid</code>函数图像如下所示，它会把任意范围的数映射到0和1之间，我们可以把它的输出当作二分类的概率，当最后一层的输出大于0.5时我们认为它类别为1，小于0.5时我们认为类别为0。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726123606_MsxRaI_全连接层.jpeg" alt="Sigmoid"></center>

<p>本试验采用的<strong>损失函数(Loss Function)</strong>为最简单的二次损失函数，即：<br>$$<br>Loss(y,t)=\frac{1}{2}(y-t)^2<br>$$<br>其中t表示标签数据，y表示神经网络的输出。</p>
<p><br></p>
<h1 id="实验数据"><a href="#实验数据" class="headerlink" title="实验数据"></a>实验数据</h1><p>本次试验构造一个三层全连接神经网络，第一层为输入层，维度为2；最后一层为输出层，维度为1；隐藏层神经元为超参数，可自由设定。</p>
<p>训练样本共有96个，特征数据为浮点数，标签数据为0或者1，分别表示两类。</p>
<p>训练数据如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">-0.1097</th>
<th style="text-align:center">0.5517</th>
<th style="text-align:center">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.1097</td>
<td style="text-align:center">-0.5517</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">-0.2392</td>
<td style="text-align:center">0.5774</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">0.2392</td>
<td style="text-align:center">-0.5774</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<p>可视化如图：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726130640_GLPFuK_train.jpeg" alt="Train Data" width="500"></center>

<p>测试数据和其相似，只是在坐标上有轻微变化。</p>
<h1 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h1><p>我简单的画了下前向传播和反向传播的流程，为方便起见这里令隐藏层神经元数目为4个，这个值对后续的代码没有影响。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726134924_1EqSco_WechatIMG202.jpeg" alt=""></p>
<p>此处x的每一行都表示一个样本，总的行数表示样本的个数。</p>
<p><br></p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>我们首先来定义全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FC</span>:</span></div><div class="line">    <span class="string">"""Define a fully connected layer"""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, output_dim, lr)</span>:</span></div><div class="line">        self._input_dim = input_dim</div><div class="line">        self._output_dim = output_dim</div><div class="line">        self.lr = lr</div><div class="line">        self.w = np.random.randn(input_dim, output_dim)</div><div class="line">        self.b = np.zeros(output_dim)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(self, z)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        self.y = self._sigmoid(np.dot(x, self.w) + self.b)</div><div class="line">        self.x = x</div><div class="line">        <span class="keyword">return</span> self.y</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, gradient)</span>:</span></div><div class="line">        grad_z = gradient * self.y * (<span class="number">1</span> - self.y)</div><div class="line">        grad_w = np.dot(self.x.T, grad_z)</div><div class="line">        grad_b = grad_z.sum(<span class="number">0</span>)</div><div class="line">        grad_x = np.dot(grad_z, self.w.T)</div><div class="line">        self.w -= grad_w * self.lr</div><div class="line">        self.b -= grad_b * self.lr</div><div class="line">        <span class="keyword">return</span> grad_x</div></pre></td></tr></table></figure>
<p>注意这里反向传播时，对于参数b的偏导<code>grad_b</code>，我们令它等于z的偏导<code>grad_z</code>在第一维度上的求和。在每次前向传播时，我们都把x的值保留下来，用于反向传播。所有的梯度传播完成之后，再进行参数的更新。</p>
<p><br></p>
<p>然后我们定义损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SquareLoss</span>:</span></div><div class="line">    <span class="string">"""Define the loss function"""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, output, label)</span>:</span></div><div class="line">        self.loss = output - label</div><div class="line">        <span class="keyword">return</span> np.sum(self.loss * self.loss) / self.loss.shape[<span class="number">0</span>] / <span class="number">2</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.loss</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, output, label)</span>:</span></div><div class="line">        <span class="keyword">return</span> (np.around(output) == label).sum() / len(label)</div></pre></td></tr></table></figure>
<p><br></p>
<p>接下来我们就可以开始搭建神经网络了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>:</span></div><div class="line">    <span class="string">"""Train the model"""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, output_dim, lr)</span>:</span></div><div class="line">        self.fc1 = FC(input_dim, hidden_dim, lr)</div><div class="line">        self.fc2 = FC(hidden_dim, output_dim, lr)</div><div class="line">        self.loss = SquareLoss()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, train_label, iter)</span>:</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(iter):</div><div class="line">            <span class="comment"># forward step</span></div><div class="line">            out_fc1 = self.fc1.forward(train_data)</div><div class="line">            out_fc2 = self.fc2.forward(out_fc1)</div><div class="line">            out_loss = self.loss.forward(out_fc2, train_label)</div><div class="line">            <span class="comment"># backward step</span></div><div class="line">            loss_grad = self.loss.backward()</div><div class="line">            loss_fc2 = self.fc2.backward(loss_grad)</div><div class="line">            loss_fc1 = self.fc1.backward(loss_fc2)</div><div class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</div><div class="line">                train_accuracy = self.loss.accuracy(out_fc2, train_label)</div><div class="line">                print(<span class="string">"Iter: &#123;0&#125;   Train accuracy: &#123;1&#125;"</span>.format(</div><div class="line">                    i, train_accuracy))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, test_data, test_label)</span>:</span></div><div class="line">        out_fc1 = self.fc1.forward(test_data)</div><div class="line">        out_fc2 = self.fc2.forward(out_fc1)</div><div class="line">        out_loss = self.loss.forward(out_fc2, test_label)</div><div class="line">        accuracy = self.loss.accuracy(out_fc2, test_label)</div><div class="line">        <span class="keyword">return</span> accuracy</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, predict_data)</span>:</span></div><div class="line">        out_fc1 = self.fc1.forward(predict_data)</div><div class="line">        out_fc2 = self.fc2.forward(out_fc1)</div><div class="line">        out_result = np.around(out_fc2)</div><div class="line">        <span class="keyword">return</span> out_result</div></pre></td></tr></table></figure>
<p>我们让每10次训练打印一次训练准确率，最终训练完成后用测试数据进行测试，并打印出测试准确率。</p>
<p><br></p>
<p>下面我们把训练数据和测试数据输入进去开始训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    train_set=np.loadtxt(TRAIN_DATA)</div><div class="line">    test_set=np.loadtxt(TEST_DATA)</div><div class="line">    train_data = train_set[:, :<span class="number">2</span>]</div><div class="line">    train_label = train_set[:, <span class="number">2</span>].reshape((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">    test_data = test_set[:, :<span class="number">2</span>]</div><div class="line">    test_label = test_set[:, <span class="number">2</span>].reshape((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line"></div><div class="line">    net = Net(<span class="number">2</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">0.1</span>)</div><div class="line">    net.train(train_data, train_label, <span class="number">5000</span>)</div><div class="line">    accuracy = net.test(test_data, test_label)</div><div class="line">    print(<span class="string">'Test accuracy: &#123;0&#125;'</span>.format(accuracy))</div><div class="line">    </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<p><br></p>
<p>我们首先定义隐藏层神经元为10个，训练迭代次数为5000次，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">Iter: 0   Train accuracy: 0.6041666666666666</div><div class="line">Iter: 10   Train accuracy: 0.6458333333333334</div><div class="line">Iter: 20   Train accuracy: 0.6458333333333334</div><div class="line">...</div><div class="line">Iter: 650   Train accuracy: 0.6875</div><div class="line">Iter: 660   Train accuracy: 0.6979166666666666</div><div class="line">Iter: 670   Train accuracy: 0.6979166666666666</div><div class="line">...</div><div class="line">Iter: 1530   Train accuracy: 0.9270833333333334</div><div class="line">Iter: 1540   Train accuracy: 0.9270833333333334</div><div class="line">Iter: 1550   Train accuracy: 0.9375</div><div class="line">...</div><div class="line">Iter: 1790   Train accuracy: 0.9791666666666666</div><div class="line">Iter: 1800   Train accuracy: 0.9895833333333334</div><div class="line">Iter: 1810   Train accuracy: 1.0</div><div class="line">Iter: 1820   Train accuracy: 1.0</div><div class="line">...</div><div class="line">Iter: 4980   Train accuracy: 1.0</div><div class="line">Iter: 4990   Train accuracy: 1.0</div><div class="line">Test accuracy: 1.0</div></pre></td></tr></table></figure>
<p>可以看出从1800次迭代之后就已经达到了1的训练准确率，最终的测试数据准确率同样为1。</p>
<p><br></p>
<p>我们添加一个类<code>Draw</code>来画出整个模型的收敛结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Draw</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.x = np.linspace(<span class="number">-5</span>,<span class="number">5</span>,<span class="number">100</span>)</div><div class="line">        self.y = np.linspace(<span class="number">-5</span>,<span class="number">5</span>,<span class="number">100</span>)</div><div class="line">        self.X, self.Y = np.meshgrid(self.x,self.y)</div><div class="line">        self.X_f = self.X.flatten()</div><div class="line">        self.Y_f = self.Y.flatten()</div><div class="line">        self.p = zip(self.X_f, self.Y_f)</div><div class="line">        self.data = list()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.p:</div><div class="line">            self.data.append(list(i))</div><div class="line">        self.data = np.array(self.data)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw2D</span><span class="params">(self, Z)</span>:</span></div><div class="line">        plt.figure()</div><div class="line">        plt.scatter(self.X_f,self.Y_f,c=Z)</div><div class="line">        plt.show()</div><div class="line">        </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    ...</div><div class="line">    draw = Draw()</div><div class="line">    out = net.predict(draw.data)</div><div class="line">    draw.draw2D(out)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<p>输出如下：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726140239_JEbLon_10.jpeg" width="500"></center>

<p>我们把隐藏层神经元改为30，得出的结果：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/deeplearning/20170726140243_R5cuNf_30.jpeg" width="500"></center>

<p>可以看出隐藏层神经元在30的情况下边界相对更加平滑，而且收敛速度也更快。</p>
<p>完整代码和训练数据可在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/FC" target="_blank" rel="external">GitHub</a>下载。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/21525237" target="_blank" rel="external">神经网络-全连接层（1） - 知乎专栏</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21535703" target="_blank" rel="external">神经网络-全连接层（2） - 知乎专栏</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21572419" target="_blank" rel="external">神经网络-全连接层（3） - 知乎专栏</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;最近补了一下深度学习的基础知识，对全连接层中数据的前向传播和梯度的反向传播有了更深的认识，同时自己用Python实现了一个三层的全连接网络，并将结果可视化出来。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十五）：tf.contrib.learn日志和监控</title>
    <link href="http://zangbo.me/2017/07/22/TensorFlow_15/"/>
    <id>http://zangbo.me/2017/07/22/TensorFlow_15/</id>
    <published>2017-07-22T07:50:17.000Z</published>
    <updated>2017-07-23T06:31:06.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文介绍了<code>tf.contrib.learn</code>中的<strong>日志(Logging)</strong>和<strong>监控(Monitoring)</strong>的基础知识。当我们训练一个模型时，实时的追踪和评估训练进度往往是很有用的。在本教程中，我们将学会如何使用<code>TensorFLow</code>的日志功能和监控器<code>Monitor</code>API来监控一个神经网络分类器的训练过程，该分类器对鸾尾花(Iris)数据集进行分类。</p>
<p>该笔记的代码基于我之前的一篇笔记<a href="http://zangbo.me/2017/07/09/TensorFlow_13/">《TensorFlow 笔记（十三）：tf.contrib.learn入门》</a>改进而来，如果大家还没看那篇笔记，可以先去看完再来看这篇。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/get_started/monitors" target="_blank" rel="external">https://www.tensorflow.org/get_started/monitors</a></p>
<p><br></p>
<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><p>如下代码来源于笔记<a href="http://zangbo.me/2017/07/09/TensorFlow_13/">《TensorFlow 笔记（十三）：tf.contrib.learn入门》</a>，我们将在该代码的基础上改进。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># Data sets</span></div><div class="line">IRIS_TRAINING = os.path.join(os.path.dirname(__file__), <span class="string">"iris_training.csv"</span>)</div><div class="line">IRIS_TEST = os.path.join(os.path.dirname(__file__), <span class="string">"iris_test.csv"</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(unused_argv)</span>:</span></div><div class="line">    <span class="comment"># Load datasets.</span></div><div class="line">    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">        filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)</div><div class="line">    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">        filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)</div><div class="line"></div><div class="line">    <span class="comment"># Specify that all features have real-value data</span></div><div class="line">    feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</div><div class="line"></div><div class="line">    <span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></div><div class="line">    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</div><div class="line">                                                hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">                                                n_classes=<span class="number">3</span>,</div><div class="line">                                                model_dir=<span class="string">"/tmp/iris_model"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Fit model.</span></div><div class="line">    classifier.fit(x=training_set.data,</div><div class="line">                   y=training_set.target,</div><div class="line">                   steps=<span class="number">2000</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Evaluate accuracy.</span></div><div class="line">    accuracy_score = classifier.evaluate(x=test_set.data,</div><div class="line">                                         y=test_set.target)[<span class="string">"accuracy"</span>]</div><div class="line">    print(<span class="string">'Accuracy: &#123;0:f&#125;'</span>.format(accuracy_score))</div><div class="line"></div><div class="line">    <span class="comment"># Classify two new flower samples.</span></div><div class="line">    new_samples = np.array(</div><div class="line">        [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>], [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=float)</div><div class="line">    y = list(classifier.predict(new_samples, as_iterable=<span class="keyword">True</span>))</div><div class="line">    print(<span class="string">'Predictions: &#123;&#125;'</span>.format(str(y)))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    tf.app.run()</div></pre></td></tr></table></figure>
<p>下面，我们将在这个代码基础上，一点点更新，最终使得它具有日志和监控的功能。</p>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>上述代码仅仅是实现了一个神经网络分类器的功能，把鸾尾花样本正确分类。但是该代码没有打印任何记录模型训练过程的日志，仅仅展示出<code>print</code>语句中的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Accuracy: 0.933333</div><div class="line">Predictions: [1 2]</div></pre></td></tr></table></figure>
<p>没有任何日志记录，模型的训练就让人感觉像是一个黑箱子，在TensorFlow每次执行梯度下降算法时，我们都不知道里面发生了什么，也不了解模型是否正确收敛，或者确定是否应该提前<strong>停止训练(early stopping)</strong>。</p>
<p>其中一个解决方法是将模型训练分成多个<code>fit</code>来调用，以更小的步数来逐步评估模型的准确性。然而，这种方法在实践中并不推荐，因为这会使得训练过程变得很漫长。幸运的是，<code>tf.contrib.learn</code>提供了另外一个解决方法——<code>Monitor API</code>，我们可以用它在训练过程中打印出训练日志同时评估我们的模型。</p>
<p>在接下来的小节我们将学习如何在TensorFlow中打印日志，如何建立一个<strong>验证监控器(ValidationMonitor)</strong>来做评估，同时在TensorBoard中进行可视化。</p>
<p><br></p>
<h1 id="打印日志"><a href="#打印日志" class="headerlink" title="打印日志"></a>打印日志</h1><p>TensorFlow定义了五种不同层次的日志信息。以升序排列分别为<code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>FATAL</code>。当我们配置了其中一个层次的日志，TensorFlow不仅会输出该层次的日志，同时会输出比它高层次的日志记录。例如，当我们配置日志层次为<code>ERROR</code>，我们将会得到包含<code>ERROR</code>和<code>FATAL</code>的日志信息；当我们设置日志层次为<code>DEBUG</code>，我们将得到全部五种层次的日志信息。</p>
<p>TensorFlow默认的配置层次为<code>WARN</code>，但是当我们追踪模型的训练时，我们最好把它调整为<code>INFO</code>，这会给我们提供训练时<code>fit</code>节点的信息。</p>
<p>在代码的开始阶段(import 之后)添加以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.logging.set_verbosity(tf.logging.INFO)</div></pre></td></tr></table></figure>
<p>现在当我们执行代码，我们会看到如下额外的输出信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:loss = 1.18812, step = 1</div><div class="line">INFO:tensorflow:loss = 0.210323, step = 101</div><div class="line">INFO:tensorflow:loss = 0.109025, step = 201</div></pre></td></tr></table></figure>
<p>当我们用<code>INFO</code>层次的日志时，<code>tf.contirb.learn</code>会默认每100步迭代输出一次训练损失。</p>
<p><br></p>
<h1 id="配置验证监控器进行评估"><a href="#配置验证监控器进行评估" class="headerlink" title="配置验证监控器进行评估"></a>配置验证监控器进行评估</h1><p>打印出来训练模型可以帮助我们了解模型是否收敛，但是如果我们想要进一步的了解训练过程中发生了什么，<code>tf.contrib.learn</code>提供了一些高水平的<code>Monitor</code>，我们可以把它们运用到<code>fit</code>操作中来追踪训练过程或者调试一些低水平的TensorFlow操作。<code>Monitor</code>主要有以下几种：</p>
<table>
<thead>
<tr>
<th>Monitor</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CaptureVariable</code></td>
<td>每N次迭代把一个指定变量值保存到一个收集器中。</td>
</tr>
<tr>
<td><code>PrintTensor</code></td>
<td>每N次迭代打印出来指定<code>tensor</code>的值。</td>
</tr>
<tr>
<td><code>SummarySaver</code></td>
<td>对于一个指定<code>tensor</code>每N次迭代保存一次 <code>tf.Summary</code> <code>protocol buffers</code>，通过使用<a href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter" target="_blank" rel="external"><code>tf.summary.FileWriter</code></a> 指令。</td>
</tr>
<tr>
<td><code>ValidationMonitor</code></td>
<td>每N次迭代打印一次评估结果，还可以选择什么情况下提前停止训练(early stopping)。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="每N次迭代做一次评估"><a href="#每N次迭代做一次评估" class="headerlink" title="每N次迭代做一次评估"></a>每N次迭代做一次评估</h2><p>对于鸾尾花神经网络分类器，当我们打印训练损失时，我们可能还想要同步的在测试数据集上进行评估，进而得知模型的泛化能力。我们可以通过配置<code>ValidationMonitor</code>来实现这个功能，同时还可以设置每多少次迭代评估一次，<code>every_n_steps</code>默认值是100，这里我们设置<code>every_n_steps</code>为50，即让它每50次迭代来对测试集进行一次评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</div><div class="line">    test_set.data,</div><div class="line">    test_set.target,</div><div class="line">    every_n_steps=<span class="number">50</span>)</div></pre></td></tr></table></figure>
<p>把这行代码放在初始化<code>classifier</code>之前。</p>
<p><code>ValidationMonitor</code>依赖于保存的<em>checkpoint</em>文件来进行评估操作，因此我们需要在实例化<code>classifier</code>时添加<code>tf.contirb.learn.RunConfig</code>项，它包含了<code>save_checkpoints_secs</code>值来定义我们每多少秒保存一次<em>checkpoint</em>文件。因为鸾尾花数据集十分小，并且训练的特别快，我们可以把<code>save_checkpoints_secs</code>设置为1，也就是每秒钟保存一次<em>checkpoint</em>文件，这样我们可以确保有足够数量的<em>checkpoint</em>文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">classifier = tf.contrib.learn.DNNClassifier(</div><div class="line">    feature_columns=feature_columns,</div><div class="line">    hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">    n_classes=<span class="number">3</span>,</div><div class="line">    model_dir=<span class="string">"/tmp/iris_model"</span>,</div><div class="line">    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p><strong>注意：</strong><code>model_dir</code>参数定义了一个绝对地址来保存模型数据。每次运行代码时，该路径下的所有现有数据将被加载，并且模型训练将从上次停止的地方继续运行（例如，若训练期间每次<code>fit</code>操作迭代2000次，则连续执行脚本两次将迭代4000次 ）。</p>
<p>最后，我们来把之前定义好的监控器<code>validation_monitor</code>添加进<code>fit</code>操作里，把它赋值给<code>monitor</code>参数，该参数会接受一个包含了所有监控器的列表，并在训练过程中执行它们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">classifier.fit(x=training_set.data,</div><div class="line">               y=training_set.target,</div><div class="line">               steps=<span class="number">2000</span>,</div><div class="line">               monitors=[validation_monitor])</div></pre></td></tr></table></figure>
<p>现在，当我们运行该代码，我们可以看到打印出来的评估结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Validation (step 50): loss = 1.71139, global_step = 0, accuracy = 0.266667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 300): loss = 0.0714158, global_step = 268, accuracy = 0.966667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 1750): loss = 0.0574449, global_step = 1729, accuracy = 0.966667</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="用MetricSpec改进评估指标"><a href="#用MetricSpec改进评估指标" class="headerlink" title="用MetricSpec改进评估指标"></a>用MetricSpec改进评估指标</h2><p>如果我们没有定义评估指标，在验证阶段<code>ValidationMonitor</code>将会默认同时打印出损失值和准确率。但是我们可以更改指标列表，定义我们想要的评估指标，我们可以向<code>ValidationMonitor</code>函数添加<code>metircs</code>参数。 <code>metircs</code>采用字典的键/值对来定义，其中每个键都是我们想要记录的指标的名字，相应的值是<code>MetricSpec</code>对象。</p>
<p><code>MetricSpec</code>对象使用函数<code>tf.contirb.learn.MetricSpec()</code>来定义，它接受四个参数：</p>
<p><br></p>
<ul>
<li><code>metric_fn</code>：函数计算并且返回一组指标的值，可以是<code>tf.contrib.metrics</code>模块中已经预定义好的函数，例如<code>tf.contrib.metrics.streaming_precision</code>或<code>tf.contrib.metrics.streaming_recall</code>。同时我们可以定义我们自己需要的指标函数，这需要把<code>pridictions</code>和<code>labels</code>张量作为参数(<code>weights</code>参数是可选的)。该函数必须返回两种形式的指标值<ul>
<li>一个<code>tensor</code></li>
<li>一组<code>ops(value_op, update_op)</code>，其中<code>value_op</code>返回指标的值，<code>update_op</code>执行一个相关的操作来更新内部模型状态。</li>
</ul>
</li>
</ul>
<p><br></p>
<ul>
<li><code>prediction_key</code>：该<code>tensor</code>包含了模型返回的预测值。如果模型返回单个张量或具有单个条目的字典，则可以省略此参数。对于<code>DNNClassifier</code>模型，类预测将使用关键字<code>tf.contrib.learn.PredictionKey.CLASSES</code>在张量中返回。</li>
</ul>
<p><br></p>
<ul>
<li><code>label_key</code>：该<code>tensor</code>包含了模型返回的标签，它被模型的<code>input_fn</code>函数定义。与<code>prediction_key</code>一样，如果<code>input_fn</code>返回单个张量或具有单个条目的字典，则可以省略此参数。在本教程中，<code>DNNClassifier</code>没有<code>input_fn</code>（x，y数据直接传递给fit），因此不需要提供<code>label_key</code>。</li>
</ul>
<p><br></p>
<ul>
<li><code>weights_key</code>：可选项。<code>tensor</code>（由<code>input_fn</code>返回）包含<code>metric_fn</code>的权重输入。</li>
</ul>
<p><br></p>
<p>下面的代码创建一个<code>validation_metrics</code>字典，定义了三个在模型评估过程中需要打印的指标。</p>
<ul>
<li><code>&quot;accuracy&quot;</code>，使用<code>tf.contrib.metrics.streaming_accuracy</code>作为<code>metric_fn</code></li>
<li><code>&quot;precision&quot;</code>，使用<code>tf.contrib.metrics.streaming_precision</code>作为<code>metric_fn</code></li>
<li><code>&quot;recall&quot;</code>使用<code>tf.contrib.metrics.streaming_recall</code>作为<code>metric_fn</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">validation_metrics = &#123;</div><div class="line">    <span class="string">"accuracy"</span>:</div><div class="line">        tf.contrib.learn.MetricSpec(</div><div class="line">            metric_fn=tf.contrib.metrics.streaming_accuracy,</div><div class="line">            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),</div><div class="line">    <span class="string">"precision"</span>:</div><div class="line">        tf.contrib.learn.MetricSpec(</div><div class="line">            metric_fn=tf.contrib.metrics.streaming_precision,</div><div class="line">            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),</div><div class="line">    <span class="string">"recall"</span>:</div><div class="line">        tf.contrib.learn.MetricSpec(</div><div class="line">            metric_fn=tf.contrib.metrics.streaming_recall,</div><div class="line">            prediction_key=tf.contrib.learn.PredictionKey.CLASSES)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>把上述代码添加到<code>ValidationMonitor</code>的定义之前，然后修改关于<code>ValidationMonitor</code>的定义，添加一个<code>metrics</code>参数来打印准确率，预测值和召回率指标，这些都是之前在<code>validation_metrics</code>中定义过的。（损失值是始终都会被打印出来的，我们不需要明确去定义它）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</div><div class="line">    test_set.data,</div><div class="line">    test_set.target,</div><div class="line">    every_n_steps=<span class="number">50</span>,</div><div class="line">    metrics=validation_metrics)</div></pre></td></tr></table></figure>
<p>执行该代码，我们可以看到预测值和召回率包含在输出中了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Validation (step 50): recall = 0.0, loss = 1.20626, global_step = 1, precision = 0.0, accuracy = 0.266667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 600): recall = 1.0, loss = 0.0530696, global_step = 571, precision = 1.0, accuracy = 0.966667</div><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 1500): recall = 1.0, loss = 0.0617403, global_step = 1452, precision = 1.0, accuracy = 0.966667</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="用验证监控器实现Early-Stopping"><a href="#用验证监控器实现Early-Stopping" class="headerlink" title="用验证监控器实现Early Stopping"></a>用验证监控器实现Early Stopping</h2><p>注意在上述打印的日志中，到第600次迭代时，模型在测试集上已经到达接近于1的准确率和召回率，这种情况下我们可以使用<code>early stopping</code>。</p>
<p>除了打印评估指标，<code>ValidationMonitor</code>实现<code>early stopping</code>也十分的简单。我们可以通过定义以下三个参数来实现：</p>
<table>
<thead>
<tr>
<th>Param</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>early_stopping_metric</code></td>
<td>引发early stopping的指标 (例如loss或accuracy)，触发条件定义在<code>early_stopping_rounds</code>和 <code>early_stopping_metric_minimize</code>中，默认的值是 <code>&quot;loss&quot;</code>。</td>
</tr>
<tr>
<td><code>early_stopping_metric_minimize</code></td>
<td>如果模型是想要最小化<code>early_stopping_metric</code>则为<code>True</code>; 如果模型是想要最大化<code>early_stopping_metric</code>则为<code>False</code>。默认为<code>True</code>。</td>
</tr>
<tr>
<td><code>early_stopping_rounds</code></td>
<td>设置一定的迭代次数，如果在这些次数内<code>early_stopping_metric</code>没有减少(<code>early_stopping_metric_minimize</code>值为<code>True</code>) 或者没有增加(<code>early_stopping_metric_minimize</code>值为<code>False</code>)，训练将会停止。默认值为<code>None</code>，这意味着<code>early stopping</code>不会被触发。</td>
</tr>
</tbody>
</table>
<p>我们对<code>ValidationMonitor</code>的定义做如下修改，当我们的损失持续了200次迭代都没有减小，模型的训练将会停止，将不会执行完<code>fit</code>中定义的2000次迭代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(</div><div class="line">    test_set.data,</div><div class="line">    test_set.target,</div><div class="line">    every_n_steps=<span class="number">50</span>,</div><div class="line">    metrics=validation_metrics,</div><div class="line">    early_stopping_metric=<span class="string">"loss"</span>,</div><div class="line">    early_stopping_metric_minimize=<span class="keyword">True</span>,</div><div class="line">    early_stopping_rounds=<span class="number">200</span>)</div></pre></td></tr></table></figure>
<p>再次执行该代码，我们会看到模型提前停止了训练：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">INFO:tensorflow:Validation (step 1150): recall = 1.0, loss = 0.056436, global_step = 1119, precision = 1.0, accuracy = 0.966667</div><div class="line">INFO:tensorflow:Stopping. Best step: 800 with loss = 0.048313818872.</div></pre></td></tr></table></figure>
<p>训练在第1150次迭代停止，这表明在之前的200次训练迭代中，损失都没有下降，总体来说，800次迭代就已经在测试集达到了最小的损失值。这表明通过减少训练次数可以进一步改进模型，同时改善超参数。</p>
<p><br></p>
<h2 id="TensorBoard可视化日志数据"><a href="#TensorBoard可视化日志数据" class="headerlink" title="TensorBoard可视化日志数据"></a>TensorBoard可视化日志数据</h2><p><code>ValidationMonitor</code>在训练期间生成了大量关于模型性能的原始数据，我们可以可视化它们来进一步了解训练情况——例如准确率是如何随着训练次数改变的。我们可以使用TensorBoard来实现这个功能，地址是我们保存模型数据的地址，打开命令行输入以下代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ tensorboard --logdir=/tmp/iris_model/</div></pre></td></tr></table></figure>
<p>然后打开浏览器在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板。具体的使用方法可以参考我的笔记<a href="http://zangbo.me/2017/06/27/TensorFlow_4/">《TensorFlow 笔记（四）：TensorBoard可视化》</a></p>
<p><br></p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><p>完整代码可以在TensorFlow官方GitHub下载：</p>
<p><a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/monitors/iris_monitors.py" target="_blank" rel="external">iris_monitors.py</a></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/monitors" target="_blank" rel="external">Logging and Monitoring Basics with tf.contrib.learn  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文介绍了&lt;code&gt;tf.contrib.learn&lt;/code&gt;中的&lt;strong&gt;日志(Logging)&lt;/strong&gt;和&lt;strong&gt;监控(Monitoring)&lt;/strong&gt;的基础知识。当我们训练一个模型时，实时的追踪和评估训练进度往往是很有用的。在本教程中，我们将学会如何使用&lt;code&gt;TensorFLow&lt;/code&gt;的日志功能和监控器&lt;code&gt;Monitor&lt;/code&gt;API来监控一个神经网络分类器的训练过程，该分类器对鸾尾花(Iris)数据集进行分类。&lt;/p&gt;
&lt;p&gt;该笔记的代码基于我之前的一篇笔记&lt;a href=&quot;http://zangbo.me/2017/07/09/TensorFlow_13/&quot;&gt;《TensorFlow 笔记（十三）：tf.contrib.learn入门》&lt;/a&gt;改进而来，如果大家还没看那篇笔记，可以先去看完再来看这篇。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十四）：tf.contrib.learn输入函数</title>
    <link href="http://zangbo.me/2017/07/18/TensorFlow_14/"/>
    <id>http://zangbo.me/2017/07/18/TensorFlow_14/</id>
    <published>2017-07-18T11:42:52.000Z</published>
    <updated>2017-07-18T15:19:36.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本教程介绍了<code>tf.contrib.learn</code>中的输入函数，我们将首先学习如何建立一个<code>input_fn</code>来预处理数据并且把它们送入模型中训练、评估或测试。接着我们将运用<code>input_fn</code>构造一个神经网络回归器用来预测房价。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/get_started/input_fn" target="_blank" rel="external">https://www.tensorflow.org/get_started/input_fn</a></p>
<p><br></p>
<h1 id="使用input-fn的输入方法"><a href="#使用input-fn的输入方法" class="headerlink" title="使用input_fn的输入方法"></a>使用input_fn的输入方法</h1><p>当我们用<code>tf.contrib.learn</code>来训练一个神经网络时，我们可以直接把特征数据和标签数据送入<code>fit</code>、<code>evaluate</code>或<code>predict</code>操作中。如下例子是我们上篇教程中用到的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)</div><div class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)</div><div class="line">...</div><div class="line"></div><div class="line">classifier.fit(x=training_set.data,</div><div class="line">               y=training_set.target,</div><div class="line">               steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>当我们对源数据操作较小时，这种方法很好。但如果我们需要大量的特征工程，我们可以使用输入函数<code>input_fn</code>来进行数据预处理的操作。</p>
<p><br></p>
<h2 id="input-fn结构"><a href="#input-fn结构" class="headerlink" title="input_fn结构"></a>input_fn结构</h2><p>如下代码显示了一个输入函数的基本框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 在这里预处理你的数据...</span></div><div class="line">    <span class="comment"># return:1) 特征列与相应特征数据的Tensors的映射 </span></div><div class="line">    <span class="comment">#        2) 包含标签的Tensor</span></div><div class="line">    <span class="keyword">return</span> feature_cols, labels</div></pre></td></tr></table></figure>
<p>函数主体包含了对输入数据的预处理操作，例如清除异常点等。输入函数必须返回如下两个值：</p>
<ul>
<li><code>feature_cols</code>：一个包含了键值对的字典，把特征名映射到对应的<code>Tensor</code>上，该<code>Tensor</code>包含了特征数据。</li>
<li><code>labels</code>：一个包含了标签数据的<code>Tensor</code></li>
</ul>
<p><br></p>
<h2 id="把特征数据转化为Tensors"><a href="#把特征数据转化为Tensors" class="headerlink" title="把特征数据转化为Tensors"></a>把特征数据转化为Tensors</h2><p>如果在<code>input_fn</code>函数中，我们的特征数据或标签数据保存在<code>pandas dataframes</code>或者<code>numpy arrays</code>里面，我们需要把它们转化为<code>Tensor</code>返回。</p>
<p>对于连续数据，我们可以用<code>tf.constant</code>创建一个<code>Tensor</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feature_column_data = [<span class="number">1</span>, <span class="number">2.4</span>, <span class="number">0</span>, <span class="number">9.9</span>, <span class="number">3</span>, <span class="number">120</span>]</div><div class="line">feature_tensor = tf.constant(feature_column_data)</div></pre></td></tr></table></figure>
<p>对于稀疏矩阵数据（大部分值为0），我们可以使用<code>SparseTensor</code>，一般来说我们用三个参数来实例化一个<code>SparseTensor</code>：</p>
<ul>
<li><code>dense_shape</code>：<code>tensor</code>的形状，用一个列表来表示每个维度的元素的个数，例如<code>dense_shape=[3,5]</code>表示一个二维的<code>tensor</code>，有3行5列；而<code>dense_shape=[9]</code>表示一个一维的<code>tensor</code>，有9个元素。</li>
<li><code>indices</code>：表示<code>tensor</code>中非0值的位置。用一个嵌套列表来表示，每个嵌套列表代表一个非0值所在的位置。例如<code>indices=[[0,1], [2,4]]</code>表示在<code>tensor</code>中第0行第1个元素和第2行第4个元素为非零值。</li>
<li><code>values</code>：一维<code>tensor</code>，<code>values</code>中的第i个值位于在<code>indices</code>中第i个值表示的位置上。例如下面的例子，表示在第0行第1个元素为6，第2行第4个元素为0.5。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sparse_tensor = tf.SparseTensor(indices=[[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">4</span>]],</div><div class="line">                                values=[<span class="number">6</span>, <span class="number">0.5</span>],</div><div class="line">                                dense_shape=[<span class="number">3</span>, <span class="number">5</span>])</div></pre></td></tr></table></figure>
<p>对应的<code>tensor</code>为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[[<span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.5</span>]]</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="把input-fn送入模型中"><a href="#把input-fn送入模型中" class="headerlink" title="把input_fn送入模型中"></a>把input_fn送入模型中</h2><p>为了把数据送入模型进行训练，我们只需要把创建的输入函数赋值给<code>fit</code>的<code>input_fn</code>参数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=my_input_fn, steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>注意<code>input_fn</code>会把特征数据和标签数据一起送入模型中，它代替了<code>fit</code>中的<code>x</code>和<code>y</code>参数，如果我们同时保留了<code>input_fn</code>和<code>x</code>或者<code>y</code>，程序会报错。</p>
<p>同时需要注意的是<code>input_fn</code>参数必须接受一个函数名(<code>input_fn=my_input_fn</code>)而不是接受一个函数调用(<code>input_fn=my_input_fn()</code>)。这意味着如果我们尝试在<code>fit</code>函数的参数中使用一个函数调用，如下代码，将会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=my_input_fn(training_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>不过如果我们想让我们的输入函数具有输入参数，第一个方法是用另一个无参数函数包含它，然后用该函数名作为<code>input_fn</code>的值，这样我们就可以在内层函数送入我们需要的参数。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_function_training_set</span><span class="params">()</span>:</span></div><div class="line">	<span class="keyword">return</span> my_input_function(training_set)</div><div class="line"></div><div class="line">classifier.fit(input_fn=my_input_fn_training_set, steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>第二个方法是使用python内置的偏函数<code>functools.partial</code>来实现上述功能，它会建立一个新的函数同时把所有的参数固定住：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=functools.partial(my_input_function,</div><div class="line">                                          data_set=training_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>第三个方法我们可以使用<code>lambda</code>操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=<span class="keyword">lambda</span>: my_input_fn(training_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>上述三个方法最大的优点是我们可以把同一个<code>input_fn</code>送到<code>evaluate</code>和<code>predict</code>操作上，而只需要修改内层函数的输入参数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.evaluate(input_fn=<span class="keyword">lambda</span>: my_input_fn(test_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>这些方法增强了代码的可操作性，我们不需要对每个操作都分别获取<code>x</code>和<code>y</code>的值并用两个变量表示出来，如 <code>x_train</code>, <code>x_test</code>, <code>y_train</code>, <code>y_test</code>。</p>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>关于<code>functools.partial()</code>：</p>
<p>如果某一个函数的其中一个输入参数始终固定，而我们每次调用该函数都要输入一次就显得很没有必要，于是我们就可以对该函数进行封装，使得输入参数减少。用到的函数就是<code>functools.partial()</code>。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input1</span><span class="params">(key1,key2)</span>:</span></div><div class="line">    <span class="keyword">return</span> key1+<span class="string">'_'</span>+key2</div></pre></td></tr></table></figure>
<p>如果我们的<code>key1</code>在使用过程中始终是固定的，而只有<code>key2</code>是变化的，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = input1(<span class="string">'China'</span>,<span class="string">'liming'</span>)</div><div class="line">b = input1(<span class="string">'China'</span>,<span class="string">'zhangsan'</span>)</div><div class="line">c = input1(<span class="string">'China'</span>,<span class="string">'wangwei'</span>)</div><div class="line">...</div></pre></td></tr></table></figure>
<p>这时候我们就可以固定<code>key1</code>的值，对<code>input1()</code>进行封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> functools</div><div class="line">input2 = functools.partial(input1, <span class="string">'China'</span>)</div></pre></td></tr></table></figure>
<p>这样我们就可以很方便的使用该函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = input2(<span class="string">'liming'</span>)</div><div class="line">b = input2(<span class="string">'zhangsan'</span>)</div><div class="line">c = input2(<span class="string">'wangwei'</span>)</div></pre></td></tr></table></figure>
<p>输出<code>a,b,c</code>的值得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">China_liming </div><div class="line">China_zhangsan </div><div class="line">China_wangwei</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="波士顿房价神经网络模型"><a href="#波士顿房价神经网络模型" class="headerlink" title="波士顿房价神经网络模型"></a>波士顿房价神经网络模型</h1><p>接下来我们将携一个输入函数来预处理波士顿房价数据，然后我们把处理后的数据送入神经网络回归器中进行训练，进而预测房价中位数。</p>
<p>我们要用到的数据集的CSV文件包含以下特征：</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>CRIM</td>
<td>Crime rate per capita</td>
</tr>
<tr>
<td>ZN</td>
<td>Fraction of residential land zoned to permit 25,000+ sq ft lots</td>
</tr>
<tr>
<td>INDUS</td>
<td>Fraction of land that is non-retail business</td>
</tr>
<tr>
<td>NOX</td>
<td>Concentration of nitric oxides in parts per 10 million</td>
</tr>
<tr>
<td>RM</td>
<td>Average Rooms per dwelling</td>
</tr>
<tr>
<td>AGE</td>
<td>Fraction of owner-occupied residences built before 1940</td>
</tr>
<tr>
<td>DIS</td>
<td>Distance to Boston-area employment centers</td>
</tr>
<tr>
<td>TAX</td>
<td>Property tax rate per $10,000</td>
</tr>
<tr>
<td>PTRATIO</td>
<td>Student-teacher ratio</td>
</tr>
</tbody>
</table>
<p>模型预测的标签是MEDV，表示房价的中位数，以<strong>千美元</strong>为计数单位。</p>
<p>通过以下链接下载数据集：</p>
<ul>
<li>训练数据：<a href="http://download.tensorflow.org/data/boston_train.csv" target="_blank" rel="external">boston_train.csv</a></li>
<li>测试数据：<a href="http://download.tensorflow.org/data/boston_test.csv" target="_blank" rel="external">boston_test.csv</a></li>
<li>预测数据：<a href="http://download.tensorflow.org/data/boston_predict.csv" target="_blank" rel="external">boston_predict.csv</a></li>
</ul>
<p>接下来我们将依次介绍如何创建一个输入函数，把它们送入神经网络回归器中，训练并评估模型，进行房价预测。</p>
<p><br></p>
<h2 id="引入房价数据"><a href="#引入房价数据" class="headerlink" title="引入房价数据"></a>引入房价数据</h2><p>首先我们需要引入必要的库，包括<code>pandas</code>和<code>tensorflow</code>，同时我们利用<code>set_verbosity</code>来获得更多的输出信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">tf.logging.set_verbosity(tf.logging.INFO)</div></pre></td></tr></table></figure>
<p>关于<code>logging</code>我们将在下个笔记中介绍，简单来说我们在代码开始(<code>import</code>之后)加上这行代码，训练过程将打印出详细的<code>loss</code>信息和<code>step</code>信息，可以更好的监控整个训练过程。</p>
<p>因为我们是使用<code>pandas</code>来倒入CSV数据，所以在此之前先定义好<code>column</code>的名字，以及特征名和标签名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">COLUMNS = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>, <span class="string">"age"</span>,</div><div class="line">           <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>, <span class="string">"medv"</span>]</div><div class="line">FEATURES = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>,</div><div class="line">            <span class="string">"age"</span>, <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>]</div><div class="line">LABEL = <span class="string">"medv"</span></div><div class="line"></div><div class="line">training_set = pd.read_csv(<span class="string">"boston_train.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</div><div class="line">                           skiprows=<span class="number">1</span>, names=COLUMNS)</div><div class="line">test_set = pd.read_csv(<span class="string">"boston_test.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</div><div class="line">                       skiprows=<span class="number">1</span>, names=COLUMNS)</div><div class="line">prediction_set = pd.read_csv(<span class="string">"boston_predict.csv"</span>, skipinitialspace=<span class="keyword">True</span>,</div><div class="line">                             skiprows=<span class="number">1</span>, names=COLUMNS)</div></pre></td></tr></table></figure>
<p>因为第一行是关于数据的介绍，所以我们在这里跳过第一行，用<code>skiprows=1</code>。</p>
<p><br></p>
<h2 id="定义FeatureColumns并创建回归器"><a href="#定义FeatureColumns并创建回归器" class="headerlink" title="定义FeatureColumns并创建回归器"></a>定义FeatureColumns并创建回归器</h2><p>接下来我们创建一系列<code>FeatureColumn</code>来组成特征数据。每个<code>FeatureColumn</code>表示一个特征，根据数据类型的不同用相应的函数创建，更详细内容可以看以下两个参考链接：</p>
<ul>
<li>所有函数类型：<a href="https://www.tensorflow.org/api_guides/python/contrib.layers#Feature_columns" target="_blank" rel="external">Layers (contrib)  |  TensorFlow</a>，</li>
<li>特征是分类数据(categorical data)的例子：<a href="https://www.tensorflow.org/tutorials/wide#base_categorical_feature_columns" target="_blank" rel="external">TensorFlow Linear Model Tutorial  |  TensorFlow</a>。</li>
</ul>
<p>因为本数据集所有的特征都是连续值，所以我们可以用<code>tf.contrib.layers.real_valued_column()</code>函数来创建，函数输入是特征的名字，这和下面的<code>input_fn()</code>函数相对应。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feature_cols = [tf.contrib.layers.real_valued_column(k)</div><div class="line">                  <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES]</div></pre></td></tr></table></figure>
<p>接下来，我们将实例化一个<code>DNNRegressor</code>，我们需要提供两个参数，一个<code>hidden_units</code>，它是用来定义隐藏层神经元个数的超参数，这里我们用两层各10个神经元；另一个是<code>feature_columns</code>，即我们刚刚定义的<code>FeatureColumns</code>的列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,</div><div class="line">                                          hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</div><div class="line">                                          model_dir=<span class="string">"/tmp/boston_model"</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="建立input-fn"><a href="#建立input-fn" class="headerlink" title="建立input_fn"></a>建立input_fn</h2><p>为了把输入数据送入<code>regressor</code>中，我们要创建一个输入函数，它接受一个<code>pandas</code>的<code>Datafram</code>同时返回包含特征信息和标签信息的两个<code>Tensor</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(data_set)</span>:</span></div><div class="line">  feature_cols = &#123;k: tf.constant(data_set[k].values)</div><div class="line">                  <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES&#125;</div><div class="line">  labels = tf.constant(data_set[LABEL].values)</div><div class="line">  <span class="keyword">return</span> feature_cols, labels</div></pre></td></tr></table></figure>
<p>注意<code>data_set[k].values</code>将返回一个<code>numpy array</code>，<code>feature_cols</code>是一个字典。<code>input_fn()</code>函数将对<code>data_set</code>进行处理，这意味着我们可以把任何我们需要的<code>DataFrame</code>送入该函数中，包括<code>training_set</code>、<code>test_set</code>和 <code>prediction_set</code>。</p>
<p><br></p>
<h2 id="训练回归器"><a href="#训练回归器" class="headerlink" title="训练回归器"></a>训练回归器</h2><p>训练神经网络回归器，我们只需要执行<code>fit</code>方法，然后把<code>training_set</code>送入<code>input_fn</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">regressor.fit(input_fn=<span class="keyword">lambda</span>: input_fn(training_set), steps=<span class="number">5000</span>)</div></pre></td></tr></table></figure>
<p>我们将会看到打印出的训练信息，每100步迭代打印一次训练损失：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Step 1: loss = 483.179</div><div class="line">INFO:tensorflow:Step 101: loss = 81.2072</div><div class="line">INFO:tensorflow:Step 201: loss = 72.4354</div><div class="line">...</div><div class="line">INFO:tensorflow:Step 1801: loss = 33.4454</div><div class="line">INFO:tensorflow:Step 1901: loss = 32.3397</div><div class="line">INFO:tensorflow:Step 2001: loss = 32.0053</div><div class="line">INFO:tensorflow:Step 4801: loss = 27.2791</div><div class="line">INFO:tensorflow:Step 4901: loss = 27.2251</div><div class="line">INFO:tensorflow:Saving checkpoints for 5000 into /tmp/boston_model/model.ckpt.</div><div class="line">INFO:tensorflow:Loss for final step: 27.1674.</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>借下来我们在测试集上评估训练好的模型，执行<code>evaluate</code>指令，这次把<code>test_set</code>送入<code>input_fn</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ev = regressor.evaluate(input_fn=<span class="keyword">lambda</span>: input_fn(test_set), steps=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>打印出损失的确切值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">loss_score = ev[<span class="string">"loss"</span>]</div><div class="line">print(<span class="string">"Loss: &#123;0:f&#125;"</span>.format(loss_score))</div></pre></td></tr></table></figure>
<p>可以看到以下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:Eval steps [0,1) for training step 5000.</div><div class="line">INFO:tensorflow:Saving evaluation summary for 5000 step: loss = 11.9221</div><div class="line">Loss: 11.922098</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="进行预测"><a href="#进行预测" class="headerlink" title="进行预测"></a>进行预测</h2><p>最后，我们可以用训练好的模型来进行房价的预测，这里我们使用的数据集是<code>prediction_set</code>，共有6个样本，包含了特征数据但是没有包含标签数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">y = regressor.predict(input_fn=<span class="keyword">lambda</span>: input_fn(prediction_set))</div><div class="line"><span class="comment"># .predict() returns an iterator; convert to a list and print predictions</span></div><div class="line">predictions = list(itertools.islice(y, <span class="number">6</span>))</div><div class="line"><span class="keyword">print</span> (<span class="string">"Predictions: &#123;&#125;"</span>.format(str(predictions)))</div></pre></td></tr></table></figure>
<p>结果输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Predictions: [ 33.30348587  17.04452896  22.56370163  34.74345398  14.55953979  19.58005714]</div></pre></td></tr></table></figure>
<p>结果给出了六个样本的预测房价。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/input_fn" target="_blank" rel="external">Building Input Functions with tf.contrib.learn  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/handsomekang/article/details/9712125" target="_blank" rel="external">飘逸的python - 偏函数functools.partial - mattkang - CSDN博客</a></li>
<li><a href="https://www.tensorflow.org/get_started/monitors#enabling_logging_with_tensorflow" target="_blank" rel="external">Logging and Monitoring Basics with tf.contrib.learn  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本教程介绍了&lt;code&gt;tf.contrib.learn&lt;/code&gt;中的输入函数，我们将首先学习如何建立一个&lt;code&gt;input_fn&lt;/code&gt;来预处理数据并且把它们送入模型中训练、评估或测试。接着我们将运用&lt;code&gt;input_fn&lt;/code&gt;构造一个神经网络回归器用来预测房价。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十三）：tf.contrib.learn入门</title>
    <link href="http://zangbo.me/2017/07/09/TensorFlow_13/"/>
    <id>http://zangbo.me/2017/07/09/TensorFlow_13/</id>
    <published>2017-07-09T05:32:36.000Z</published>
    <updated>2017-07-18T11:41:20.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>TensorFlow有很多高级的机器学习API，<code>tf.contrib.learn</code>就是其中之一。<code>tf.contrib.learn</code>的出现让机器学习模型的建立、训练和评估变得十分的简单。本文介绍了用<code>tf.contrib.learn</code>搭建神经网络分类器并对Iris数据集进行分类，我们的代码分为以下五步：</p>
<ol>
<li>载入包含Iris训练数据和测试数据的CSV文件并保存为<code>Dataset</code>格式</li>
<li>构建神经网络分类器</li>
<li>用训练数据训练模型</li>
<li>评估模型的准确率</li>
<li>对新的样本进行分类</li>
</ol>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/get_started/tflearn" target="_blank" rel="external">https://www.tensorflow.org/get_started/tflearn</a></p>
<p><br></p>
<h1 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h1><p>Iris数据集是一个关于鸢尾花的数据集，样式如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Sepal Length</th>
<th style="text-align:center">Sepal Width</th>
<th style="text-align:center">Petal Length</th>
<th style="text-align:center">Petal Width</th>
<th style="text-align:center">Species</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">5.1</td>
<td style="text-align:center">3.5</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">4.9</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">4.7</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">1.3</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">7.0</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">4.7</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">6.4</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">4.5</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">6.9</td>
<td style="text-align:center">3.1</td>
<td style="text-align:center">4.9</td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">6.5</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">5.2</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">6.2</td>
<td style="text-align:center">3.4</td>
<td style="text-align:center">5.4</td>
<td style="text-align:center">2.3</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">5.9</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">5.1</td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">2</td>
</tr>
</tbody>
</table>
<p>数据集包含了150行数据，代表着150个样本，共有3个类别，每个类别有50个样本，具体类别如下（分别用0、1、2表示）：</p>
<ul>
<li>setosa</li>
<li>versicolor</li>
<li>virginica</li>
</ul>
<p>数据集共有四个特征（浮点数）：</p>
<ul>
<li>Sepal length</li>
<li>Sepal width</li>
<li>Petal length</li>
<li>Petal width</li>
</ul>
<p>我们要利用花萼的长度和宽度，花瓣的长度和宽度这四个特征来预测花的类别。</p>
<p><br></p>
<h1 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h1><p>对于本教程，Iris数据集已经被随机排序并且分割成了两个CSV文件：</p>
<ul>
<li>训练集包含120个样本，文件名<code>iris_training.csv</code></li>
<li>测试集包含30个样本，文件名<code>iris_test.csv</code></li>
</ul>
<p>我们只需要从TensorFlow官网下载这两个文件即可，其中<code>iris_training.csv</code>文件如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">120</th>
<th style="text-align:center">4</th>
<th style="text-align:center">setosa</th>
<th style="text-align:center">versicolor</th>
<th style="text-align:center">virginica</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">6.4</td>
<td style="text-align:center">2.8</td>
<td style="text-align:center">5.6</td>
<td style="text-align:center">2.2</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">5.0</td>
<td style="text-align:center">2.3</td>
<td style="text-align:center">3.3</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<p>第一行为头信息，分别描述了训练数据的样本数、特征数以及种类名，接下来的120行为训练数据。前四列为四种特征，最后一列为所属类别。<code>iris_test.csv</code>与此类似。</p>
<p>代码开始阶段，我们首先要导入必要的模块，并且定义我们下载并存储数据集的地址：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> urllib</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">IRIS_TRAINING = <span class="string">"iris_training.csv"</span></div><div class="line">IRIS_TRAINING_URL = <span class="string">"http://download.tensorflow.org/data/iris_training.csv"</span></div><div class="line"></div><div class="line">IRIS_TEST = <span class="string">"iris_test.csv"</span></div><div class="line">IRIS_TEST_URL = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></div></pre></td></tr></table></figure>
<p>接下来判断目标地址有没有该数据集，如果没有则需要下载数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TRAINING):</div><div class="line">  raw = urllib.urlopen(IRIS_TRAINING_URL).read()</div><div class="line">  <span class="keyword">with</span> open(IRIS_TRAINING,<span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(raw)</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TEST):</div><div class="line">  raw = urllib.urlopen(IRIS_TEST_URL).read()</div><div class="line">  <span class="keyword">with</span> open(IRIS_TEST,<span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(raw)</div></pre></td></tr></table></figure>
<p>最后使用<code>load_csv_with_header()</code>函数载入训练和测试数据到<code>Dataset</code>中，该函数在<code>tf.contrib.learn.datasets.base</code>类中，有三个必须的参数：</p>
<ul>
<li><code>filename</code>：表示CSV文件的路径。</li>
<li><code>target_dtype</code>：表示数据集label的<code>numpy</code>类型。</li>
<li><code>features_dtype</code>：表示数据集特征的<code>numpy</code>类型。</li>
</ul>
<p>在这里，我们的label共有三类，用0-2表示，因此它的<code>numpy</code>类型为<code>np.int</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load datasets.</span></div><div class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TRAINING,</div><div class="line">    target_dtype=np.int,</div><div class="line">    features_dtype=np.float32)</div><div class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TEST,</div><div class="line">    target_dtype=np.int,</div><div class="line">    features_dtype=np.float32)</div></pre></td></tr></table></figure>
<p>在<code>tf.contrib.learn</code>中<code>Dataset</code>是命名元组(named tuples)，我们可以通过<code>training_set.data</code>和 <code>training_set.target</code>命令非常方便的获得训练集的特征数据和目标标签数据。同样的，<code>test_set.data</code>和<code>test_set.target</code>分别包含了测试集的特征数据和目标标签数据。 </p>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>1、关于<code>load_csv_with_header()</code>函数的定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">Dataset = collections.namedtuple(<span class="string">'Dataset'</span>, [<span class="string">'data'</span>, <span class="string">'target'</span>])</div><div class="line"><span class="comment">#...</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_csv_with_header</span><span class="params">(filename,</span></span></div><div class="line">                         target_dtype,</div><div class="line">                         features_dtype,</div><div class="line">                         target_column=<span class="number">-1</span>):</div><div class="line">	<span class="string">"""Load dataset from CSV file with a header row."""</span></div><div class="line">	<span class="keyword">with</span> gfile.Open(filename) <span class="keyword">as</span> csv_file:</div><div class="line">		data_file = csv.reader(csv_file)</div><div class="line">    	header = next(data_file) <span class="comment"># 返回第一行数据</span></div><div class="line">        <span class="comment"># 此处data_file可以理解为一个迭代器</span></div><div class="line">        <span class="comment"># 该行代码可以替换为: header = data_file.__next__()</span></div><div class="line">    	n_samples = int(header[<span class="number">0</span>]) <span class="comment"># 返回样本数目</span></div><div class="line">    	n_features = int(header[<span class="number">1</span>]) <span class="comment"># 返回特征数目</span></div><div class="line">    	data = np.zeros((n_samples, n_features), dtype=features_dtype)</div><div class="line">    	target = np.zeros((n_samples,), dtype=target_dtype)</div><div class="line">    	<span class="keyword">for</span> i, row <span class="keyword">in</span> enumerate(data_file): </div><div class="line">    	<span class="comment"># enumerate()函数可以同时遍历索引和元素</span></div><div class="line">        <span class="comment"># 因为data_file为迭代器，上面用过一次next，此处则从第二行开始</span></div><div class="line">      		target[i] = np.asarray(row.pop(target_column), dtype=target_dtype)</div><div class="line">      		data[i] = np.asarray(row, dtype=features_dtype)</div><div class="line"></div><div class="line">	<span class="keyword">return</span> Dataset(data=data, target=target)</div></pre></td></tr></table></figure>
<p>关于生成器的相关概念，可以看下面这个教程：</p>
<ul>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/" target="_blank" rel="external">Python yield 使用浅析</a></li>
</ul>
<p><br></p>
<p>2、关于命名元组：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">collections.namedtuple(typename, </div><div class="line">                       field_names, </div><div class="line">                       verbose=<span class="keyword">False</span>, </div><div class="line">                       rename=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<blockquote>
<p>返回一个名为typenname的元组。参数field_names是一个字符串表示的元素名称，每个字段之间可以通过空格、逗号方式来分隔，比如’x y’，’x, y’。另外也可以采用列表的方式，比如[‘x’, ‘y’]。在字段名称命名上需要注意的是每个字段必须是有效的Python标识符，同时不能是python关键字，另外不要以下划线或数字开头。</p>
<p>如果参数rename为True就会自动地把不合法名称转换为相应合法的名称，比如：<code>[&#39;abc&#39;, &#39;def&#39;, &#39;ghi&#39;, &#39;abc&#39;]</code>转换为<code>[&#39;abc&#39;, &#39;_1&#39;, &#39;ghi&#39;, &#39;_3&#39;]</code>，在这里把def转换<code>_1</code>，同时把重复的abc转换<code>_3</code>。</p>
</blockquote>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#python 3.4</span></div><div class="line"><span class="keyword">import</span> collections</div><div class="line">Point = collections.namedtuple(<span class="string">'Point'</span>, <span class="string">'x, y, z'</span>)</div><div class="line">p1 = Point(<span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>)</div><div class="line">print(p1)</div><div class="line"><span class="comment"># Point(x=30, y=40, z=50)</span></div><div class="line"></div><div class="line">print(p1[<span class="number">0</span>] + p1[<span class="number">1</span>] + p1[<span class="number">2</span>])</div><div class="line"><span class="comment"># 120</span></div><div class="line"></div><div class="line">x, y, z = p1</div><div class="line"></div><div class="line">print(x, y, z)</div><div class="line"><span class="comment"># 30 40 50</span></div><div class="line"></div><div class="line">print(p1.x, p1.y, p1.z)</div><div class="line"><span class="comment"># 30 40 50</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="构建深度神经网络分类器"><a href="#构建深度神经网络分类器" class="headerlink" title="构建深度神经网络分类器"></a>构建深度神经网络分类器</h1><p><code>tf.contrib.learn</code>提供了多种预定义好的模型，我们叫它<code>Estimators</code>，我们可以把它当作黑箱子来训练和评估我们的数据。在这里，我们将搭建一个深度神经网络分类器来训练我们的Iris数据。在使用<code>tf.contrib.learn</code>之前，我们首先要用简短的代码实例化<code>tf.contrib.learn.DNNClassifier</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Specify that all features have real-value data</span></div><div class="line">feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</div><div class="line"></div><div class="line"><span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></div><div class="line">classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</div><div class="line">                                            hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">                                            n_classes=<span class="number">3</span>,</div><div class="line">                                            model_dir=<span class="string">"/tmp/iris_model"</span>)</div></pre></td></tr></table></figure>
<p>上面代码的第一部分定义了特征列(feature columns)，它定义了数据集的数据类型。因为所有的特征数据是连续的，因此我们用<code>tf.contrib.layers.real_valued_column()</code>函数来构建特征列。在数据集中有四种特征(sepal width, sepal height, petal width, and petal height)，因此我们设置<code>dimension=4</code>。</p>
<p>然后，代码用以下参数创建了一个<code>DNNClassifier</code>模型：</p>
<ul>
<li><code>feature_columns=feature_columns</code>，上面提到的特征列的设置。</li>
<li><code>hidden_units=[10, 20, 10]</code>，三个隐藏层，分别包含10、20和10个神经元</li>
<li><code>n_classes=3</code>，目标种类数，共三个类别</li>
<li><code>model_dir=/tmp/iris_model</code>，在训练过程中TensorFlow保存<em>checkpoint</em>数据的路径。</li>
</ul>
<p><br></p>
<h1 id="输入训练数据"><a href="#输入训练数据" class="headerlink" title="输入训练数据"></a>输入训练数据</h1><p><code>tf.contrib.learn</code>的API使用的是输入函数，它们会创建TensorFlow的<code>ops</code>然后生成数据。在本例中，因为数据量足够小因此我们直接把它们存在<code>TensorFlow constants</code>中。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define the training inputs</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_inputs</span><span class="params">()</span>:</span></div><div class="line">  x = tf.constant(training_set.data)</div><div class="line">  y = tf.constant(training_set.target)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> x, y</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>我们已经构建好模型名为<code>classifier</code>，现在我们可以用<code>fit</code>方法来训练Iris数据，通过调用<code>get_train_inputs</code>函数给参数<code>input_fn</code>来传入训练数据，同时设置迭代次数。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Fit model.</span></div><div class="line">classifier.fit(input_fn=get_train_inputs, steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>模型的状态将会保存在<code>classifier</code>中，这意味着我们可以反复的训练。上面的代码等同于如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">classifier.fit(x=training_set.data, y=training_set.target, steps=<span class="number">1000</span>)</div><div class="line">classifier.fit(x=training_set.data, y=training_set.target, steps=<span class="number">1000</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>我们已经在Iris数据集上训练好了一个<code>DNNClassifier</code>模型<code>classifier</code>，现在我们要在Iris测试集上评估模型的准确率。用到的函数为<code>evaluate</code>。诸如<code>fit</code>和<code>evaluate</code>这种函数需要传入一个输入函数来建立输入管道。<code>evaluate</code>函数会返回一个包含着评估结果的<code>dict</code>字典。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define the test inputs</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_inputs</span><span class="params">()</span>:</span></div><div class="line">  x = tf.constant(test_set.data)</div><div class="line">  y = tf.constant(test_set.target)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> x, y</div><div class="line"></div><div class="line"><span class="comment"># Evaluate accuracy.</span></div><div class="line">accuracy_score = classifier.evaluate(input_fn=get_test_inputs,</div><div class="line">                                     steps=<span class="number">1</span>)[<span class="string">"accuracy"</span>]</div><div class="line"></div><div class="line">print(<span class="string">"\nTest Accuracy: &#123;0:f&#125;\n"</span>.format(accuracy_score))</div></pre></td></tr></table></figure>
<p>我们会得到以下输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Test Accuracy: 0.966667</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h2><p>1、<code>evaluate</code>函数输出的为一个<code>dict</code>字典，字典如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'accuracy'</span>: <span class="number">0.93333334</span>, <span class="string">'global_step'</span>: <span class="number">2000</span>, <span class="string">'loss'</span>: <span class="number">0.068770193</span>&#125;</div></pre></td></tr></table></figure>
<p>其中<code>global_step</code>为训练迭代的次数。代码中我们直接令<code>accuracy_score</code>等于其中的<code>accuracy</code>的值。</p>
<p><br></p>
<p>2、python格式化输出<code>print(&quot;\nTest Accuracy: {0:f}\n&quot;.format(accuracy_score))</code></p>
<p>其中的0表示后面第0个数，f表示数据类型为<code>float</code>型。</p>
<p><br></p>
<h1 id="分类新样本"><a href="#分类新样本" class="headerlink" title="分类新样本"></a>分类新样本</h1><p>使用<code>classifier</code>的<code>predict()</code>函数可以对新的样本进行分类。例如我们有如下两个新的鸾尾花的样本：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Sepal Length</th>
<th style="text-align:center">Sepal Width</th>
<th style="text-align:center">Petal Length</th>
<th style="text-align:center">Petal Width</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">6.4</td>
<td style="text-align:center">3.2</td>
<td style="text-align:center">4.5</td>
<td style="text-align:center">1.5</td>
</tr>
<tr>
<td style="text-align:center">5.8</td>
<td style="text-align:center">3.1</td>
<td style="text-align:center">5.0</td>
<td style="text-align:center">1.7</td>
</tr>
</tbody>
</table>
<p>我们可以用<code>predict()</code>函数来预测它们的种类。<code>predict</code>会返回一个生成器，我们可以把它很方便的转化为列表(list)。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Classify two new flower samples.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_samples</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">return</span> np.array(</div><div class="line">    [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>],</div><div class="line">     [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=np.float32)</div><div class="line"></div><div class="line">predictions = list(classifier.predict(input_fn=new_samples))</div><div class="line"></div><div class="line">print(</div><div class="line">    <span class="string">"New Samples, Class Predictions:    &#123;&#125;\n"</span></div><div class="line">    .format(predictions))</div></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">New Samples, Class Predictions:    [1 2]</div></pre></td></tr></table></figure>
<p> 结果表明第一个样本种类为<em>Iris versicolor</em>，第二个样本种类为<em>Iris virginica</em>。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/get_started/tflearn" target="_blank" rel="external">tf.contrib.learn Quickstart  |  TensorFlow</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/learn/python/learn/datasets/base.py" target="_blank" rel="external">load_csv_with_header()</a></li>
<li><a href="http://blog.csdn.net/caimouse/article/details/50493926" target="_blank" rel="external">5.3.5 namedtuple() 创建命名字段的元组结构 - 大坡3D软件开发 - CSDN博客</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier" target="_blank" rel="external">tf.contrib.learn.DNNClassifier  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;TensorFlow有很多高级的机器学习API，&lt;code&gt;tf.contrib.learn&lt;/code&gt;就是其中之一。&lt;code&gt;tf.contrib.learn&lt;/code&gt;的出现让机器学习模型的建立、训练和评估变得十分的简单。本文介绍了用&lt;code&gt;tf.contrib.learn&lt;/code&gt;搭建神经网络分类器并对Iris数据集进行分类，我们的代码分为以下五步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;载入包含Iris训练数据和测试数据的CSV文件并保存为&lt;code&gt;Dataset&lt;/code&gt;格式&lt;/li&gt;
&lt;li&gt;构建神经网络分类器&lt;/li&gt;
&lt;li&gt;用训练数据训练模型&lt;/li&gt;
&lt;li&gt;评估模型的准确率&lt;/li&gt;
&lt;li&gt;对新的样本进行分类&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十二）：CNN示例代码CIFAR-10分析（下）</title>
    <link href="http://zangbo.me/2017/07/07/TensorFlow_12/"/>
    <id>http://zangbo.me/2017/07/07/TensorFlow_12/</id>
    <published>2017-07-07T07:48:47.000Z</published>
    <updated>2017-07-08T06:29:28.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文接上文，继续学习TensorFlow在CIFAR-10上的教程，该代码主要由以下五部分组成：</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py" target="_blank" rel="external"><code>cifar10_input.py</code></a></td>
<td>读取原始的 CIFAR-10 二进制格式文件</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py" target="_blank" rel="external"><code>cifar10.py</code></a></td>
<td>建立 CIFAR-10 网络模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py" target="_blank" rel="external"><code>cifar10_train.py</code></a></td>
<td>在单块CPU或者GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py" target="_blank" rel="external"><code>cifar10_multi_gpu_train.py</code></a></td>
<td>在多块GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py" target="_blank" rel="external"><code>cifar10_eval.py</code></a></td>
<td>在测试集上评估 CIFAR-10 模型的表现</td>
</tr>
</tbody>
</table>
<p>本次主要学习<code>cifar10_train.py</code>和<code>cifar10_eval.py</code>两个文件，内容分别为训练模型和评估模型，并最终给出实验过程。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">https://www.tensorflow.org/tutorials/deep_cnn</a></p>
<p>代码地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>
<p><br></p>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>这部分代码在<code>cifar10_train.py</code>文件中，实现了用单块GPU训练模型，具体训练过程设计为：</p>
<ul>
<li>共计100万次迭代（自己实验时改成了10万次）</li>
<li>batch_size为128</li>
<li>每10次迭代打印一次训练数据（损失、样本/秒、秒/batch）</li>
<li>每600s保存一次<em>checkpoint</em>文件</li>
<li>每300s对最新的<em>checkpoint</em>文件执行一次评估</li>
<li>每100次迭代保存一次summary</li>
</ul>
<p><br></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""用单块GPU训练CIFAR-10"""</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10</div><div class="line"></div><div class="line"><span class="comment">#作用类似于argparse，通过命令行传参改变训练参数</span></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'train_dir'</span>, <span class="string">'/tmp/cifar10_train'</span>,</div><div class="line">                           <span class="string">"""Directory where to write event logs """</span></div><div class="line">                           <span class="string">"""and checkpoint."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'max_steps'</span>, <span class="number">1000000</span>,</div><div class="line">                            <span class="string">"""Number of batches to run."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'log_device_placement'</span>, <span class="keyword">False</span>,</div><div class="line">                            <span class="string">"""Whether to log device placement."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'log_frequency'</span>, <span class="number">10</span>,</div><div class="line">                            <span class="string">"""How often to log results to the console."""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""训练 CIFAR-10 数据集."""</span></div><div class="line">  <span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">    <span class="comment"># 返回或创建全局迭代张量（是一个不会被训练的变量）</span></div><div class="line">    global_step = tf.contrib.framework.get_or_create_global_step()</div><div class="line"></div><div class="line">    <span class="comment"># 获得CIFAR-10的训练数据和标签</span></div><div class="line">    <span class="comment"># 强迫输入管道在 CPU:0 上操作避免有时候操作在GPU上会停止并导致运行变慢</span></div><div class="line">    <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</div><div class="line">      images, labels = cifar10.distorted_inputs()</div><div class="line"></div><div class="line">    <span class="comment"># 用模型的接口函数inference()建立Graph并且计算logits</span></div><div class="line">    logits = cifar10.inference(images)</div><div class="line"></div><div class="line">    <span class="comment"># 计算损失</span></div><div class="line">    loss = cifar10.loss(logits, labels)</div><div class="line"></div><div class="line">    <span class="comment"># 建立 Graph 并用一个batch的数据来训练模型并更新参数</span></div><div class="line">    train_op = cifar10.train(loss, global_step)</div><div class="line"></div><div class="line">    <span class="class"><span class="keyword">class</span> <span class="title">_LoggerHook</span><span class="params">(tf.train.SessionRunHook)</span>:</span></div><div class="line">      <span class="string">"""打印损失和运行时间"""</span></div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">begin</span><span class="params">(self)</span>:</span></div><div class="line">        self._step = <span class="number">-1</span></div><div class="line">        self._start_time = time.time()</div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">before_run</span><span class="params">(self, run_context)</span>:</span></div><div class="line">        self._step += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> tf.train.SessionRunArgs(loss)  <span class="comment"># 计算损失</span></div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">after_run</span><span class="params">(self, run_context, run_values)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._step % FLAGS.log_frequency == <span class="number">0</span>:</div><div class="line">          current_time = time.time()</div><div class="line">          duration = current_time - self._start_time</div><div class="line">          self._start_time = current_time</div><div class="line"></div><div class="line">          loss_value = run_values.results</div><div class="line">          <span class="comment"># 计算每秒钟训练了多少个样本</span></div><div class="line">          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration </div><div class="line">          <span class="comment"># 计算每次迭代用了多长时间</span></div><div class="line">          sec_per_batch = float(duration / FLAGS.log_frequency) </div><div class="line"></div><div class="line">          format_str = (<span class="string">'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '</span></div><div class="line">                        <span class="string">'sec/batch)'</span>)</div><div class="line">          <span class="keyword">print</span> (format_str % (datetime.now(), self._step, loss_value,</div><div class="line">                               examples_per_sec, sec_per_batch))</div><div class="line"></div><div class="line">    <span class="comment"># 开启一个会话执行训练过程</span></div><div class="line">    <span class="comment"># tf.train.NanTensorHook(loss)：监控loss，如果loss为NaN则停止训练</span></div><div class="line">    <span class="keyword">with</span> tf.train.MonitoredTrainingSession(</div><div class="line">        checkpoint_dir=FLAGS.train_dir,</div><div class="line">        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),</div><div class="line">               tf.train.NanTensorHook(loss),</div><div class="line">               _LoggerHook()],</div><div class="line">        config=tf.ConfigProto(</div><div class="line">            log_device_placement=FLAGS.log_device_placement)) <span class="keyword">as</span> mon_sess:</div><div class="line">      <span class="keyword">while</span> <span class="keyword">not</span> mon_sess.should_stop(): <span class="comment"># 如果没有到最大迭代次数</span></div><div class="line">        mon_sess.run(train_op) <span class="comment"># 执行训练过程</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span> </div><div class="line">  cifar10.maybe_download_and_extract() <span class="comment"># 下载数据并解压缩</span></div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(FLAGS.train_dir):</div><div class="line">    tf.gfile.DeleteRecursively(FLAGS.train_dir)</div><div class="line">  tf.gfile.MakeDirs(FLAGS.train_dir)</div><div class="line">  train()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  tf.app.run()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">tf.train.MonitoredTrainingSession(</div><div class="line">    master=<span class="string">''</span>,</div><div class="line">    is_chief=<span class="keyword">True</span>,</div><div class="line">    checkpoint_dir=<span class="keyword">None</span>,</div><div class="line">    scaffold=<span class="keyword">None</span>,</div><div class="line">    hooks=<span class="keyword">None</span>,</div><div class="line">    chief_only_hooks=<span class="keyword">None</span>,</div><div class="line">    save_checkpoint_secs=<span class="number">600</span>,</div><div class="line">    save_summaries_steps=<span class="number">100</span>,</div><div class="line">    save_summaries_secs=<span class="keyword">None</span>,</div><div class="line">    config=<span class="keyword">None</span>,</div><div class="line">    stop_grace_period_secs=<span class="number">120</span>,</div><div class="line">    log_step_count_steps=<span class="number">100</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>每600s保存一次<em>checkpoint</em>，每100s保存一次<em>summary</em>。</p>
<p><br></p>
<h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>这部分代码在<code>cifar10_eval.py</code>中，默认每300s执行一次评估，具体流程：</p>
<ul>
<li><code>evaluate()</code>负责创建和维护整个评估过程： </li>
</ul>
<ol>
<li>获得测试数据</li>
<li>搭建神经网络模型（和训练过程一样） </li>
<li>创建<code>saver</code> ，<code>saver</code>负责恢复<code>shadow variable</code>的值并赋给<code>variable</code></li>
<li>每隔固定的间隔(300s)，运行一次<code>eval_once()</code></li>
</ol>
<ul>
<li><code>eval_once()</code>负责完成一次评估，步骤是： </li>
</ul>
<ol>
<li>从checkpoint中取出最新模型 </li>
<li>运行<code>saver.restore</code>从<code>checkpoint</code>中恢复<code>shadow variable</code>的值并赋给<code>variable</code>。</li>
<li>运行神经网络，对测试集的数据按批次进行预测</li>
<li>计算整个测试集的预测精度</li>
</ol>
<p><br></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""Evaluation for CIFAR-10"""</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'eval_dir'</span>, <span class="string">'/tmp/cifar10_eval'</span>,</div><div class="line">                           <span class="string">"""Directory where to write event logs."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'eval_data'</span>, <span class="string">'test'</span>,</div><div class="line">                           <span class="string">"""Either 'test' or 'train_eval'."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'checkpoint_dir'</span>, <span class="string">'/tmp/cifar10_train'</span>,</div><div class="line">                           <span class="string">"""Directory where to read model checkpoints."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'eval_interval_secs'</span>, <span class="number">60</span> * <span class="number">5</span>,</div><div class="line">                            <span class="string">"""How often to run the eval."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'num_examples'</span>, <span class="number">10000</span>,</div><div class="line">                            <span class="string">"""Number of examples to run."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'run_once'</span>, <span class="keyword">False</span>,</div><div class="line">                         <span class="string">"""Whether to run eval only once."""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_once</span><span class="params">(saver, summary_writer, top_k_op, summary_op)</span>:</span></div><div class="line">  <span class="string">"""运行一次评估</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    saver: Saver.</div><div class="line">    summary_writer: Summary writer.</div><div class="line">    top_k_op: Top K op.</div><div class="line">    summary_op: Summary op.</div><div class="line">  """</div><div class="line">  <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</div><div class="line">      <span class="comment"># 从checkpoint恢复变量的值</span></div><div class="line">      saver.restore(sess, ckpt.model_checkpoint_path)</div><div class="line">      <span class="comment"># model_checkpoint_path提取最新的checkpoint文件名，看起来如下:</span></div><div class="line">      <span class="comment"># /my-favorite-path/cifar10_train/model.ckpt-0</span></div><div class="line">      <span class="comment"># 从中提取出global_step </span></div><div class="line">      global_step = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      print(<span class="string">'No checkpoint file found'</span>)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    <span class="comment"># 开始队列</span></div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">      threads = []</div><div class="line">      <span class="keyword">for</span> qr <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):</div><div class="line">        threads.extend(qr.create_threads(sess, coord=coord, daemon=<span class="keyword">True</span>,</div><div class="line">                                         start=<span class="keyword">True</span>))</div><div class="line"></div><div class="line">      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size)) <span class="comment"># 总的迭代数目</span></div><div class="line">      true_count = <span class="number">0</span>  <span class="comment"># 统计预测正确的数目</span></div><div class="line">      total_sample_count = num_iter * FLAGS.batch_size <span class="comment"># 总的样本数目</span></div><div class="line">      step = <span class="number">0</span></div><div class="line">      <span class="keyword">while</span> step &lt; num_iter <span class="keyword">and</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">        predictions = sess.run([top_k_op])</div><div class="line">        true_count += np.sum(predictions)</div><div class="line">        step += <span class="number">1</span></div><div class="line"></div><div class="line">      <span class="comment"># 计算准确率 @ 1.</span></div><div class="line">      precision = true_count / total_sample_count</div><div class="line">      print(<span class="string">'%s: precision @ 1 = %.3f'</span> % (datetime.now(), precision))</div><div class="line"></div><div class="line">      summary = tf.Summary()</div><div class="line">      summary.ParseFromString(sess.run(summary_op))</div><div class="line">      summary.value.add(tag=<span class="string">'Precision @ 1'</span>, simple_value=precision)</div><div class="line">      summary_writer.add_summary(summary, global_step)</div><div class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:  </div><div class="line">      coord.request_stop(e)</div><div class="line"></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads, stop_grace_period_secs=<span class="number">10</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""Eval CIFAR-10 for a number of steps."""</span></div><div class="line">  <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">    <span class="comment"># 从CIFAR-10中获取图像数据和标签数据</span></div><div class="line">    eval_data = FLAGS.eval_data == <span class="string">'test'</span></div><div class="line">    images, labels = cifar10.inputs(eval_data=eval_data)</div><div class="line"></div><div class="line">    <span class="comment"># 建立一个Graph来计算logits</span></div><div class="line">    logits = cifar10.inference(images)</div><div class="line"></div><div class="line">    <span class="comment"># 计算预测值，输出一个batch_size大小的bool数组</span></div><div class="line">    top_k_op = tf.nn.in_top_k(logits, labels, <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 恢复训练变量的滑动平均值来评估模型</span></div><div class="line">    variable_averages = tf.train.ExponentialMovingAverage(</div><div class="line">        cifar10.MOVING_AVERAGE_DECAY)</div><div class="line">    variables_to_restore = variable_averages.variables_to_restore()</div><div class="line">    saver = tf.train.Saver(variables_to_restore)</div><div class="line"></div><div class="line">    summary_op = tf.summary.merge_all()</div><div class="line"></div><div class="line">    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)</div><div class="line"></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">      eval_once(saver, summary_writer, top_k_op, summary_op)</div><div class="line">      <span class="keyword">if</span> FLAGS.run_once:</div><div class="line">        <span class="keyword">break</span></div><div class="line">      time.sleep(FLAGS.eval_interval_secs)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span>  </div><div class="line">  cifar10.maybe_download_and_extract()</div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(FLAGS.eval_dir):</div><div class="line">    tf.gfile.DeleteRecursively(FLAGS.eval_dir)</div><div class="line">  tf.gfile.MakeDirs(FLAGS.eval_dir)</div><div class="line">  evaluate()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  tf.app.run()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tf.nn.in_top_k(</div><div class="line">    predictions,</div><div class="line">    targets,</div><div class="line">    k,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>判断<code>targets</code>是否在top k的预测之中。输出<code>batch_size</code>大小的bool数组，如果对目标累的预测在所有预测的top k中，则<code>out[i]=True</code>。</p>
<p><br></p>
<h1 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h1><p>作者在单块Tesla K40中训练了10万次用了8小时（350 - 600 images/sec），我在单块Quadro M5000上只用了46分钟（4800～5000 images/sec），下面是训练过程：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">2017-07-07 15:30:08.459355: step 0, loss = 4.68 (317.5 examples/sec; 0.403 sec/batch)</div><div class="line">2017-07-07 15:30:08.794469: step 10, loss = 4.62 (3819.4 examples/sec; 0.034 sec/batch)</div><div class="line">2017-07-07 15:30:09.067413: step 20, loss = 4.49 (4689.6 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:09.343335: step 30, loss = 4.45 (4638.9 examples/sec; 0.028 sec/batch)</div><div class="line">2017-07-07 15:30:09.618720: step 40, loss = 4.31 (4648.0 examples/sec; 0.028 sec/batch)</div><div class="line">2017-07-07 15:30:09.889763: step 50, loss = 4.32 (4722.5 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.162925: step 60, loss = 4.26 (4685.9 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.436191: step 70, loss = 4.07 (4684.1 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.702081: step 80, loss = 4.20 (4814.0 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.963494: step 90, loss = 4.26 (4896.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 15:30:11.442152: step 100, loss = 4.08 (2674.1 examples/sec; 0.048 sec/batch)</div><div class="line">...</div><div class="line">2017-07-07 16:16:08.694992: step 99900, loss = 0.67 (3468.2 examples/sec; 0.037 sec/batch)</div><div class="line">2017-07-07 16:16:08.952094: step 99910, loss = 0.71 (4978.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.211538: step 99920, loss = 0.65 (4933.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.472046: step 99930, loss = 0.76 (4913.5 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.728118: step 99940, loss = 0.81 (4998.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.986376: step 99950, loss = 0.77 (4956.3 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:10.241033: step 99960, loss = 0.56 (5026.4 examples/sec; 0.025 sec/batch)</div><div class="line">2017-07-07 16:16:10.496853: step 99970, loss = 0.71 (5003.5 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:10.760321: step 99980, loss = 0.64 (4858.3 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:11.018312: step 99990, loss = 0.76 (4961.4 examples/sec; 0.026 sec/batch)</div></pre></td></tr></table></figure>
<p>训练和评估过程是放在两个程序分开进行的，具体的实现方法是，在训练过程中，为每个训练变量添加指数滑动平均变量，然后每600s就将模型训练到的变量值保存在<em>checkpoint</em>中，评估过程运行时，从最新存储的<em>checkpoint</em>中取出模型的<code>shadow variable</code>，赋值给对应的变量，然后进行评估。</p>
<p>我们需要同时运行两个程序才能实时的对训练过程进行评估，否则得到的永远只是最新的<em>checkpoint</em>文件中的评估结果。具体可以先运行<code>python cifar_train.py</code> ，再打开另一个窗口运行<code>python cifar_eval.py</code> 。</p>
<p>官方给的代码最大迭代次数是100万，我运行的时候改成了10万。</p>
<p>因为我的迭代速度太快了，到600s时第一次保存<em>checkpoint</em>就已经是两万多次迭代了，可以通过修改<code>tf.train.MonitoredTrainingSession()</code>函数的<code>save_checkpoint_secs</code>参数来修改保存<em>checkpoint</em>的时间间隔，默认600s。</p>
<p>最终10万次迭代后的评估准确率是86.2%，和官方给出的数据还是吻合的。</p>
<p>最后来张TensorBoard的图：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170707164106_Jfe0c9_total_loss.jpeg" alt="Total Loss"></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks  |  TensorFlow</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10 and CIFAR-100 datasets</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/image" target="_blank" rel="external">Images  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/strided_slice" target="_blank" rel="external">tf.strided_slice  |  TensorFlow</a></li>
<li><a href="https://segmentfault.com/a/1190000008793389" target="_blank" rel="external">TensorFlow学习笔记（11）：数据操作指南 - 数据实验室 - SegmentFault</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="external">tf.nn.local_response_normalization  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession" target="_blank" rel="external">tf.train.MonitoredTrainingSession  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文接上文，继续学习TensorFlow在CIFAR-10上的教程，该代码主要由以下五部分组成：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_input.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;读取原始的 CIFAR-10 二进制格式文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;建立 CIFAR-10 网络模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_train.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在单块CPU或者GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_multi_gpu_train.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在多块GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_eval.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在测试集上评估 CIFAR-10 模型的表现&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;本次主要学习&lt;code&gt;cifar10_train.py&lt;/code&gt;和&lt;code&gt;cifar10_eval.py&lt;/code&gt;两个文件，内容分别为训练模型和评估模型，并最终给出实验过程。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十一）：CNN示例代码CIFAR-10分析（上）</title>
    <link href="http://zangbo.me/2017/07/06/TensorFlow_11/"/>
    <id>http://zangbo.me/2017/07/06/TensorFlow_11/</id>
    <published>2017-07-06T07:28:12.000Z</published>
    <updated>2017-07-15T06:45:55.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>TensorFlow官方网站关于卷积神经网络的教程有具体实例，该实例在CIFAR-10数据集上实现，我对这部分代码进行了学习，该代码主要由以下五部分组成：</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cifar10_input.py</code></td>
<td>读取原始的 CIFAR-10 二进制格式文件</td>
</tr>
<tr>
<td><code>cifar10.py</code></td>
<td>建立 CIFAR-10 网络模型</td>
</tr>
<tr>
<td><code>cifar10_train.py</code></td>
<td>在单块CPU或者GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><code>cifar10_multi_gpu_train.py</code></td>
<td>在多块GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><code>cifar10_eval.py</code></td>
<td>在测试集上评估 CIFAR-10 模型的表现</td>
</tr>
</tbody>
</table>
<p>本次只对单GPU情况进行学习，对多GPU不做学习。本次学习分上下两部分，本文首先介绍<code>cifar10_input.py</code>、<code>cifar10.py</code>两个函数，内容分别为数据的获取和模型的建立，同时我们还介绍了本次教程的重点和CIFAR-10数据集。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">https://www.tensorflow.org/tutorials/deep_cnn</a></p>
<p>代码地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>
<p><br></p>
<h1 id="教程重点"><a href="#教程重点" class="headerlink" title="教程重点"></a>教程重点</h1><p>该教程主要实现了以下几个用TensorFlow设计大型且复杂网络模型时的重要构造：</p>
<ul>
<li>核心的运算包括卷积(convolution)、relu激活(rectified linear activations)、最大池化(max poolnig)和局部响应归一化(LRN)。</li>
<li>网络训练过程中的可视化操作</li>
<li>对学习到的参数计算滑动平均值(moving average)，并且在评估模型表现时使用它们。</li>
<li>实现学习率衰减策略来训练，采用指数衰减(exponential_decay)方式。</li>
<li>使用队列(queues)操作获取输入数据。</li>
</ul>
<p><br></p>
<h1 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h1><p>CIFAR-10 数据集分类是机器学习领域很经典的任务，该任务旨在把32x32的RGB图像分成十类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.</div></pre></td></tr></table></figure>
<p>我们这里用到的是二进制数据，该数据共有六个文件，其中五个训练数据文件，文件名为：<code>data_batch_1.bin</code>,…, <code>data_batch_5.bin</code>，一个测试数据文件，名为<code>test_batch.bin</code>。每个文件里包含10000个样本，共计50000个训练样本，10000个测试样本。</p>
<p>文件中数据结构如下（文件中并没具体的划分行）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;1 x label&gt;&lt;3072 x pixel&gt;</div><div class="line">...</div><div class="line">&lt;1 x label&gt;&lt;3072 x pixel&gt;</div></pre></td></tr></table></figure>
<p>第一个字节代表着标签，范围0-9分别代表十类。接下来的3072个字节代表着图像像素值，前1024个字节是red通道的值，接着1024个字节是green通道值，最后1024个字节是blue通道值。字节排列是以行为主的顺序，也就是说前32个字节代表red通道下第一行的图像数据。每个文件由10000个3073字节组成，理论上来说每个文件包含30730000字节长的数据。</p>
<p><br></p>
<h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><p>这部分的代码在<code>cifar10_input.py</code>文件中，该代码主要由四个函数组成：</p>
<ul>
<li><code>read_cifar10()</code>：从文件名队列读取二进制数据并提取出单张图片数据。</li>
<li><code>_generate_image_and_label_batch()</code>：利用单张图片数据生成<code>batch</code>数据。</li>
<li><code>distorted_inputs()</code>：构建训练数据并进行预处理。</li>
<li><code>inputs()</code>：构建测试数据并进行预处理（也可以用在训练集上）。</li>
</ul>
<p>给外部调用的主要是后两个函数，分别生成训练数据和测试数据。</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 如果是python2的代码，会有不兼容，这里把python3的特性引入，使得python2也可以运行该代码。</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import <span class="comment"># 绝对引用</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division <span class="comment"># 精确除法</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function <span class="comment"># print函数</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="comment"># xrange返回一个生成器，range返回一个列表，xrange在生成大范围数据的时候更节省内存。</span></div><div class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">IMAGE_SIZE = <span class="number">24</span> <span class="comment"># 图像尺寸</span></div><div class="line"></div><div class="line"><span class="comment"># 描述 CIFAR-10 数据的全局常量。</span></div><div class="line">NUM_CLASSES = <span class="number">10</span> <span class="comment"># 类别数目</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = <span class="number">50000</span> <span class="comment"># 训练样本数目</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = <span class="number">10000</span> <span class="comment"># 测试样本数目</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_cifar10</span><span class="params">(filename_queue)</span>:</span></div><div class="line">    </div><div class="line">  <span class="string">"""从文件名队列读取二进制数据并提取出单张图像数据</span></div><div class="line"></div><div class="line">  建议: </div><div class="line">    如果想N个线程同步读取, 调用这个函数N次即可。</div><div class="line">    这将会产生N个独立的Readers从不同文件不同位置读取数据，进而产生更好的样本混合效果。</div><div class="line">    </div><div class="line">  输入参数:</div><div class="line">    filename_queue: 一个包含文件名列表的字符串队列</div><div class="line"></div><div class="line">  返回一个类，包含了单张图像的各种数据。</div><div class="line">  """</div><div class="line"></div><div class="line">  <span class="class"><span class="keyword">class</span> <span class="title">CIFAR10Record</span><span class="params">(object)</span>:</span> <span class="comment"># 定义一个类</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line">  result = CIFAR10Record() <span class="comment"># 建立类的实体对象</span></div><div class="line"></div><div class="line">  </div><div class="line">  <span class="comment"># 输入数据格式</span></div><div class="line">  label_bytes = <span class="number">1</span>  </div><div class="line">  result.height = <span class="number">32</span></div><div class="line">  result.width = <span class="number">32</span></div><div class="line">  result.depth = <span class="number">3</span></div><div class="line">  image_bytes = result.height * result.width * result.depth</div><div class="line">  </div><div class="line">  record_bytes = label_bytes + image_bytes</div><div class="line"></div><div class="line">  <span class="comment"># 定义固定长度阅读器读取长度为record_bytes的数据，从文件名队列中获取文件并读出单张图像数据。</span></div><div class="line">  <span class="comment"># CIFAR-10格式数据没有头数据和尾数据，所以我们令 header_bytes 和 footer_bytes 保持默认值0。</span></div><div class="line">  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)</div><div class="line">  result.key, value = reader.read(filename_queue)</div><div class="line"></div><div class="line">  <span class="comment"># 使用解码器把字符串类型转化为uint8类型的数据</span></div><div class="line">  record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line"></div><div class="line">  <span class="comment"># 第一个字节代表着标签数据，所以我们把它的格式从uint8转化为int32。</span></div><div class="line">  <span class="comment"># tf.strided_slice(input_, begin, end, strides)</span></div><div class="line">  result.label = tf.cast(</div><div class="line">      tf.strided_slice(record_bytes, [<span class="number">0</span>], [label_bytes]), tf.int32)</div><div class="line"></div><div class="line">  <span class="comment"># 剩下的字节表示图像数据，我们首先根据CIFAR-10的数据排列[depth * height * width] </span></div><div class="line">  <span class="comment"># 把它转化为 [depth, height, width] 形状的张量。</span></div><div class="line">  depth_major = tf.reshape(</div><div class="line">      tf.strided_slice(record_bytes, [label_bytes],</div><div class="line">                       [label_bytes + image_bytes]),</div><div class="line">      [result.depth, result.height, result.width])</div><div class="line">  <span class="comment"># 把 [depth, height, width] 转化为 [height, width, depth] 形状的张量，这是TensorFlow处理图像的格式。</span></div><div class="line">  result.uint8image = tf.transpose(depth_major, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</div><div class="line"></div><div class="line">  <span class="keyword">return</span> result</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_generate_image_and_label_batch</span><span class="params">(image, label, min_queue_examples, batch_size, shuffle)</span>:</span></div><div class="line">  <span class="string">"""生成图像和标签的batch数据</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    image: 3-D Tensor of [height, width, 3] of type.float32.</div><div class="line">    label: 1-D Tensor of type.int32</div><div class="line">    min_queue_examples: int32, 每次出队后队伍中剩下的样本数量的最小值。</div><div class="line">    batch_size: 每个batch的图像数量。</div><div class="line">    shuffle: boolean值表明是否对图像队列随机排序。</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, height, width, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  <span class="comment"># 创造一个样本队列，根据需求决定是否对样本随机排序，每次从队列中出队batch_size个图像数据和标签数据</span></div><div class="line">  num_preprocess_threads = <span class="number">16</span> </div><div class="line">  <span class="comment">#16个Reader平行读取，每个Reader读不同的文件或者位置，可以充分的混合样本数据</span></div><div class="line">  <span class="keyword">if</span> shuffle: <span class="comment"># 对样本队列随机排序</span></div><div class="line">    images, label_batch = tf.train.shuffle_batch(</div><div class="line">        [image, label],</div><div class="line">        batch_size=batch_size,</div><div class="line">        num_threads=num_preprocess_threads,</div><div class="line">        capacity=min_queue_examples + <span class="number">3</span> * batch_size,</div><div class="line">        min_after_dequeue=min_queue_examples)</div><div class="line">  <span class="keyword">else</span>: <span class="comment"># 不对样本队列随机排序</span></div><div class="line">    images, label_batch = tf.train.batch(</div><div class="line">        [image, label],</div><div class="line">        batch_size=batch_size,</div><div class="line">        num_threads=num_preprocess_threads,</div><div class="line">        capacity=min_queue_examples + <span class="number">3</span> * batch_size)</div><div class="line"></div><div class="line">  <span class="comment"># 添加summary节点以便在TensorBoard中对图像信息进行可视化</span></div><div class="line">  tf.summary.image(<span class="string">'images'</span>, images)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> images, tf.reshape(label_batch, [batch_size])</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">distorted_inputs</span><span class="params">(data_dir, batch_size)</span>:</span></div><div class="line">  <span class="string">""" 构造训练数据并对其进行失真处理。</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    data_dir: CIFAR-10数据的存放路径.</div><div class="line">    batch_size: 每个batch中图像的数目.</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  filenames = [os.path.join(data_dir, <span class="string">'data_batch_%d.bin'</span> % i)</div><div class="line">               <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">6</span>)]</div><div class="line">  <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line"></div><div class="line">  <span class="comment"># 创建一个文件名队列</span></div><div class="line">  filename_queue = tf.train.string_input_producer(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># 调用read_cifar10函数从文件名队列中读取文件，并得到单张图像信息</span></div><div class="line">  read_input = read_cifar10(filename_queue)</div><div class="line">  reshaped_image = tf.cast(read_input.uint8image, tf.float32)</div><div class="line"></div><div class="line">  height = IMAGE_SIZE <span class="comment"># 24</span></div><div class="line">  width = IMAGE_SIZE <span class="comment"># 24</span></div><div class="line"></div><div class="line">  <span class="comment"># 对训练数据图像预处理，包括多个随机失真操作。</span></div><div class="line"></div><div class="line">  <span class="comment"># 把原始的32*32的图像随机裁剪为24*24</span></div><div class="line">  distorted_image = tf.random_crop(reshaped_image, [height, width, <span class="number">3</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 随机左右翻转</span></div><div class="line">  distorted_image = tf.image.random_flip_left_right(distorted_image)</div><div class="line"></div><div class="line">  <span class="comment"># 随机改变图像亮度和对比度</span></div><div class="line">  distorted_image = tf.image.random_brightness(distorted_image,</div><div class="line">                                               max_delta=<span class="number">63</span>)</div><div class="line">  distorted_image = tf.image.random_contrast(distorted_image,</div><div class="line">                                             lower=<span class="number">0.2</span>, upper=<span class="number">1.8</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 把图像进行归一化处理，变为0均值和1方差。</span></div><div class="line">  float_image = tf.image.per_image_standardization(distorted_image)</div><div class="line"></div><div class="line">  <span class="comment"># 有时候graph没法推断出tensors的形状，我们可以手动保存tensors的形状信息</span></div><div class="line">  float_image.set_shape([height, width, <span class="number">3</span>]) </div><div class="line">  read_input.label.set_shape([<span class="number">1</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 当采用随机生成batch操作时设定min_after_dequeue的值为50000*0.4=20000，保证足够的随机性。</span></div><div class="line">  min_fraction_of_examples_in_queue = <span class="number">0.4</span></div><div class="line">  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *</div><div class="line">                           min_fraction_of_examples_in_queue)</div><div class="line">  <span class="keyword">print</span> (<span class="string">'Filling queue with %d CIFAR images before starting to train. '</span></div><div class="line">         <span class="string">'This will take a few minutes.'</span> % min_queue_examples)</div><div class="line"></div><div class="line">  <span class="comment"># 通过建立样本队列来生成batch图像数据和batch标签数据</span></div><div class="line">  <span class="keyword">return</span> _generate_image_and_label_batch(float_image, read_input.label, </div><div class="line">                                         min_queue_examples, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(eval_data, data_dir, batch_size)</span>:</span></div><div class="line">  <span class="string">"""构造测试数据.</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    eval_data: bool型，表明使用训练数据还是测试数据，True为测试集</div><div class="line">    data_dir: CIFAR-10数据集的存放路径</div><div class="line">    batch_size: 每个batch的图片数量</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> eval_data: <span class="comment"># 训练数据</span></div><div class="line">    filenames = [os.path.join(data_dir, <span class="string">'data_batch_%d.bin'</span> % i)</div><div class="line">                 <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">6</span>)]</div><div class="line">    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN</div><div class="line">  <span class="keyword">else</span>: <span class="comment"># 测试数据</span></div><div class="line">    filenames = [os.path.join(data_dir, <span class="string">'test_batch.bin'</span>)]</div><div class="line">    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL</div><div class="line"></div><div class="line">  <span class="keyword">for</span> f <span class="keyword">in</span> filenames: <span class="comment"># 检查文件是否存在</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line"></div><div class="line">  <span class="comment"># 创建一个文件名队列</span></div><div class="line">  filename_queue = tf.train.string_input_producer(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># 从文件名队列中的文件中读取单个样本</span></div><div class="line">  read_input = read_cifar10(filename_queue)</div><div class="line">  reshaped_image = tf.cast(read_input.uint8image, tf.float32)</div><div class="line"></div><div class="line">  height = IMAGE_SIZE</div><div class="line">  width = IMAGE_SIZE</div><div class="line"></div><div class="line">  <span class="comment"># 测试时的图像预处理</span></div><div class="line">  <span class="comment"># 沿中心裁剪28*28大小的图像</span></div><div class="line">  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, height, width)</div><div class="line"></div><div class="line">  <span class="comment"># 图像归一化处理</span></div><div class="line">  float_image = tf.image.per_image_standardization(resized_image)</div><div class="line"></div><div class="line">  <span class="comment"># 设置张量的形状</span></div><div class="line">  float_image.set_shape([height, width, <span class="number">3</span>])</div><div class="line">  read_input.label.set_shape([<span class="number">1</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 设置min_after_dequeue参数为20000(训练数据)，4000(测试数据)，保证足够随机性。</span></div><div class="line">  min_fraction_of_examples_in_queue = <span class="number">0.4</span></div><div class="line">  min_queue_examples = int(num_examples_per_epoch * min_fraction_of_examples_in_queue)</div><div class="line"></div><div class="line">  <span class="comment"># 通过建立一个样本队列生成batch图像数据和batch标签数据</span></div><div class="line">  <span class="keyword">return</span> _generate_image_and_label_batch(float_image, read_input.label, </div><div class="line">                                         min_queue_examples, batch_size, shuffle=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>1、<code>tf.strided_slice(input_, begin, end, strides)</code></p>
<p>用法示例(注意和<code>tf.slice()</code>的区分)：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 'input' is [[[1, 1, 1], [2, 2, 2]],</div><div class="line">#             [[3, 3, 3], [4, 4, 4]],</div><div class="line">#             [[5, 5, 5], [6, 6, 6]]]</div><div class="line">tf.strided_slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3]]]</div><div class="line">tf.strided_slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3],</div><div class="line">                                                               [4, 4, 4]]]</div><div class="line">tf.strided_slice(input, [1, -1, 0], [2, -3, 3], [1, -1, 1]) ==&gt;[[[4, 4, 4],</div><div class="line">                                                              [3, 3, 3]]]</div></pre></td></tr></table></figure>
<p><br></p>
<p>2、TensorFlow提供两种类型的拼接：</p>
<ul>
<li><code>tf.concat(values, axis, name=&#39;concat&#39;)</code>：按照指定的<strong>已经存在</strong>的轴进行拼接</li>
</ul>
<ul>
<li><code>tf.stack(values, axis=0, name=&#39;stack&#39;)</code>：按照指定的<strong>新建</strong>的轴进行拼接</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">t1 = [[1, 2, 3], [4, 5, 6]]</div><div class="line">t2 = [[7, 8, 9], [10, 11, 12]]</div><div class="line">tf.concat([t1, t2], 0) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</div><div class="line">tf.concat([t1, t2], 1) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</div><div class="line">tf.stack([t1, t2], 0)  ==&gt; [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]</div><div class="line">tf.stack([t1, t2], 1)  ==&gt; [[[1, 2, 3], [7, 8, 9]], [[4, 5, 6], [10, 11, 12]]]</div><div class="line">tf.stack([t1, t2], 2)  ==&gt; [[[1, 7], [2, 8], [3, 9]], [[4, 10], [5, 11], [6, 12]]]</div></pre></td></tr></table></figure>
<p>上面的结果读起来不太直观，我们从shape角度看一下就很容易明白了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</div><div class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</div><div class="line">tf.concat([t1, t2], <span class="number">0</span>)  <span class="comment"># [2,3] + [2,3] ==&gt; [4, 3]</span></div><div class="line">tf.concat([t1, t2], <span class="number">1</span>)  <span class="comment"># [2,3] + [2,3] ==&gt; [2, 6]</span></div><div class="line">tf.stack([t1, t2], <span class="number">0</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2*,2,3]</span></div><div class="line">tf.stack([t1, t2], <span class="number">1</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2,2*,3]</span></div><div class="line">tf.stack([t1, t2], <span class="number">2</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2,3,2*]</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><p>这部分代码在<code>cifar10.py</code>文件中，该代码主要由以下函数组成：</p>
<ul>
<li><code>_activation_summary()</code>：为激活函数创造可视化summary</li>
<li><code>_variable_on_cpu()</code>：新建一个变量存储在CPU上</li>
<li><code>_variable_with_weight_decay()</code>：新建一个已经初始化的变量并计算正则化损失</li>
<li><code>distorted_inputs()</code>：获得训练数据</li>
<li><code>inputs()</code>：获得测试数据</li>
<li><code>inference()</code>：搭建神经网络模型，输出Logits</li>
<li><code>loss()</code>：计算总体损失=交叉熵+正则化</li>
<li><code>_add_loss_summaries()</code>：为损失添加可视化summary</li>
<li><code>train()</code>：创建一个optimizer并给所有训练变量添加滑动平均</li>
<li><code>maybe_download_and_extract()</code>：下载并解压训练数据</li>
</ul>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""搭建 CIFAR-10 网络"""</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> tarfile <span class="comment"># 实现文件的压缩与解压缩</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10_input</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line"><span class="comment"># 基本的模型参数，默认batch_size128</span></div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'batch_size'</span>, <span class="number">128</span>,</div><div class="line">                            <span class="string">"""Number of images to process in a batch."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'data_dir'</span>, <span class="string">'/tmp/cifar10_data'</span>,</div><div class="line">                           <span class="string">"""Path to the CIFAR-10 data directory."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'use_fp16'</span>, <span class="keyword">False</span>,</div><div class="line">                            <span class="string">"""Train the model using fp16."""</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设置描述CIFAR-10数据的全局常量</span></div><div class="line">IMAGE_SIZE = cifar10_input.IMAGE_SIZE <span class="comment"># 28</span></div><div class="line">NUM_CLASSES = cifar10_input.NUM_CLASSES <span class="comment"># 10</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN <span class="comment"># 50000</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL <span class="comment"># 10000</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 训练过程中用到的常量</span></div><div class="line">MOVING_AVERAGE_DECAY = <span class="number">0.9999</span>     <span class="comment"># 计算滑动平均(moving average)时的衰减(decay)</span></div><div class="line">NUM_EPOCHS_PER_DECAY = <span class="number">350.0</span>      <span class="comment"># 学习率衰减的epochs</span></div><div class="line">LEARNING_RATE_DECAY_FACTOR = <span class="number">0.1</span>  <span class="comment"># 学习率衰减因子</span></div><div class="line">INITIAL_LEARNING_RATE = <span class="number">0.1</span>       <span class="comment"># 初始学习率</span></div><div class="line"></div><div class="line"><span class="comment"># 如果一个模型在多GPU上训练, 在Op名字上加上前缀tower_name来区分操作</span></div><div class="line"><span class="comment"># 注意当我们可视化一个模型的时候该前缀会被移去。</span></div><div class="line">TOWER_NAME = <span class="string">'tower'</span></div><div class="line"></div><div class="line">DATA_URL = <span class="string">'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_activation_summary</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="string">"""为激活函数创建summary</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    x: Tensor</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    nothing</div><div class="line">  """</div><div class="line">  <span class="comment"># 如果是一个多GPU训练，就把'tower_[0-9]/'移除，这将会帮助我们更清晰的在TensorBoard上展示。</span></div><div class="line">  tensor_name = re.sub(<span class="string">'%s_[0-9]*/'</span> % TOWER_NAME, <span class="string">''</span>, x.op.name)</div><div class="line">  tf.summary.histogram(tensor_name + <span class="string">'/activations'</span>, x)</div><div class="line">  tf.summary.scalar(tensor_name + <span class="string">'/sparsity'</span>,</div><div class="line">                                       tf.nn.zero_fraction(x)) <span class="comment"># 统计0的比例反映稀疏度</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_variable_on_cpu</span><span class="params">(name, shape, initializer)</span>:</span></div><div class="line">  <span class="string">"""帮助新建一个存储在CPU上的变量</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    name: 变量名</div><div class="line">    shape: 变量形状</div><div class="line">    initializer: 变量的初始化器</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    变量Tensor</div><div class="line">  """</div><div class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</div><div class="line">    dtype = tf.float16 <span class="keyword">if</span> FLAGS.use_fp16 <span class="keyword">else</span> tf.float32</div><div class="line">    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)</div><div class="line">  <span class="keyword">return</span> var</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_variable_with_weight_decay</span><span class="params">(name, shape, stddev, wd)</span>:</span></div><div class="line">  <span class="string">"""帮助新建一个已经初始化的变量并且计算正则化损失</span></div><div class="line"></div><div class="line">  变量初始化采用的 truncated normal 分布</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    name: 变量名</div><div class="line">    shape: 变量形状</div><div class="line">    stddev: truncated Gaussian 分布的标准差</div><div class="line">    wd: 正则化系数，添加正则化损失乘以该系数，如果是None则不添加。</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    Variable Tensor</div><div class="line">  """</div><div class="line">  dtype = tf.float16 <span class="keyword">if</span> FLAGS.use_fp16 <span class="keyword">else</span> tf.float32</div><div class="line">  var = _variable_on_cpu(</div><div class="line">      name,</div><div class="line">      shape,</div><div class="line">      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))</div><div class="line">  <span class="keyword">if</span> wd <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=<span class="string">'weight_loss'</span>)</div><div class="line">    tf.add_to_collection(<span class="string">'losses'</span>, weight_decay) <span class="comment"># 把正则化损失存储为全局变量</span></div><div class="line">  <span class="keyword">return</span> var</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">distorted_inputs</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""获得训练数据，对cifar10_input.py文件里distorted_inputs()进行封装</span></div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line"></div><div class="line">  引发:</div><div class="line">    ValueError: 如果没有data_dir</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.data_dir:</div><div class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Please supply a data_dir'</span>)</div><div class="line">  data_dir = os.path.join(FLAGS.data_dir, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,</div><div class="line">                                                  batch_size=FLAGS.batch_size)</div><div class="line">  <span class="keyword">if</span> FLAGS.use_fp16:</div><div class="line">    images = tf.cast(images, tf.float16)</div><div class="line">    labels = tf.cast(labels, tf.float16)</div><div class="line">  <span class="keyword">return</span> images, labels</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(eval_data)</span>:</span></div><div class="line">  <span class="string">"""获得测试数据，对cifar10_input.py文件里inputs()进行封装</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    eval_data: bool, 指出是否使用测试数据还是训练数据（一般是True）</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line"></div><div class="line">  引发:</div><div class="line">    ValueError: 如果没有data_dir</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.data_dir:</div><div class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Please supply a data_dir'</span>)</div><div class="line">  data_dir = os.path.join(FLAGS.data_dir, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  images, labels = cifar10_input.inputs(eval_data=eval_data,</div><div class="line">                                        data_dir=data_dir,</div><div class="line">                                        batch_size=FLAGS.batch_size)</div><div class="line">  <span class="keyword">if</span> FLAGS.use_fp16:</div><div class="line">    images = tf.cast(images, tf.float16)</div><div class="line">    labels = tf.cast(labels, tf.float16)</div><div class="line">  <span class="keyword">return</span> images, labels</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(images)</span>:</span></div><div class="line">  <span class="string">"""搭建 CIFAR-10 模型.</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    images: 从 distorted_inputs() 或 inputs() 中返回的图像数据</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    Logits</div><div class="line">  """</div><div class="line">  <span class="comment"># 我们是用 tf.get_variable() 而不是 tf.Variable() 来新建变量以便在多GPU训练中共享变量</span></div><div class="line">  <span class="comment"># 如果我们只是在单块GPU上训练，可以简化 tf.get_variable() 变成tf.Variable()</span></div><div class="line"></div><div class="line">  <span class="comment"># conv1</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv1'</span>) <span class="keyword">as</span> scope:</div><div class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</div><div class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>],</div><div class="line">                                         stddev=<span class="number">5e-2</span>,</div><div class="line">                                         wd=<span class="number">0.0</span>)</div><div class="line">    conv = tf.nn.conv2d(images, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    pre_activation = tf.nn.bias_add(conv, biases) <span class="comment"># tf.add()的特例，该函数中biases只能是1-D维度的</span></div><div class="line">    conv1 = tf.nn.relu(pre_activation, name=scope.name)</div><div class="line">    _activation_summary(conv1)</div><div class="line"></div><div class="line">  <span class="comment"># pool1</span></div><div class="line">  pool1 = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                         padding=<span class="string">'SAME'</span>, name=<span class="string">'pool1'</span>)</div><div class="line">  <span class="comment"># norm1</span></div><div class="line">  norm1 = tf.nn.lrn(pool1, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>,</div><div class="line">                    name=<span class="string">'norm1'</span>)</div><div class="line"></div><div class="line">  <span class="comment"># conv2</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv2'</span>) <span class="keyword">as</span> scope:</div><div class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</div><div class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>],</div><div class="line">                                         stddev=<span class="number">5e-2</span>,</div><div class="line">                                         wd=<span class="number">0.0</span>)</div><div class="line">    conv = tf.nn.conv2d(norm1, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    pre_activation = tf.nn.bias_add(conv, biases)</div><div class="line">    conv2 = tf.nn.relu(pre_activation, name=scope.name)</div><div class="line">    _activation_summary(conv2)</div><div class="line"></div><div class="line">  <span class="comment"># norm2</span></div><div class="line">  norm2 = tf.nn.lrn(conv2, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>,</div><div class="line">                    name=<span class="string">'norm2'</span>)</div><div class="line">  <span class="comment"># pool2</span></div><div class="line">  pool2 = tf.nn.max_pool(norm2, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</div><div class="line">                         strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=<span class="string">'pool2'</span>)</div><div class="line"></div><div class="line">  <span class="comment"># local3</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local3'</span>) <span class="keyword">as</span> scope:</div><div class="line">    <span class="comment"># Move everything into depth so we can perform a single matrix multiply.</span></div><div class="line">    reshape = tf.reshape(pool2, [FLAGS.batch_size, <span class="number">-1</span>])</div><div class="line">    dim = reshape.get_shape()[<span class="number">1</span>].value</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[dim, <span class="number">384</span>],</div><div class="line">                                          stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">384</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)</div><div class="line">    _activation_summary(local3)</div><div class="line"></div><div class="line">  <span class="comment"># local4</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local4'</span>) <span class="keyword">as</span> scope:</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[<span class="number">384</span>, <span class="number">192</span>],</div><div class="line">                                          stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">192</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)</div><div class="line">    _activation_summary(local4)</div><div class="line"></div><div class="line">  <span class="comment"># 线性层(WX + b)</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax_linear'</span>) <span class="keyword">as</span> scope:</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, [<span class="number">192</span>, NUM_CLASSES],</div><div class="line">                                          stddev=<span class="number">1</span>/<span class="number">192.0</span>, wd=<span class="number">0.0</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [NUM_CLASSES],</div><div class="line">                              tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)</div><div class="line">    _activation_summary(softmax_linear)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> softmax_linear</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(logits, labels)</span>:</span></div><div class="line">  <span class="string">"""添加L2正则化损失到所有的训练变量中</span></div><div class="line"></div><div class="line">  为 "Loss" 和 "Loss/avg" 添加可视化summary</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    logits: 从 inference() 得到的logits</div><div class="line">    labels: 从 distorted_inputs() 或 inputs() 得到的标签数据. 1-D tensor 形状为 [batch_size]</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    float类型的损失tensor.</div><div class="line">  """</div><div class="line">  <span class="comment"># 计算batch的平均交叉熵损失</span></div><div class="line">  <span class="comment"># 如果是label是one-hot码则用tf.nn.softmax_cross_entropy_with_logits()</span></div><div class="line">  <span class="comment"># 这里label是从0-9的数字来表示，则用tf.nn.sparse_softmax_cross_entropy_with_logits()</span></div><div class="line">  labels = tf.cast(labels, tf.int64)</div><div class="line">  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</div><div class="line">      labels=labels, logits=logits, name=<span class="string">'cross_entropy_per_example'</span>)</div><div class="line">  cross_entropy_mean = tf.reduce_mean(cross_entropy, name=<span class="string">'cross_entropy'</span>)</div><div class="line">  tf.add_to_collection(<span class="string">'losses'</span>, cross_entropy_mean)</div><div class="line"></div><div class="line">  <span class="comment"># 总的损失是交叉熵损失加上所有变量的L2正则化损失</span></div><div class="line">  <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">'losses'</span>), name=<span class="string">'total_loss'</span>) </div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_loss_summaries</span><span class="params">(total_loss)</span>:</span></div><div class="line">  <span class="string">"""为损失添加可视化summary</span></div><div class="line"></div><div class="line">  为所有的损失生成滑动平均并且关联summaries来可视化网络表现。</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    total_loss: 从loss()函数得到的总损失.</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    loss_averages_op: 生成损失滑动平均的op</div><div class="line">  """</div><div class="line">  <span class="comment"># 为所有的独立损失和总的损失计算滑动平均</span></div><div class="line">  loss_averages = tf.train.ExponentialMovingAverage(<span class="number">0.9</span>, name=<span class="string">'avg'</span>)</div><div class="line">  losses = tf.get_collection(<span class="string">'losses'</span>) <span class="comment"># 得到包含独立loss的列表</span></div><div class="line">  <span class="comment"># 把每个独立loss和总的loss合并成一个列表，并计算滑动平均</span></div><div class="line">  loss_averages_op = loss_averages.apply(losses + [total_loss])</div><div class="line"></div><div class="line">  <span class="comment"># 添加一个scalar summary给所有的独立损失和总体损失以及它们的滑动平均</span></div><div class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> losses + [total_loss]:</div><div class="line">    <span class="comment"># 给每个损失命名'(raw)'同时给滑动平均损失原始的命名</span></div><div class="line">    tf.summary.scalar(l.op.name + <span class="string">' (raw)'</span>, l)</div><div class="line">    tf.summary.scalar(l.op.name, loss_averages.average(l))</div><div class="line"></div><div class="line">  <span class="keyword">return</span> loss_averages_op</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(total_loss, global_step)</span>:</span></div><div class="line">  <span class="string">"""训练 CIFAR-10 模型.</span></div><div class="line"></div><div class="line">  新建一个优化器并且应用到所有的训练变量上，给所有的训练变量添加滑动平均</div><div class="line">  用tf.control_dependencies控制着执行的先后顺序</div><div class="line">  执行顺序如下：</div><div class="line">  计算损失的滑动平均——&gt;计算学习率——&gt;计算梯度——&gt;更新参数——&gt;计算训练变量的滑动平均——&gt;train_op(返回的值)</div><div class="line">  通过执行train_op来依次执行之前的所有步骤</div><div class="line">  </div><div class="line">  输入参数:</div><div class="line">    total_loss: 从loss()得到的总损失.</div><div class="line">    global_step: 整型Variable来计算迭代次数</div><div class="line"></div><div class="line">  输出数据:</div><div class="line">    train_op: 训练的op.</div><div class="line">  """</div><div class="line">  <span class="comment"># 影响学习率的变量.</span></div><div class="line">  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size</div><div class="line">  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)</div><div class="line"></div><div class="line">  <span class="comment"># 学习率随着迭代次数指数衰减</span></div><div class="line">  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,</div><div class="line">                                  global_step,</div><div class="line">                                  decay_steps,</div><div class="line">                                  LEARNING_RATE_DECAY_FACTOR,</div><div class="line">                                  staircase=<span class="keyword">True</span>)</div><div class="line">  tf.summary.scalar(<span class="string">'learning_rate'</span>, lr)</div><div class="line"></div><div class="line">  <span class="comment"># 对所有损失生成滑动平均并且关联可视化summaries.</span></div><div class="line">  loss_averages_op = _add_loss_summaries(total_loss)</div><div class="line"></div><div class="line">  <span class="comment"># 计算梯度（先执行生成滑动损失的操作loss_averages_op，再计算梯度）。</span></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([loss_averages_op]):</div><div class="line">    opt = tf.train.GradientDescentOptimizer(lr)</div><div class="line">    grads = opt.compute_gradients(total_loss)</div><div class="line"></div><div class="line">  <span class="comment"># 应用梯度更新参数</span></div><div class="line">  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有训练变量添加可视化histograms</span></div><div class="line">  <span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</div><div class="line">    tf.summary.histogram(var.op.name, var)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有梯度添加可视化histograms</span></div><div class="line">  <span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</div><div class="line">    <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      tf.summary.histogram(var.op.name + <span class="string">'/gradients'</span>, grad)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有训练变量添加滑动平均</span></div><div class="line">  variable_averages = tf.train.ExponentialMovingAverage(</div><div class="line">      MOVING_AVERAGE_DECAY, global_step)</div><div class="line">  variables_averages_op = variable_averages.apply(tf.trainable_variables())</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([apply_gradient_op, variables_averages_op]):</div><div class="line">    train_op = tf.no_op(name=<span class="string">'train'</span>) <span class="comment"># 什么也不做，只是为了确保上面两个op的执行</span></div><div class="line"></div><div class="line">  <span class="keyword">return</span> train_op</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download_and_extract</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""下载并解压缩数据"""</span></div><div class="line">  dest_directory = FLAGS.data_dir</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dest_directory):</div><div class="line">    os.makedirs(dest_directory)</div><div class="line">  filename = DATA_URL.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</div><div class="line">  filepath = os.path.join(dest_directory, filename)</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filepath):</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_progress</span><span class="params">(count, block_size, total_size)</span>:</span></div><div class="line">      sys.stdout.write(<span class="string">'\r&gt;&gt; Downloading %s %.1f%%'</span> % (filename,</div><div class="line">          float(count * block_size) / float(total_size) * <span class="number">100.0</span>))</div><div class="line">      sys.stdout.flush()</div><div class="line">    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)</div><div class="line">    print()</div><div class="line">    statinfo = os.stat(filepath)</div><div class="line">    print(<span class="string">'Successfully downloaded'</span>, filename, statinfo.st_size, <span class="string">'bytes.'</span>)</div><div class="line">  extracted_dir_path = os.path.join(dest_directory, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(extracted_dir_path):</div><div class="line">    tarfile.open(filepath, <span class="string">'r:gz'</span>).extractall(dest_directory)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>这里介绍下LRN层，也就是<code>local response normalization</code>，局部响应归一化。最早是由Krizhevsky和Hinton在论文《ImageNet Classification with Deep Convolutional Neural Networks》里面使用的一种数据标准化方法。这种方法是受到神经科学的启发，激活的神经元会抑制其邻近神经元的活动（侧抑制现象），至于为什么使用这种正则手段，以及它为什么有效，查阅了很多文献似乎也没有详细的解释。加上后来BN层太火，LRN用的就比较少了。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks  |  TensorFlow</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10 and CIFAR-100 datasets</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/image" target="_blank" rel="external">Images  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/strided_slice" target="_blank" rel="external">tf.strided_slice  |  TensorFlow</a></li>
<li><a href="https://segmentfault.com/a/1190000008793389" target="_blank" rel="external">TensorFlow学习笔记（11）：数据操作指南 - 数据实验室 - SegmentFault</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="external">tf.nn.local_response_normalization  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession" target="_blank" rel="external">tf.train.MonitoredTrainingSession  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;TensorFlow官方网站关于卷积神经网络的教程有具体实例，该实例在CIFAR-10数据集上实现，我对这部分代码进行了学习，该代码主要由以下五部分组成：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cifar10_input.py&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;读取原始的 CIFAR-10 二进制格式文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cifar10.py&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;建立 CIFAR-10 网络模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cifar10_train.py&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;在单块CPU或者GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cifar10_multi_gpu_train.py&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;在多块GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cifar10_eval.py&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;在测试集上评估 CIFAR-10 模型的表现&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;本次只对单GPU情况进行学习，对多GPU不做学习。本次学习分上下两部分，本文首先介绍&lt;code&gt;cifar10_input.py&lt;/code&gt;、&lt;code&gt;cifar10.py&lt;/code&gt;两个函数，内容分别为数据的获取和模型的建立，同时我们还介绍了本次教程的重点和CIFAR-10数据集。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十）：张量的阶、形状和数据类型</title>
    <link href="http://zangbo.me/2017/07/05/TensorFlow_10/"/>
    <id>http://zangbo.me/2017/07/05/TensorFlow_10/</id>
    <published>2017-07-05T09:00:56.000Z</published>
    <updated>2017-07-05T09:23:17.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文整理了TensorFlow中<strong>张量(Tensor)</strong>相关的一些概念：</p>
<ul>
<li>阶(Rank)</li>
<li>形状(Shape)</li>
<li>数据类型(Type)</li>
</ul>
<p>TensorFlow用张量这种数据结构来表示所有的数据，我们可以把一个张量想象成一个n维的数组或列表。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="阶-Rank"><a href="#阶-Rank" class="headerlink" title="阶(Rank)"></a>阶(Rank)</h1><p>在TensorFlow系统中，张量的维数来被描述为阶(Rank)。张量的阶和矩阵的阶并不是同一个概念，张量的阶是张量维数的一个数量描述。比如下面的张量（使用Python中list定义的）就是2阶。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">t = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</div></pre></td></tr></table></figure>
<p>可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量。对于一个二阶张量我们可以用语句<code>t[i, j]</code>来访问其中的任何元素。而对于三阶张量你可以用<code>t[i, j, k]</code>来访问其中的任何元素。</p>
<p>维数越靠后，位置越靠里。比如上面的二阶张量，<code>t[1, 3] = 3</code>。维数越靠前，位置越靠外，比如第一维度的数据，是最外层中括号的数据。</p>
<table>
<thead>
<tr>
<th>阶</th>
<th>数学实例</th>
<th>Python 例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>纯量 (只有大小)</td>
<td><code>s = 483</code></td>
</tr>
<tr>
<td>1</td>
<td>向量(大小和方向)</td>
<td><code>v = [1.1, 2.2, 3.3]</code></td>
</tr>
<tr>
<td>2</td>
<td>矩阵(数据表)</td>
<td><code>m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</code></td>
</tr>
<tr>
<td>3</td>
<td>3阶张量 (数据立体)</td>
<td><code>t = [[[2], [4], [6]], [[8], [10], [12]], [[14], [16], [18]]]</code></td>
</tr>
<tr>
<td>n</td>
<td>n阶 (自己想想看)</td>
<td><code>....</code></td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="形状-Shape"><a href="#形状-Shape" class="headerlink" title="形状(Shape)"></a>形状(Shape)</h1><p>TensorFlow中使用了三种记号来方便地描述张量的维度：阶，形状以及维数。下表展示了它们之间的关系：</p>
<table>
<thead>
<tr>
<th>阶</th>
<th>形状</th>
<th>维数</th>
<th>实例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[ ]</td>
<td>0-D</td>
<td>一个0维张量，一个纯量。</td>
</tr>
<tr>
<td>1</td>
<td>[D0]</td>
<td>1-D</td>
<td>一个1维张量的形式[5]</td>
</tr>
<tr>
<td>2</td>
<td>[D0, D1]</td>
<td>2-D</td>
<td>一个2维张量的形式[3, 4]</td>
</tr>
<tr>
<td>3</td>
<td>[D0, D1, D2]</td>
<td>3-D</td>
<td>一个3维张量的形式 [1, 4, 3]</td>
</tr>
<tr>
<td>n</td>
<td>[D0, D1, … Dn]</td>
<td>n-D</td>
<td>一个n维张量的形式 [D0, D1, … Dn]</td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="数据类型-Type"><a href="#数据类型-Type" class="headerlink" title="数据类型(Type)"></a>数据类型(Type)</h1><p>除了维度，Tensors有一个数据类型属性.你可以为一个张量指定下列数据类型中的任意一个类型：</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>Python 类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DT_FLOAT</code></td>
<td><code>tf.float32</code></td>
<td>32 位浮点数.</td>
</tr>
<tr>
<td><code>DT_DOUBLE</code></td>
<td><code>tf.float64</code></td>
<td>64 位浮点数.</td>
</tr>
<tr>
<td><code>DT_INT64</code></td>
<td><code>tf.int64</code></td>
<td>64 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT32</code></td>
<td><code>tf.int32</code></td>
<td>32 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT16</code></td>
<td><code>tf.int16</code></td>
<td>16 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT8</code></td>
<td><code>tf.int8</code></td>
<td>8 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_UINT8</code></td>
<td><code>tf.uint8</code></td>
<td>8 位无符号整型.</td>
</tr>
<tr>
<td><code>DT_STRING</code></td>
<td><code>tf.string</code></td>
<td>可变长度的字节数组，每一个张量元素都是一个字节数组。</td>
</tr>
<tr>
<td><code>DT_BOOL</code></td>
<td><code>tf.bool</code></td>
<td>布尔型</td>
</tr>
<tr>
<td><code>DT_COMPLEX64</code></td>
<td><code>tf.complex64</code></td>
<td>由两个32位浮点数组成的复数：实数和虚数。</td>
</tr>
<tr>
<td><code>DT_QINT32</code></td>
<td><code>tf.qint32</code></td>
<td>用于量化Ops的32位有符号整型</td>
</tr>
<tr>
<td><code>DT_QINT8</code></td>
<td><code>tf.qint8</code></td>
<td>用于量化Ops的8位有符号整型</td>
</tr>
<tr>
<td><code>DT_QUINT8</code></td>
<td><code>tf.quint8</code></td>
<td>用于量化Ops的8位无符号整型</td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/dims_types" target="_blank" rel="external">Tensor Ranks, Shapes, and Types  |  TensorFlow</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/resources/dims_types.html" target="_blank" rel="external">张量的阶、形状、数据类型 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文整理了TensorFlow中&lt;strong&gt;张量(Tensor)&lt;/strong&gt;相关的一些概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阶(Rank)&lt;/li&gt;
&lt;li&gt;形状(Shape)&lt;/li&gt;
&lt;li&gt;数据类型(Type)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TensorFlow用张量这种数据结构来表示所有的数据，我们可以把一个张量想象成一个n维的数组或列表。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（九）：数据读取</title>
    <link href="http://zangbo.me/2017/07/05/TensorFlow_9/"/>
    <id>http://zangbo.me/2017/07/05/TensorFlow_9/</id>
    <published>2017-07-05T02:04:11.000Z</published>
    <updated>2017-07-15T06:50:54.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文整理了TensorFlow中的数据读取方法，在TensorFlow中主要有三种方法读取数据：</p>
<ol>
<li>Feeding：由Python提供数据。</li>
<li>Preloaded data：预加载数据。</li>
<li>Reading from files：从文件读取。</li>
</ol>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Feeding"><a href="#Feeding" class="headerlink" title="Feeding"></a>Feeding</h1><p>我们一般用<code>tf.placeholder</code>节点来<code>feed</code>数据，该节点不需要初始化也不包含任何数据，我们在执行<code>run()</code>或者<code>eval()</code>指令时通过<code>feed_dict</code>参数把数据传入<code>graph</code>中来计算。如果在运行过程中没有对<code>tf.placeholder</code>节点传入数据，程序会报错。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 设计Graph</span></div><div class="line">x1 = tf.placeholder(tf.int16)</div><div class="line">x2 = tf.placeholder(tf.int16)</div><div class="line">y = tf.add(x1, x2)</div><div class="line"><span class="comment"># 用Python产生数据</span></div><div class="line">li1 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</div><div class="line">li2 = [<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>]</div><div class="line"><span class="comment"># 打开一个session --&gt; 喂数据 --&gt; 计算y</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="keyword">print</span> sess.run(y, feed_dict=&#123;x1: li1, x2: li2&#125;)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Preloaded-data"><a href="#Preloaded-data" class="headerlink" title="Preloaded data"></a>Preloaded data</h1><p>预加载数据方法仅限于用在可以完全加载到内存中的小数据集上，主要有两种方法：</p>
<ol>
<li>把数据存在常量（constant）中。</li>
<li>把数据存在变量（variable）中，我们初始化并且永不改变它的值。</li>
</ol>
<p>用常量更简单些，但会占用更多的内存，因为常量存储在<code>graph</code>数据结构内部。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">training_data = ...</div><div class="line">training_labels = ...</div><div class="line"><span class="keyword">with</span> tf.Session():</div><div class="line">  input_data = tf.constant(training_data)</div><div class="line">  input_labels = tf.constant(training_labels)</div><div class="line">  ...</div></pre></td></tr></table></figure>
<p>如果用变量的话，我们需要在<code>graph</code>构建好之后初始化该变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">training_data = ...</div><div class="line">training_labels = ...</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  data_initializer = tf.placeholder(dtype=training_data.dtype,</div><div class="line">                                    shape=training_data.shape)</div><div class="line">  label_initializer = tf.placeholder(dtype=training_labels.dtype,</div><div class="line">                                     shape=training_labels.shape)</div><div class="line">  input_data = tf.Variable(data_initializer, trainable=<span class="keyword">False</span>, collections=[])</div><div class="line">  input_labels = tf.Variable(label_initializer, trainable=<span class="keyword">False</span>, collections=[])</div><div class="line">  ...</div><div class="line">  sess.run(input_data.initializer,</div><div class="line">           feed_dict=&#123;data_initializer: training_data&#125;)</div><div class="line">  sess.run(input_labels.initializer,</div><div class="line">           feed_dict=&#123;label_initializer: training_labels&#125;)</div></pre></td></tr></table></figure>
<p>设定<code>trainable=False</code> 可以防止该变量被数据流图的 <code>GraphKeys.TRAINABLE_VARIABLES</code> 收集, 这样我们就不会在训练的时候尝试更新它的值； 设定 <code>collections=[]</code> 可以防止<code>GraphKeys.VARIABLES</code> 把它收集后做为保存和恢复的中断点。</p>
<p>无论哪种方式，我们可以用<code>tf.train.slice_input_producer</code>函数每次产生一个切片。这样就会让样本在整个迭代中被打乱，所以在使用批处理的时候不需要再次打乱样本。所以我们不使用<code>shuffle_batch</code>函数，取而代之的是纯<code>tf.train.batch</code> 函数。 如果要使用多个线程进行预处理，需要将<code>num_threads</code>参数设置为大于1的数字。</p>
<p><br></p>
<h1 id="Reading-from-files"><a href="#Reading-from-files" class="headerlink" title="Reading from files"></a>Reading from files</h1><p>从文件中读取数据一般包含以下步骤：</p>
<ol>
<li>文件名列表</li>
<li>文件名随机排序（可选的）</li>
<li>迭代控制（可选的）</li>
<li>文件名队列</li>
<li>针对输入文件格式的阅读器</li>
<li>记录解析器</li>
<li>预处理器（可选的）</li>
<li>样本队列</li>
</ol>
<p><br></p>
<h2 id="文件名、随机排序和迭代控制"><a href="#文件名、随机排序和迭代控制" class="headerlink" title="文件名、随机排序和迭代控制"></a>文件名、随机排序和迭代控制</h2><p>我们首先要有个文件名列表，为了产生文件名列表，我们可以手动用Python输入字符串，例如：</p>
<ul>
<li><code>[&quot;file0&quot;, &quot;file1&quot;]</code></li>
<li><code>[(&quot;file%d&quot; % i) for i in range(2)]</code></li>
<li><code>[(&quot;file%d&quot; % i) for i in range(2)]</code></li>
</ul>
<p>我们也可以用<code>tf.train.match_filenames_once</code>函数来生成文件名列表。</p>
<p>有了文件名列表后，我们需要把它送入<code>tf.train.string_input_producer</code>函数中生成一个先入先出的文件名队列，文件阅读器需要从该队列中读取文件名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">string_input_producer(</div><div class="line">    string_tensor,</div><div class="line">    num_epochs=<span class="keyword">None</span>,</div><div class="line">    shuffle=<span class="keyword">True</span>,</div><div class="line">    seed=<span class="keyword">None</span>,</div><div class="line">    capacity=<span class="number">32</span>,</div><div class="line">    shared_name=<span class="keyword">None</span>,</div><div class="line">    name=<span class="keyword">None</span>,</div><div class="line">    cancel_op=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>一个<code>QueueRunner</code>每次会把每批次的所有文件名送入队列中，可以通过设置<code>string_input_producer</code>函数的<code>shuffle</code>参数来对文件名随机排序，或者通过设置<code>num_epochs</code>来决定对<code>string_tensor</code>里的文件使用多少次，类型为整型，如果想要迭代控制则需要设置了<code>num_epochs</code>参数，同时需要添加<code>tf.local_variables_initializer()</code>进行初始化，如果不初始化会报错。</p>
<p>这个<code>QueueRunner</code>的工作线程独立于文件阅读器的线程， 因此随机排序和将文件名送入到文件名队列这些过程不会阻碍文件阅读器的运行。</p>
<p><br></p>
<h2 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h2><p>根据不同的文件格式， 应该选择对应的文件阅读器， 然后将文件名队列提供给阅读器的<code>read</code>方法。阅读器每次从队列中读取一个文件，它的<code>read</code>方法会输出一个<code>key</code>来表征读入的文件和其中的纪录(对于调试非常有用)，同时得到一个字符串标量， 这个字符串标量可以被一个或多个解析器，或者转换操作将其解码为张量并且构造成为样本。</p>
<p>根据不同的文件类型，有三种不同的文件阅读器：</p>
<ul>
<li><code>tf.TextLineReader</code></li>
<li><code>tf.FixedLengthRecordReader</code></li>
<li><code>tf.TFRecordReader</code></li>
</ul>
<p>它们分别用于单行读取(如CSV文件)、固定长度读取(如CIFAR-10的.bin二进制文件)、TensorFlow标准格式读取。</p>
<p>根据不同的文件阅读器，有三种不同的解析器，它们分别对应上面三种阅读器：</p>
<ul>
<li><code>tf.decode_csv</code></li>
<li><code>tf.decode_raw</code></li>
<li><code>tf.parse_single_example</code>和<code>tf.parse_example</code></li>
</ul>
<p><br></p>
<h3 id="CSV文件"><a href="#CSV文件" class="headerlink" title="CSV文件"></a>CSV文件</h3><p>当我们读入CSV格式的文件时，我们可以使用<code>tf.TextLineReader</code>阅读器和<code>tf.decode_csv</code>解析器。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">filename_queue = tf.train.string_input_producer([<span class="string">"file0.csv"</span>, <span class="string">"file1.csv"</span>]) </div><div class="line"><span class="comment"># 创建一个Filename Queue</span></div><div class="line"><span class="comment"># 该例csv文件中共有5列数据，前四列为features，最后一列为label</span></div><div class="line"></div><div class="line">reader = tf.TextLineReader() <span class="comment"># 文件阅读器</span></div><div class="line">key, value = reader.read(filename_queue) <span class="comment"># 每次执行阅读器都从文件读一行内容</span></div><div class="line"></div><div class="line"><span class="comment"># Default values, in case of empty columns. Also specifies the type of the decoded result.</span></div><div class="line">record_defaults = [[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]] <span class="comment"># 文件数据皆为整数</span></div><div class="line">col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line">features = tf.stack([col1, col2, col3, col4])</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Start populating the filename queue.</span></div><div class="line">  coord = tf.train.Coordinator() <span class="comment">#创建一个协调器，管理线程</span></div><div class="line">  threads = tf.train.start_queue_runners(coord=coord) <span class="comment">#启动QueueRunner, 此时文件名队列已经进队。</span></div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1200</span>):</div><div class="line">    <span class="comment"># Retrieve a single instance:</span></div><div class="line">    example, label = sess.run([features, col5])</div><div class="line"></div><div class="line">  coord.request_stop()</div><div class="line">  coord.join(threads)</div></pre></td></tr></table></figure>
<p>每次<code>read</code>的执行都会从文件中读取一行内容， <code>decode_csv</code> 操作会解析这一行内容并将其转为张量列表。在调用<code>run</code>或者<code>eval</code>去执行<code>read</code>之前， 必须先调用<code>tf.train.start_queue_runners</code>来将文件名填充到队列。否则<code>read</code>操作会被阻塞到文件名队列中有值为止。</p>
<p><code>record_defaults = [[1], [1], [1], [1], [1]]</code>代表了解析的摸版，默认用<code>,</code>隔开，是用于指定矩阵格式以及数据类型的，CSV文件中的矩阵是NXM的，则此处为1XM，例如上例中M=5。[1]表示解析为整型，如果矩阵中有小数，则应为float型，[1]应该变为[1.0]，[‘null’]解析为string类型。</p>
<p><code>col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults = record_defaults)</code>， 矩阵中有几列，这里就要写几个参数，比如5列，就要写到col5,不管你到底用多少。否则报错。</p>
<p><br></p>
<h3 id="固定长度记录"><a href="#固定长度记录" class="headerlink" title="固定长度记录"></a>固定长度记录</h3><p>我们也可以从<strong>二进制文件(.bin)</strong>中读取固定长度的数据，使用的是<code>tf.FixedLengthRecordReader</code>阅读器和<code>tf.decode_raw</code>解析器。<code>decode_raw</code>节点会把<code>string</code>转化为<code>uint8</code>类型的张量。</p>
<p>例如CIFAR-10数据集就采用的固定长度的数据，1字节的标签，后面跟着3072字节的图像数据。使用<code>uint8</code>类型张量的标准操作可以把每个图像的片段截取下来并且按照需要重组。下面有一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">reader = tf.FixedLengthRecordReader(record_bytes = record_bytes)</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line">label = tf.cast(tf.slice(record_bytes, [<span class="number">0</span>], [label_bytes]), tf.int32)</div><div class="line">image_raw = tf.slice(record_bytes, [label_bytes], [image_bytes])</div><div class="line">image_raw = tf.reshape(image_raw, [depth, height, width])</div><div class="line">image = tf.transpose(image_raw, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)) <span class="comment"># 图像形状为[height, width, channels]     </span></div><div class="line">image = tf.cast(image, tf.float32)</div></pre></td></tr></table></figure>
<p>这里介绍上述代码中出现的函数：<code>tf.slice()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">slice(</div><div class="line">    input_,</div><div class="line">    begin,</div><div class="line">    size,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>从一个张量<code>input</code>中提取出长度为<code>size</code>的一部分，提取的起点由<code>begin</code>定义。<code>size</code>是一个向量，它代表着在每个维度提取出的<code>tensor</code>的大小。<code>begin</code>表示提取的位置，它表示的是<code>input</code>的起点偏离值，也就是从每个维度第几个值开始提取。</p>
<p><code>begin</code>从0开始，<code>size</code>从1开始，如果<code>size[i]</code>的值为-1，则第i个维度从<code>begin</code>处到余下的所有值都被提取出来。</p>
<p>例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 'input' is [[[1, 1, 1], [2, 2, 2]],</div><div class="line">#             [[3, 3, 3], [4, 4, 4]],</div><div class="line">#             [[5, 5, 5], [6, 6, 6]]]</div><div class="line">tf.slice(input, [1, 0, 0], [1, 1, 3]) ==&gt; [[[3, 3, 3]]]</div><div class="line">tf.slice(input, [1, 0, 0], [1, 2, 3]) ==&gt; [[[3, 3, 3],</div><div class="line">                                            [4, 4, 4]]]</div><div class="line">tf.slice(input, [1, 0, 0], [2, 1, 3]) ==&gt; [[[3, 3, 3]],</div><div class="line">                                           [[5, 5, 5]]]</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="标准TensorFlow格式"><a href="#标准TensorFlow格式" class="headerlink" title="标准TensorFlow格式"></a>标准TensorFlow格式</h3><p>我们也可以把任意的数据转换为TensorFlow所支持的格式， 这种方法使TensorFlow的数据集更容易与网络应用架构相匹配。这种方法就是使用TFRecords文件，TFRecords文件包含了<code>tf.train.Example</code>的<em>protocol buffer</em>（里面包含了名为 <code>Features</code>的字段）。你可以写一段代码获取你的数据， 将数据填入到<code>Example</code>的<em>protocol buffer</em>，将<em>protocol buffer</em>序列化为一个字符串， 并且通过<code>tf.python_io.TFRecordWriter</code>类写入到TFRecords文件。</p>
<p>从TFRecords文件中读取数据， 可以使用<code>tf.TFRecordReader</code>阅读器以及<code>tf.parse_single_example</code>解析器。<code>parse_single_example</code>操作可以将<code>Example</code><em>protocol buffer</em>解析为张量。 具体可以参考如下例子，把MNIST数据集转化为TFRecords格式：</p>
<ul>
<li><a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/convert_to_records.py" target="_blank" rel="external">tensorflow/examples/how_tos/reading_data/convert_to_records.py</a></li>
</ul>
<p>SparseTensors这种稀疏输入数据类型使用队列来处理不是太好。如果要使用SparseTensors你就必须在批处理<strong>之后</strong>使用<code>tf.parse_example</code>去解析字符串记录 (而不是在批处理<strong>之前</strong>使用<code>tf.parse_single_example</code>) 。</p>
<p><br></p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>我们可以对输入的样本数据进行任意的预处理， 这些预处理不依赖于训练参数， 比如数据归一化， 提取随机数据片，增加噪声或失真等等。具体可以参考如下对CIFAR-10处理的例子：</p>
<ul>
<li><a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10.py" target="_blank" rel="external">tensorflow/models/image/cifar10/cifar10.py</a></li>
</ul>
<p><br></p>
<h2 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h2><p>经过了之前的步骤，在数据读取流程的最后， 我们需要有另一个队列来批量执行输入样本的训练，评估或者推断。根据要不要打乱顺序，我们常用的有两个函数：</p>
<ul>
<li><code>tf.train.batch()</code></li>
<li><code>tf.train.shuffle_batch()</code></li>
</ul>
<p>下面来分别介绍：</p>
<h3 id="tf-train-batch"><a href="#tf-train-batch" class="headerlink" title="tf.train.batch()"></a>tf.train.batch()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">tf.train.batch(</div><div class="line">    tensors,</div><div class="line">    batch_size,</div><div class="line">    num_threads=<span class="number">1</span>,</div><div class="line">    capacity=<span class="number">32</span>,</div><div class="line">    enqueue_many=<span class="keyword">False</span>,</div><div class="line">    shapes=<span class="keyword">None</span>,</div><div class="line">    dynamic_pad=<span class="keyword">False</span>,</div><div class="line">    allow_smaller_final_batch=<span class="keyword">False</span>,</div><div class="line">    shared_name=<span class="keyword">None</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>该函数将会使用一个队列，函数读取一定数量的<code>tensors</code>送入队列，然后每次从中选取<code>batch_size</code>个<code>tensors</code>组成一个新的<code>tensors</code>返回出来。</p>
<p><code>capacity</code>参数决定了队列的长度。</p>
<p><code>num_threads</code>决定了有多少个线程进行入队操作，如果设置的超过一个线程，它们将从不同文件不同位置同时读取，可以更加充分的混合训练样本。</p>
<p>如果<code>enqueue_many</code>参数为<code>False</code>，则输入参数<code>tensors</code>为一个形状为<code>[x, y, z]</code>的张量，输出为一个形状为<code>[batch_size, x, y, z]</code>的张量。如果<code>enqueue_many</code>参数为<code>True</code>，则输入参数<code>tensors</code>为一个形状为<code>[*, x, y, z]</code>的张量，其中所有<code>*</code>的数值相同，输出为一个形状为<code>[batch_size, x, y, z]</code>的张量。</p>
<p>当<code>allow_smaller_final_batch</code>为<code>True</code>时，如果队列中的张量数量不足<code>batch_size</code>，将会返回小于<code>batch_size</code>长度的张量，如果为<code>False</code>，剩下的张量会被丢弃。</p>
<h3 id="tf-train-shuffle-batch"><a href="#tf-train-shuffle-batch" class="headerlink" title="tf.train.shuffle_batch()"></a>tf.train.shuffle_batch()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">tf.train.shuffle_batch(</div><div class="line">    tensors,</div><div class="line">    batch_size,</div><div class="line">    capacity,</div><div class="line">    min_after_dequeue,</div><div class="line">    num_threads=1,</div><div class="line">    seed=None,</div><div class="line">    enqueue_many=False,</div><div class="line">    shapes=None,</div><div class="line">    allow_smaller_final_batch=False,</div><div class="line">    shared_name=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>该函数类似于上面的<code>tf.train.batch()</code>，同样创建一个队列，主要区别是会首先把队列中的张量进行乱序处理，然后再选取其中的<code>batch_size</code>个张量组成一个新的张量返回。但是新增加了几个参数。</p>
<p><code>capacity</code>参数依然为队列的长度，建议<code>capacity</code>的取值如下：</p>
<p><code>min_after_dequeue + (num_threads + a small safety margin) * batch_size</code></p>
<p><code>min_after_dequeue</code>这个参数的意思是队列中，做dequeue（取数据）的操作后，线程要保证队列中至少剩下<code>min_after_dequeue</code>个数据。如果<code>min_after_dequeue</code>设置的过少，则即使<code>shuffle</code>为<code>True</code>，也达不到好的混合效果。</p>
<blockquote>
<p>假设你有一个队列，现在里面有m个数据，你想要每次随机从队列中取n个数据，则代表先混合了m个数据，再从中取走n个。 </p>
<p>当第一次取走n个后，队列就变为m-n个数据；当你下次再想要取n个时，假设队列在此期间入队进来了k个数据，则现在的队列中有(m-n+k)个数据，则此时会从混合的(m-n+k)个数据中随机取走n个。</p>
<p>如果队列填充的速度比较慢，k就比较小，那你取出来的n个数据只是与周围很小的一部分(m-n+k)个数据进行了混合。</p>
<p>因为我们的目的肯定是想尽最大可能的混合数据，因此设置<code>min_after_dequeue</code>，可以保证每次dequeue后都有足够量的数据填充尽队列，保证下次dequeue时可以很充分的混合数据。</p>
<p>但是<code>min_after_dequeue</code>也不能设置的太大，这样会导致队列填充的时间变长，尤其是在最初的装载阶段，会花费比较长的时间。</p>
</blockquote>
<p>其他参数和<code>tf.train.batch()</code>相同。</p>
<p><br></p>
<p>这里我们使用<code>tf.train.shuffle_batch</code>函数来对队列中的样本进行乱序处理。如下的模版：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_my_file_format</span><span class="params">(filename_queue)</span>:</span></div><div class="line">  reader = tf.SomeReader()</div><div class="line">  key, record_string = reader.read(filename_queue)</div><div class="line">  example, label = tf.some_decoder(record_string)</div><div class="line">  processed_example = some_processing(example)</div><div class="line">  <span class="keyword">return</span> processed_example, label</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_pipeline</span><span class="params">(filenames, batch_size, num_epochs=None)</span>:</span></div><div class="line">  filename_queue = tf.train.string_input_producer(</div><div class="line">      filenames, num_epochs=num_epochs, shuffle=<span class="keyword">True</span>)</div><div class="line">  example, label = read_my_file_format(filename_queue)</div><div class="line">  <span class="comment"># min_after_dequeue 越大意味着随机效果越好但是也会占用更多的时间和内存</span></div><div class="line">  <span class="comment"># capacity 必须比 min_after_dequeue 大</span></div><div class="line">  <span class="comment"># 建议capacity的取值如下：</span></div><div class="line">  <span class="comment"># min_after_dequeue + (num_threads + a small safety margin) * batch_size</span></div><div class="line">  min_after_dequeue = <span class="number">10000</span></div><div class="line">  capacity = min_after_dequeue + <span class="number">3</span> * batch_size</div><div class="line">  example_batch, label_batch = tf.train.shuffle_batch(</div><div class="line">      [example, label], batch_size=batch_size, capacity=capacity,</div><div class="line">      min_after_dequeue=min_after_dequeue)</div><div class="line">  <span class="keyword">return</span> example_batch, label_batch</div></pre></td></tr></table></figure>
<p>一个具体的例子如下，该例采用了CIFAR-10数据集，采用了固定长度读取的<code>tf.FixedLengthRecordReader</code>阅读器和<code>tf.decode_raw</code>解析器，同时进行了数据预处理操作中的标准化操作，最后使用<code>tf.train.shuffle_batch</code>函数批量执行数据的乱序处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">cifar10_data</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filename_queue)</span>:</span></div><div class="line">        self.height = <span class="number">32</span></div><div class="line">        self.width = <span class="number">32</span></div><div class="line">        self.depth = <span class="number">3</span></div><div class="line">        self.label_bytes = <span class="number">1</span></div><div class="line">        self.image_bytes = self.height * self.width * self.depth</div><div class="line">        self.record_bytes = self.label_bytes + self.image_bytes</div><div class="line">        self.label, self.image = self.read_cifar10(filename_queue)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_cifar10</span><span class="params">(self, filename_queue)</span>:</span></div><div class="line">        reader = tf.FixedLengthRecordReader(record_bytes = self.record_bytes)</div><div class="line">        key, value = reader.read(filename_queue)</div><div class="line">        record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line">        label = tf.cast(tf.slice(record_bytes, [<span class="number">0</span>], [self.label_bytes]), tf.int32)</div><div class="line">        image_raw = tf.slice(record_bytes, [self.label_bytes], [self.image_bytes])</div><div class="line">        image_raw = tf.reshape(image_raw, [self.depth, self.height, self.width])</div><div class="line">        image = tf.transpose(image_raw, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))        </div><div class="line">        image = tf.cast(image, tf.float32)</div><div class="line">        <span class="keyword">return</span> label, image</div><div class="line">    </div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(data_dir, batch_size, train = True, name = <span class="string">'input'</span>)</span>:</span></div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">        <span class="keyword">if</span> train:    </div><div class="line">            filenames = [os.path.join(data_dir,<span class="string">'data_batch_%d.bin'</span> % ii) </div><div class="line">                        <span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>)]</div><div class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line">                    </div><div class="line">            filename_queue = tf.train.string_input_producer(filenames)</div><div class="line">            read_input = cifar10_data(filename_queue)</div><div class="line">            images = read_input.image</div><div class="line">            images = tf.image.per_image_standardization(images)</div><div class="line">            labels = read_input.label</div><div class="line">            image, label = tf.train.shuffle_batch(</div><div class="line">                                    [images,labels], batch_size = batch_size, </div><div class="line">                                    min_after_dequeue = <span class="number">20000</span>, capacity = <span class="number">20192</span>)</div><div class="line">        </div><div class="line">            <span class="keyword">return</span> image, tf.reshape(label, [batch_size])</div><div class="line">            </div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            filenames = [os.path.join(data_dir,<span class="string">'test_batch.bin'</span>)]</div><div class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line">                    </div><div class="line">            filename_queue = tf.train.string_input_producer(filenames)</div><div class="line">            read_input = cifar10_data(filename_queue)</div><div class="line">            images = read_input.image</div><div class="line">            images = tf.image.per_image_standardization(images)</div><div class="line">            labels = read_input.label</div><div class="line">            image, label = tf.train.shuffle_batch(</div><div class="line">                                    [images,labels], batch_size = batch_size, </div><div class="line">                                    min_after_dequeue = <span class="number">20000</span>, capacity = <span class="number">20192</span>)</div><div class="line">        </div><div class="line">            <span class="keyword">return</span> image, tf.reshape(label, [batch_size])</div></pre></td></tr></table></figure>
<p>这里介绍下函数<code>tf.image.per_image_standardization(image)</code>，该函数对图像进行线性变换使它具有零均值和单位方差，即规范化。其中参数<code>image</code>是一个3-D的张量，形状为<code>[height, width, channels]</code>。</p>
<p><br></p>
<h2 id="多个样本和多个阅读器"><a href="#多个样本和多个阅读器" class="headerlink" title="多个样本和多个阅读器"></a>多个样本和多个阅读器</h2><p>下面讲分别展示三个不同Reader数目和不同样本数的代码示例：</p>
<h3 id="文件准备"><a href="#文件准备" class="headerlink" title="文件准备"></a>文件准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Alpha1,A1\nAlpha2,A2\nAlpha3,A3"</span> &gt; A.csv</div><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Bee1,B1\nBee2,B2\nBee3,B3"</span> &gt; B.csv</div><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Sea1,C1\nSea2,C2\nSea3,C3"</span> &gt; C.csv</div><div class="line">$ cat A.csv</div><div class="line">Alpha1,A1</div><div class="line">Alpha2,A2</div><div class="line">Alpha3,A3</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="单个Reader，单个样本"><a href="#单个Reader，单个样本" class="headerlink" title="单个Reader，单个样本"></a>单个Reader，单个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 生成一个先入先出队列和一个QueueRunner</span></div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># 定义Reader</span></div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line"><span class="comment"># 定义Decoder</span></div><div class="line">example, label = tf.decode_csv(value, record_defaults=[[<span class="string">'null'</span>], [<span class="string">'null'</span>]])</div><div class="line"><span class="comment"># 运行Graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()  <span class="comment">#创建一个协调器，管理线程</span></div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)  <span class="comment">#启动QueueRunner, 此时文件名队列已经进队。</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example.eval()   <span class="comment">#取样本的时候，一个Reader先从文件名队列中取出文件名，读出数据，Decoder解析后进入样本队列。</span></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line"><span class="comment"># outpt</span></div><div class="line">Alpha1</div><div class="line">Alpha2</div><div class="line">Alpha3</div><div class="line">Bee1</div><div class="line">Bee2</div><div class="line">Bee3</div><div class="line">Sea1</div><div class="line">Sea2</div><div class="line">Sea3</div><div class="line">Alpha1</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="单个Reader，多个样本"><a href="#单个Reader，多个样本" class="headerlink" title="单个Reader，多个样本"></a>单个Reader，多个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">example, label = tf.decode_csv(value, record_defaults=[[<span class="string">'null'</span>], [<span class="string">'null'</span>]])</div><div class="line"><span class="comment"># 使用tf.train.batch()会多加了一个样本队列和一个QueueRunner。Decoder解后数据会进入这个队列，再批量出队。</span></div><div class="line"><span class="comment"># 虽然这里只有一个Reader，但可以设置多线程，通过在tf.train.batch()中添加“num_threads="，相应增加线程数会提高读取速度，但并不是线程越多越好。</span></div><div class="line">example_batch, label_batch = tf.train.batch(</div><div class="line">      [example, label], batch_size=<span class="number">5</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example_batch.eval()</div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line"><span class="comment"># output</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div><div class="line"><span class="comment"># ['Bee3' 'Sea1' 'Sea2' 'Sea3' 'Alpha1']</span></div><div class="line"><span class="comment"># ['Alpha2' 'Alpha3' 'Bee1' 'Bee2' 'Bee3']</span></div><div class="line"><span class="comment"># ['Sea1' 'Sea2' 'Sea3' 'Alpha1' 'Alpha2']</span></div><div class="line"><span class="comment"># ['Alpha3' 'Bee1' 'Bee2' 'Bee3' 'Sea1']</span></div><div class="line"><span class="comment"># ['Sea2' 'Sea3' 'Alpha1' 'Alpha2' 'Alpha3']</span></div><div class="line"><span class="comment"># ['Bee1' 'Bee2' 'Bee3' 'Sea1' 'Sea2']</span></div><div class="line"><span class="comment"># ['Sea3' 'Alpha1' 'Alpha2' 'Alpha3' 'Bee1']</span></div><div class="line"><span class="comment"># ['Bee2' 'Bee3' 'Sea1' 'Sea2' 'Sea3']</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="多个Reader，多个样本"><a href="#多个Reader，多个样本" class="headerlink" title="多个Reader，多个样本"></a>多个Reader，多个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">record_defaults = [[<span class="string">'null'</span>], [<span class="string">'null'</span>]]</div><div class="line">example_list = [tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line">                  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]  <span class="comment"># Reader设置为2</span></div><div class="line"><span class="comment"># 使用tf.train.batch_join()，可以使用多个reader，并行读取数据。每个Reader使用一个线程。</span></div><div class="line">example_batch, label_batch = tf.train.batch_join(</div><div class="line">      example_list, batch_size=<span class="number">5</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example_batch.eval()</div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line">    </div><div class="line"><span class="comment"># output</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div><div class="line"><span class="comment"># ['Bee3' 'Sea1' 'Sea2' 'Sea3' 'Alpha1']</span></div><div class="line"><span class="comment"># ['Alpha2' 'Alpha3' 'Bee1' 'Bee2' 'Bee3']</span></div><div class="line"><span class="comment"># ['Sea1' 'Sea2' 'Sea3' 'Alpha1' 'Alpha2']</span></div><div class="line"><span class="comment"># ['Alpha3' 'Bee1' 'Bee2' 'Bee3' 'Sea1']</span></div><div class="line"><span class="comment"># ['Sea2' 'Sea3' 'Alpha1' 'Alpha2' 'Alpha3']</span></div><div class="line"><span class="comment"># ['Bee1' 'Bee2' 'Bee3' 'Sea1' 'Sea2']</span></div><div class="line"><span class="comment"># ['Sea3' 'Alpha1' 'Alpha2' 'Alpha3' 'Bee1']</span></div><div class="line"><span class="comment"># ['Bee2' 'Bee3' 'Sea1' 'Sea2' 'Sea3']</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p><code>tf.train.batch</code>与<code>tf.train.shuffle_batch</code>函数是单个Reader读取，但是可以多线程，通过设置<code>num_threads</code>参数来设置多线程。<code>tf.train.batch_join</code>与<code>tf.train.shuffle_batch_join</code>可设置多Reader读取，每个Reader使用一个线程。至于两种方法的效率，单Reader时，2个线程就达到了速度的极限。多Reader时，2个Reader就达到了极限。所以并不是线程越多越快，甚至更多的线程反而会使效率下降。</p>
<p>上述两种方法，前者相比于后者的好处是：</p>
<ul>
<li>避免了两个不同的线程从同一个文件中读取同一个样本。</li>
<li>避免了过多的磁盘搜索操作。</li>
</ul>
<p>那么具体需要多少个读取线程呢？ 函数<code>tf.train.shuffle_batch*</code>为<code>graph</code>提供了获取文件名队列中的元素个数之和的方法。 如果你有足够多的读取线程， 文件名队列中的元素个数之和应该一直是一个略高于0的数。具体可以参考TensorBoard的教程。</p>
<p><br></p>
<h2 id="创建线程并使用QueueRunner对象来获取"><a href="#创建线程并使用QueueRunner对象来获取" class="headerlink" title="创建线程并使用QueueRunner对象来获取"></a>创建线程并使用QueueRunner对象来获取</h2><p>我们要添加<code>tf.train.QueueRunner</code>对象到数据流图中，在运行任何训练步骤之前，需要调用<code>tf.train.start_queue_runners</code>函数，否则数据流图将一直挂起，该函数将会启动输入管道的线程，填充样本到队列中，以便出队操作可以从队列中拿到样本。这种情况下最好配合使用一个<code>tf.train.Coordinator</code>，这样可以在发生错误的情况下正确地关闭这些线程。如果我们对训练迭代数做了限制，那么需要使用一个训练迭代数计数器，并且需要初始化它。推荐的代码模板如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create the graph, etc.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Create a session for running operations in the Graph.</span></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line"><span class="comment"># Initialize the variables (like the epoch counter).</span></div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"><span class="comment"># Start input enqueue threads.</span></div><div class="line">coord = tf.train.Coordinator()</div><div class="line">threads = tf.train.start_queue_runners(sess=sess, coord=coord)</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">        <span class="comment"># Run training steps or whatever</span></div><div class="line">        sess.run(train_op)</div><div class="line"></div><div class="line"><span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">    print(<span class="string">'Done training -- epoch limit reached'</span>)</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    <span class="comment"># When done, ask the threads to stop.</span></div><div class="line">    coord.request_stop()</div><div class="line"></div><div class="line"><span class="comment"># Wait for threads to finish.</span></div><div class="line">coord.join(threads)</div><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p><br></p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170704205652_Agg5Zf_AnimatedFileQueues.gif" alt="Animated File Queues" width="900"></center>

<p>如上图所示，每个<code>QueueRunner</code>负责一个阶段，处理那些需要在线程中运行的入队操作的列表。一旦数据流图构造成功，<code>tf.train.start_queue_runners</code>函数就会要求数据流图中每个<code>QueueRunner</code>去开始它的线程运行入队操作。</p>
<p>如果一切顺利的话，我们可以执行训练步骤，同时队列也会被后台线程来填充。如果设置了最大训练迭代数，在某些时候，样本出队的操作可能会得到一个<code>tf.OutOfRangeError</code>的错误。这其实是TensorFlow的“文件结束”（EOF）——这就意味着已经达到了最大训练迭代数，已经没有更多可用的样本了。</p>
<p>最后一个因素是<code>Coordinator</code>。这是负责在收到任何关闭信号的时候，让所有的线程都知道。最常见的情况是在发生异常时，比如说其中一个线程在运行某些操作时出现错误（或一个普通的Python异常）。</p>
<p><br></p>
<h3 id="疑问：在达到最大训练迭代数的时候如何关闭线程"><a href="#疑问：在达到最大训练迭代数的时候如何关闭线程" class="headerlink" title="疑问：在达到最大训练迭代数的时候如何关闭线程?"></a>疑问：在达到最大训练迭代数的时候如何关闭线程?</h3><p>想象一下，我们有一个模型并且设置了最大训练迭代数。这意味着，生成文件的那个线程只会在产生<code>OutOfRange</code>错误之前运行。<code>QueueRunner</code>会捕获该错误，并且关闭文件名的队列，最后退出线程。关闭队列做了两件事情：</p>
<ul>
<li>如果试着对文件名队列执行入队操作将发生错误。</li>
<li>当前或将来的出队操作要么成功（如果队列中还有足够的元素）或立即失败（发生<code>OutOfRange</code>错误）。它们不会等待更多的元素被添加到队列中，因为上面的一点已经保证了这种情况不会发生。</li>
</ul>
<p>关键是，当在文件名队列被关闭时候，有可能还有许多文件名在该队列中，这样下一阶段的流水线（包括reader和其它预处理）还可以继续运行一段时间。 一旦文件名队列空了之后，如果后面的流水线还要尝试从文件名队列中取出一个文件名，这将会触发<code>OutOfRange</code>错误。在这种情况下，即使你可能有一个<code>QueueRunner</code>关联着多个线程，如果该出错线程不是<code>QueueRunner</code>中最后的那个线程，那么<code>OutOfRange</code>错误只会使得这一个线程退出。而其他那些正处理自己的最后一个文件的线程继续运行，直至他们完成为止。（但如果你使用的是<code>tf.train.Coordinator</code>来管理所有的线程，那么其他类型的错误将导致所有线程停止）。一旦所有的reader线程触发<code>OutOfRange</code>错误，样本队列才会被关闭。</p>
<p>同样，样本队列中会有一些已经入队的元素，所以样本训练将一直持续直到样本队列中再没有样本为止。如果样本队列是一个<code>RandomShuffleQueue</code>，因为你使用了<code>shuffle_batch</code> 或者 <code>shuffle_batch_join</code>，所以通常不会出现以往那种队列中的元素会比<code>min_after_dequeue</code> 定义的更少的情况。 然而，一旦该队列被关闭，<code>min_after_dequeue</code>设置的限定值将失效，最终队列将为空。在这一点来说，当实际训练线程尝试从样本队列中取出数据时，将会触发<code>OutOfRange</code>错误，然后训练线程会退出。一旦所有的训练线程完成，<code>tf.train.Coordinator.join</code>会返回，你就可以正常退出了。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/reading_data" target="_blank" rel="external">Reading data  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/extend/new_data_formats" target="_blank" rel="external">Custom Data Readers  |  TensorFlow</a></li>
<li><a href="http://www.cnblogs.com/Charles-Wan/p/6197019.html" target="_blank" rel="external">TF Boys (TensorFlow Boys ) 养成记（二）： TensorFlow 数据读取 - Charles-Wan - 博客园</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer" target="_blank" rel="external">tf.train.string_input_producer  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/batch" target="_blank" rel="external">tf.train.batch  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch" target="_blank" rel="external">tf.train.shuffle_batch  |  TensorFlow</a></li>
<li><a href="http://honggang.io" target="_blank" rel="external">honggang.io</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/reading_data.html#AUTOGENERATED-reading-from-files" target="_blank" rel="external">数据读取 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文整理了TensorFlow中的数据读取方法，在TensorFlow中主要有三种方法读取数据：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feeding：由Python提供数据。&lt;/li&gt;
&lt;li&gt;Preloaded data：预加载数据。&lt;/li&gt;
&lt;li&gt;Reading from files：从文件读取。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（八）：线程和队列</title>
    <link href="http://zangbo.me/2017/07/03/TensorFlow_8/"/>
    <id>http://zangbo.me/2017/07/03/TensorFlow_8/</id>
    <published>2017-07-03T02:11:56.000Z</published>
    <updated>2017-07-05T04:25:39.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文介绍了TensorFlow的线程和队列。在使用TensorFlow进行异步计算时，队列是一种强大的机制。正如TensorFlow中的其他组件一样，队列就是TensorFlow图中的节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，其他节点可以把新元素插入到队列后端(rear)，也可以把队列前端(front)的元素删除。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>为了感受一下队列，先来看一个简单的例子。我们先创建一个“先入先出”的队列（FIFOQueue），并将其内部所有元素初始化为零。然后，我们构建一个TensorFlow图，它从队列前端取走一个元素，加上1之后，放回队列的后端。慢慢地，队列的元素的值就会增加。</p>
<center><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/IncremeterFifoQueue.gif" alt="Incremeter Fifo Queue" width="800"></center>

<p><code>Enqueue</code>、 <code>EnqueueMany</code>和<code>Dequeue</code>都是特殊的节点，在Python API中，它们都是队列对象的方法（例如<code>q.enqueue(...)</code>）。</p>
<p>下面我们深入了解下细节。</p>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>诸如<code>FIFOQueue</code>和<code>RandomShuffleQueue</code>这样的队列，在TensorFlow的<code>tensor</code>异步计算时非常重要。</p>
<p>例如，一个典型的输入结构：使用一个<code>RandomShuffleQueue</code>来作为模型训练的输入：</p>
<ul>
<li>多个线程准备训练样本，并且把这些样本推入队列。</li>
<li>一个训练线程执行一个训练操作，此操作会从队列中移除最小批次的样本（mini-batches)。</li>
</ul>
<p>TensorFlow的<code>Session</code>对象是可以支持多线程的，因此多个线程可以很方便地使用同一个会话（Session）并且并行地执行操作。然而，在Python程序实现这样的并行运算却并不容易。所有线程都必须能被同步终止，异常必须能被正确捕获并报告，会话终止的时候， 队列必须能被正确地关闭。</p>
<p>TensorFlow提供了两个类来帮助多线程的实现：<code>tf.Coordinator</code>和 <code>tf.QueueRunner</code>，通常来说这两个类必须被一起使用。<code>Coordinator</code>类用来同时停止多个工作线程并且向那个在等待所有工作线程终止的程序报告异常。<code>QueueRunner</code>类用来协调多个工作线程并将多个张量推入同一个队列中。</p>
<p><br></p>
<h1 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h1><p><code>Coordinator</code>类用来帮助多个线程协同工作，多个线程同步终止。 其主要方法有：</p>
<ul>
<li><code>should_stop()</code>：如果线程应该停止则返回True。</li>
<li><code>request_stop(&lt;exception&gt;)</code>：请求该线程停止。</li>
<li><code>join(&lt;list of threads&gt;)</code>：等待被指定的线程终止。</li>
</ul>
<p>首先创建一个<code>Coordinator</code>对象，然后建立一些使用<code>Coordinator</code>对象的线程。这些线程通常一直循环运行，每次循环前首先判断<code>should_stop()</code>是否返回<code>True</code>，如果是的话就停止。 任何线程都可以决定什么时候应该停止，它只需要调用<code>request_stop()</code>，同时其他线程的<code>should_stop()</code>将会返回<code>True</code>，然后就都停下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Thread body: loop until the coordinator indicates a stop was requested.</span></div><div class="line"><span class="comment"># If some condition becomes true, ask the coordinator to stop.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">MyLoop</span><span class="params">(coord)</span>:</span></div><div class="line">  <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">    ...do something...</div><div class="line">    <span class="keyword">if</span> ...some condition...:</div><div class="line">      coord.request_stop()</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="comment"># ...</span></div><div class="line">  coord = tf.train.Coordinator()</div><div class="line">  <span class="comment"># Start a number of threads, passing the coordinator to each of them.</span></div><div class="line">  ...start thread <span class="number">1</span> MyLoop(coord)</div><div class="line">  ...start thread N MyLoop(coord)</div><div class="line">  <span class="comment"># Wait for all the threads to terminate.</span></div><div class="line">  coord.join(threads)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">  coord.request_stop()</div><div class="line">coord.join(threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="QueueRunner"><a href="#QueueRunner" class="headerlink" title="QueueRunner"></a>QueueRunner</h1><p><code>QueueRunner</code>类会创建一组线程， 这些线程可以重复的执行Enquene操作， 他们使用同一个<code>Coordinator</code>来处理线程同步终止。此外，一个<code>QueueRunner</code>会运行一个用于异常处理的<em>closer thread</em>，当<code>Coordinator</code>收到异常报告时，这个<em>closer thread</em>会自动关闭队列。</p>
<p>我们可以使用一个一个<code>QueueRunner</code>来实现上述结构。 首先建立一个TensorFlow图表，这个图表使用队列来输入样本，处理样本并将样本推入队列中，用training操作来移除队列中的样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">example = ...ops to create one example...</div><div class="line"><span class="comment"># Create a queue, and an op that enqueues examples one at a time in the queue.</span></div><div class="line">queue = tf.RandomShuffleQueue(...)</div><div class="line">enqueue_op = queue.enqueue(example)</div><div class="line"><span class="comment"># Create a training graph that starts by dequeuing a batch of examples.</span></div><div class="line">inputs = queue.dequeue_many(batch_size)</div><div class="line">train_op = ...use <span class="string">'inputs'</span> to build the training part of the graph...</div></pre></td></tr></table></figure>
<p>在Python的训练程序中，创建一个<code>QueueRunner</code>来运行几个线程， 这几个线程处理样本，并且将样本推入队列。创建一个<code>Coordinator</code>，让queue runner使用<code>Coordinator</code>来开始它的线程，同时创建一个训练的循环， 并且使用<code>Coordinator</code>来控制<code>QueueRunner</code>这些线程的终止。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a queue runner that will run 4 threads in parallel to enqueue</span></div><div class="line"><span class="comment"># examples.</span></div><div class="line">qr = tf.train.QueueRunner(queue, [enqueue_op] * <span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph.</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># Create a coordinator, launch the queue runner threads.</span></div><div class="line">coord = tf.train.Coordinator()</div><div class="line">enqueue_threads = qr.create_threads(sess, coord=coord, start=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Run the training loop, controlling termination with the coordinator.</span></div><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</div><div class="line">    <span class="keyword">if</span> coord.should_stop():</div><div class="line">        <span class="keyword">break</span></div><div class="line">    sess.run(train_op)</div><div class="line"><span class="comment"># When done, ask the threads to stop.</span></div><div class="line">coord.request_stop()</div><div class="line"><span class="comment"># And wait for them to actually do it.</span></div><div class="line">coord.join(enqueue_threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h1><p>通过queue runners启动的线程不仅仅推送样本到队列。它们还捕捉和处理由队列产生的异常，包括<code>OutOfRangeError</code>异常，这个异常是用于报告队列被关闭。 使用<code>Coordinator</code>训练时在主循环中必须同时捕捉和报告异常。 下面是对上面训练循环的改进版本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</div><div class="line">        <span class="keyword">if</span> coord.should_stop():</div><div class="line">            <span class="keyword">break</span></div><div class="line">        sess.run(train_op)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">    <span class="comment"># Report exceptions to the coordinator.</span></div><div class="line">    coord.request_stop(e)</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    <span class="comment"># Terminate as usual. It is safe to call `coord.request_stop()` twice.</span></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/threading_and_queues" target="_blank" rel="external">Threading and Queues  |  TensorFlow</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html" target="_blank" rel="external">线程和队列 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator" target="_blank" rel="external">tf.train.Coordinator  |  TensorFlow</a> </li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文介绍了TensorFlow的线程和队列。在使用TensorFlow进行异步计算时，队列是一种强大的机制。正如TensorFlow中的其他组件一样，队列就是TensorFlow图中的节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，其他节点可以把新元素插入到队列后端(rear)，也可以把队列前端(front)的元素删除。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（七）：Exponential_decay</title>
    <link href="http://zangbo.me/2017/07/01/TensorFlow_7/"/>
    <id>http://zangbo.me/2017/07/01/TensorFlow_7/</id>
    <published>2017-07-01T07:17:57.000Z</published>
    <updated>2017-07-01T08:23:00.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>在神经网络的训练过程中，<strong>学习率(learning rate)</strong>控制着参数的更新速度，<code>tf.train</code>类下面的五种不同的学习速率的衰减方法。</p>
<ul>
<li><code>tf.train.exponential_decay</code></li>
<li><code>tf.train.inverse_time_decay</code></li>
<li><code>tf.train.natural_exp_decay</code></li>
<li><code>tf.train.piecewise_constant</code></li>
<li><code>tf.train.polynomial_decay</code></li>
</ul>
<p>本文只对<code>exponential_decay</code>做整理。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="exponential-decay"><a href="#exponential-decay" class="headerlink" title="exponential_decay"></a>exponential_decay</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tf.train.exponential_decay(</div><div class="line">    learning_rate,</div><div class="line">    global_step,</div><div class="line">    decay_steps,</div><div class="line">    decay_rate,</div><div class="line">    staircase=<span class="keyword">False</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<ul>
<li><code>learning_rate</code>：初始学习率</li>
<li><code>global_step</code>：当前迭代次数</li>
<li><code>decay_steps</code>：衰减迭代次数（在迭代到该次数时学习率衰减为<code>earning_rate * decay_rate</code>）</li>
<li><code>decay_rate</code>：学习率衰减率，通常介于0-1之间。</li>
</ul>
<p>学习率会按照以下公式变化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</div></pre></td></tr></table></figure>
<p>直观解释：假设给定初始学习率<code>learning_rate</code>为0.1，学习率衰减率为0.1，<code>decay_steps</code>为10000，则随着迭代次数从1到10000，当前的学习率<code>decayed_learning_rate</code>慢慢的从0.1降低为<code>0.1*0.1=0.01</code>，当迭代次数到20000，当前的学习率慢慢的从0.01降低为<code>0.1*0.1^2=0.001</code>，以此类推。也就是说每10000次迭代，学习率衰减为前10000次的十分之一，该衰减是连续的，这是在<code>staircase</code>为<code>False</code>的情况下。</p>
<p>如果<code>staircase</code>为<code>True</code>，则<code>global_step / decay_steps</code>始终取整数，也就是说衰减是突变的，每<code>decay_steps</code>次变化一次，变化曲线是阶梯状。</p>
<p>例子：每100000次迭代衰减一次，学习率衰减率为0.96。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### ...</span></div><div class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</div><div class="line">starter_learning_rate = <span class="number">0.1</span></div><div class="line">learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, <span class="number">100000</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Passing global_step to minimize() will increment it at each step.</span></div><div class="line">learning_step = (</div><div class="line">    tf.train.GradientDescentOptimizer(learning_rate)</div><div class="line">    .minimize(...my loss..., global_step=global_step)</div><div class="line">)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong>这里的<code>global_step</code>变量的<code>trainable</code>要设置为<code>False</code>，它代表着当前的迭代次数，我们不能对它进行训练，系统会自动更新它的值，初始化为0，从1开始。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay" target="_blank" rel="external">tf.train.exponential_decay  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;在神经网络的训练过程中，&lt;strong&gt;学习率(learning rate)&lt;/strong&gt;控制着参数的更新速度，&lt;code&gt;tf.train&lt;/code&gt;类下面的五种不同的学习速率的衰减方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.train.exponential_decay&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.inverse_time_decay&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.natural_exp_decay&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.piecewise_constant&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.polynomial_decay&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文只对&lt;code&gt;exponential_decay&lt;/code&gt;做整理。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（六）：Moving Average</title>
    <link href="http://zangbo.me/2017/07/01/TensorFlow_6/"/>
    <id>http://zangbo.me/2017/07/01/TensorFlow_6/</id>
    <published>2017-07-01T03:08:19.000Z</published>
    <updated>2017-07-15T06:54:03.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了TensorFlow训练模型过程中对参数的<strong>滑动平均(moving average)</strong>的计算，在测试数据上评估模型性能时用这些平均值总会提升预测结果表现，用到的类主要为<code>tf.train.ExponentialMovingAverage</code>。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="MovingAverage"><a href="#MovingAverage" class="headerlink" title="MovingAverage"></a>MovingAverage</h1><p>常规的滑动平均的计算方法十分简单，对于一个给定的数列，首先设定一个固定的值k，然后分别计算第1项到第k项，第2项到第k+1项，第3项到第k+2项的平均值，依次类推。</p>
<p>以<code>1、2、3、4、5</code>共5个数为例，window为3，计算过程为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(1+2+3)/3=2</div><div class="line">(2+3+4)/3=3</div><div class="line">(3+4+5)/3=4</div></pre></td></tr></table></figure>
<p>下图很好的反映了原始数据和滑动平均之间的关系，其中绿线为原始数据，红线为MovingAverage：</p>
<ul>
<li>当window为3:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_TyB8hT_MovingAverage1.jpeg" alt="window=3" width="700"></p>
<ul>
<li>当window为10:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_UWRLDA_MovingAverage2.jpeg" alt="window=10" width="700"></p>
<p>可以发现当我们使用滑动平均时，会十分有效的提升模型在测试数据上的<strong>健壮性(robustness)</strong>。</p>
<p><br></p>
<h1 id="ExponentialMovingAverage"><a href="#ExponentialMovingAverage" class="headerlink" title="ExponentialMovingAverage"></a>ExponentialMovingAverage</h1><p>在TensorFlow中，我们计算的是<strong>指数滑动平均(ExponentialMovingAverage)</strong>，我们通过使用一个<strong>指数衰减(exponential decay)</strong>来维持着变量的滑动平均。</p>
<p>当我们训练一个模型时，计算训练参数的滑动平均经常是十分有利的，当我们用这些平均后的参数来评估模型时有时会得到比使用常规的训练参数好很多的结果。</p>
<p>我们用一个<code>apply()</code>函数返回一个<code>ops</code>来添加变量的一个副本同时得到原变量的滑动平均，它在我们训练模型的时候使用。该<code>ops</code>得到原变量的滑动平均始终是在每一次训练迭代结束后。</p>
<p><code>average()</code>和<code>average_name()</code>函数返回影子变量和它们的名字，它们在我们对测试数据进行模型评估时使用，它们用参数的滑动平均值来代替最终的训练值来对模型进行评估。它们也可以在我们从一个<code>checkpoint file</code>继续开始训练模型时使用。</p>
<p>滑动平均值用一个指数衰减来计算，当我们创建<code>ExponentialMovingAverage</code>对象时会把该<code>decay</code>值输入进去。影子变量的初始化值和原始变量初始化值相同。每个影子变量计算滑动平均值的公式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">shadow_variable = decay * shadow_variable + (1 - decay) * variable</div></pre></td></tr></table></figure>
<p>通常我们定义<code>decay</code>时会让它尽可能接近于1.0，一般来说我们会让它为0.999、0.9999等。</p>
<p>如下是我们训练一个模型时的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create variables.</span></div><div class="line">var0 = tf.Variable(...)</div><div class="line">var1 = tf.Variable(...)</div><div class="line"><span class="comment"># ... use the variables to build a training model...</span></div><div class="line">...</div><div class="line"><span class="comment"># Create an op that applies the optimizer.  This is what we usually</span></div><div class="line"><span class="comment"># would use as a training op.</span></div><div class="line">opt_op = opt.minimize(my_loss, [var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an ExponentialMovingAverage object</span></div><div class="line">ema = tf.train.ExponentialMovingAverage(decay=<span class="number">0.9999</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create the shadow variables, and add ops to maintain moving averages</span></div><div class="line"><span class="comment"># of var0 and var1.</span></div><div class="line">maintain_averages_op = ema.apply([var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an op that will update the moving averages after each training</span></div><div class="line"><span class="comment"># step.  This is what we will use in place of the usual training op.</span></div><div class="line"><span class="keyword">with</span> tf.control_dependencies([opt_op]):</div><div class="line">    training_op = tf.group(maintain_averages_op)</div><div class="line"></div><div class="line"><span class="comment"># ...train the model by running training_op...</span></div></pre></td></tr></table></figure>
<p>当我们使用滑动平均来预测时，有两种用法：</p>
<ol>
<li>用影子变量代替原始变量，使用<code>average()</code>函数来返回给定变量的影子变量。</li>
<li>通过使用影子变量的<code>name</code>来载入<code>checkpoint files</code>，我们在这里使用<code>average_name()</code>函数。对于这种用法有如下例子：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a Saver that loads variables from their saved shadow values.</span></div><div class="line">shadow_var0_name = ema.average_name(var0)</div><div class="line">shadow_var1_name = ema.average_name(var1)</div><div class="line">saver = tf.train.Saver(&#123;shadow_var0_name: var0, shadow_var1_name: var1&#125;)</div><div class="line">saver.restore(...checkpoint filename...)</div><div class="line"><span class="comment"># var0 and var1 now hold the moving average values</span></div></pre></td></tr></table></figure>
<p>详情可以查看<code>tf.train.Saver</code>，下面介绍<code>ExponentialMovingAverage</code>的相关函数。</p>
<p><br></p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul>
<li><code>__init__</code></li>
<li><code>apply</code></li>
<li><code>average</code></li>
<li><code>average_name</code></li>
<li><code>variables_to_restore</code></li>
</ul>
<p><br></p>
<h2 id="init"><a href="#init" class="headerlink" title="__init__"></a>__init__</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">__init__(</div><div class="line">    decay,</div><div class="line">    num_updates=<span class="keyword">None</span>,</div><div class="line">    zero_debias=<span class="keyword">False</span>,</div><div class="line">    name=<span class="string">'ExponentialMovingAverage'</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>创建一个<code>ExponentialMovingAverage</code>对象。</p>
<ul>
<li><code>decay</code>一般取值接近于1.0。</li>
<li><code>num_updates</code>允许<code>dacay</code>值动态的变化，在训练开端<code>dacay</code>速率较低，这使得滑动均值更快，如果有值的话，实际<code>decay</code>速率公式为：<code>min(decay, (1 + num_updates) / (10 + num_updates))</code></li>
<li><code>name</code>将会给<code>apply()</code>中的<code>ops</code>添加一个额外的前置名字。</li>
</ul>
<p><br></p>
<h2 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apply(var_list=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>该函数维护变量的滑动平均，返回一个<code>op</code>来更新所有影子变量。<code>var_list</code>必须是一个变量或者<code>Tensor</code>对象的列表，这个函数创造<code>var_list</code>中所有变量的副本，对于变量副本，初始化值和原变量初始化值相同。变量类型必须是<code>float</code>相关的类型。</p>
<p><code>apply()</code>函数对于不同的<code>var_list</code>可以被调用多次。</p>
<p><br></p>
<h2 id="average"><a href="#average" class="headerlink" title="average"></a>average</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>的滑动平均影子变量，返回类型为<code>Variable</code>。前提是该<code>var</code>使用了<code>apply()</code>函数来维护。</p>
<p><br></p>
<h2 id="average-name"><a href="#average-name" class="headerlink" title="average_name"></a>average_name</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average_name(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>变量的滑动平均影子变量的<code>name</code>。该函数一个典型的应用是在训练过程中计算原始变量的滑动平均，并且在测试时根据影子变量的<code>name</code>恢复出原始变量。</p>
<p>为了恢复原始变量，我们必须知道影子变量的<code>name</code>，影子变量的<code>name</code>和原始变量可以在训练阶段利用<code>Saver()</code>对象来保存，操作为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.train.Saver(&#123;ema.average_name(var): var&#125;)</div></pre></td></tr></table></figure>
<p><code>average_name()</code>函数在<code>apply()</code>函数调用之前或之后都可以使用。</p>
<p><br></p>
<h2 id="variables-to-restore"><a href="#variables-to-restore" class="headerlink" title="variables_to_restore"></a>variables_to_restore</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">variables_to_restore(moving_avg_variables=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>返回一个从<code>restore_name</code>到<code>Variables</code>的映射，如果一个变量有滑动平均值，那么就用该滑动平均影子变量的<code>name</code>来作为<code>restore name</code>，否则，就用原始变量的<code>name</code>。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">variables_to_restore = ema.variables_to_restore()</div><div class="line">saver = tf.train.Saver(variables_to_restore)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage" target="_blank" rel="external">tf.train.ExponentialMovingAverage  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/u014365862/article/details/54380313" target="_blank" rel="external">MovingAverage-滑动平均 - 小鹏的专栏 - 博客频道 - CSDN.NET</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文记录了TensorFlow训练模型过程中对参数的&lt;strong&gt;滑动平均(moving average)&lt;/strong&gt;的计算，在测试数据上评估模型性能时用这些平均值总会提升预测结果表现，用到的类主要为&lt;code&gt;tf.train.ExponentialMovingAverage&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（五）：常用函数和模型的保存与恢复</title>
    <link href="http://zangbo.me/2017/06/30/TensorFlow_5/"/>
    <id>http://zangbo.me/2017/06/30/TensorFlow_5/</id>
    <published>2017-06-30T12:58:59.000Z</published>
    <updated>2017-07-01T05:53:17.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文学习了TensorFlow的一些常用的基础函数，主要为以下几种：</p>
<ul>
<li>tf.group</li>
<li>tf.Graph.control_dependencies</li>
<li>tf.train.Saver</li>
</ul>
<p>同时介绍了训练过程中的模型保存和恢复方法，主要用到<code>tf.train.Saver</code>类。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a>tf.group</h1><p><code>tf.group(inputs)</code>创建一个<code>op</code>把多个<code>ops</code>给组合起来，该<code>op</code>无输出。当该<code>op</code>结束操作，其中的所有<code>ops</code>都会结束。</p>
<p>其中<code>inputs</code>为空或者很多<code>tensors</code>。</p>
<p><br></p>
<h1 id="tf-Graph-control-dependencies"><a href="#tf-Graph-control-dependencies" class="headerlink" title="tf.Graph.control_dependencies"></a>tf.Graph.control_dependencies</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">control_dependencies(control_inputs)</div></pre></td></tr></table></figure>
<p>参数<code>control_inputs</code>是一个包含<code>op</code>或者<code>tensor</code>的列表，该列表内的对象必须在控制区域内的<code>ops</code>之前执行。可以为<code>None</code>来清空控制依赖。</p>
<p>通常用<code>with</code>操作来定义一个区域，在该区域下所有的<code>ops</code>都要在<code>control_inputs</code>执行结束后才能执行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</div><div class="line">  <span class="comment"># `d` and `e` will only run after `a`, `b`, and `c` have executed.</span></div><div class="line">  d = ...</div><div class="line">  e = ...</div></pre></td></tr></table></figure>
<p>多次用<code>with</code>调用该函数会得到叠加的依赖，区域内的<code>ops</code>将会在以上所有层次的<code>control_inputs</code>运行结束后才能得到运行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">    <span class="comment"># Ops constructed here run after `a`, `b`, `c`, and `d`.</span></div></pre></td></tr></table></figure>
<p>我们可以用<code>None</code>来清空控制所有依赖。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies(<span class="keyword">None</span>):</div><div class="line">    <span class="comment"># Ops constructed here run normally, not waiting for either `a` or `b`.</span></div><div class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">      <span class="comment"># Ops constructed here run after `c` and `d`, also not waiting</span></div><div class="line">      <span class="comment"># for either `a` or `b`.</span></div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p>控制依赖起作用的区域内只有<code>ops</code>会被执行，仅仅把一个节点放在该区域是不起作用的。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># WRONG</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  t = tf.matmul(tensor, tensor)</div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created outside the context, so no control</span></div><div class="line">    <span class="comment"># dependency will be added.</span></div><div class="line">    <span class="keyword">return</span> t</div><div class="line"></div><div class="line"><span class="comment"># RIGHT</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created in the context, so a control dependency</span></div><div class="line">    <span class="comment"># will be added.</span></div><div class="line">    <span class="keyword">return</span> tf.matmul(tensor, tensor)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="tf-train-Saver"><a href="#tf-train-Saver" class="headerlink" title="tf.train.Saver"></a>tf.train.Saver</h1><p>在训练过程中我们可能会想到每隔一段训练保存一次模型，一方面为了防止过拟合，另一方面如果训练过程被意外打断还可以从某个保存点继续开始训练。之前已经学习过<code>Variable</code>的概念，今天来学习下如何用<code>tf.train.Saver</code>来保存和恢复一个模型。关于<code>tf.train.Saver</code>更详细的内容请参考官方文档。</p>
<p><code>Saver</code>类添加<code>ops</code>来保存变量到<em>checkpoints</em>或者从<em>checkpoints</em>中恢复变量，它同时提供了一些函数方法来运行这些<code>ops</code>。<em>checkpoints</em>是一种二进制文件，它把<code>variable name</code>和<code>tensor</code>值联系起来，默认的为<code>tf.Variable.name</code>。使用<em>checkpoints</em>最好的方法就是用<code>Saver</code>来载入它。</p>
<p><code>Saver</code>可以自动的给<em>checkpoints</em>文件命名，这使得我们在不同的训练阶段可以保存不同的<em>checkpoints</em>。例如我们可以用训练迭代次数来给<em>checkpoints</em>命名，为了防止训练阶段磁盘空间占用量过多，我们还可以选择只保存最近N个文件，或者每N个小时保存一次文件。</p>
<p><br></p>
<h2 id="保存变量"><a href="#保存变量" class="headerlink" title="保存变量"></a>保存变量</h2><p>用<code>tf.train.Saver()</code>创建一个<code>Saver</code>对象来控制模型中所有的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add an op to initialize the variables.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, initialize the variables, do some work, save the</span></div><div class="line"><span class="comment"># variables to disk.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  sess.run(init_op)</div><div class="line">  <span class="comment"># Do some work with the model.</span></div><div class="line">  <span class="comment"># ..</span></div><div class="line">  <span class="comment"># Save the variables to disk.</span></div><div class="line">  save_path = saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model saved in file: %s"</span> % save_path)</div></pre></td></tr></table></figure>
<p>为了自动给<em>checkpoints</em>文件命名我们可以传入一个<code>global_step</code>值给<code>save()</code>函数。例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">saver.save(sess, 'my-model', global_step=0) ==&gt; filename: 'my-model-0'</div><div class="line">...</div><div class="line">saver.save(sess, 'my-model', global_step=1000) ==&gt; filename: 'my-model-1000'</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="恢复变量"><a href="#恢复变量" class="headerlink" title="恢复变量"></a>恢复变量</h2><p>我们用同一个<code>Saver</code>对象来恢复变量，注意当我们从一个文件恢复变量时我们没必要提前对变量初始化。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and</span></div><div class="line"><span class="comment"># do some work with the model.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Restore variables from disk.</span></div><div class="line">  saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model restored."</span>)</div><div class="line">  <span class="comment"># Do some work with the model</span></div><div class="line">  <span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="保存和恢复部分变量"><a href="#保存和恢复部分变量" class="headerlink" title="保存和恢复部分变量"></a>保存和恢复部分变量</h2><p>如果我们没有给<code>tf.train.Saver()</code>任何参数，它默认保存所有变量，每个变量都和它们的<code>name</code>联系起来。</p>
<p>有时候我们想要在<em>checkpoint</em>文件中给某个变量定义一个具体的<code>name</code>，例如我们想把一个变量取名为<code>&quot;weights&quot;</code>，我们想恢复它的值到一个新的名字叫<code>&quot;param&quot;</code>的变量中。</p>
<p>有时候我们想保存和恢复模型的某一组变量，例如我们训练了一个5层的神经网络，但我们想训练一个新的6层的神经网络，并且从5层的神经网络中恢复参数到新的6层模型的前5层中。</p>
<p>我们可以传入一个变量列表到<code>tf.train.Saver()</code>中，变量在<code>checkpoint</code>文件中的<code>name</code>就是<code>op</code>的<code>name</code>。我们也可以很简单的把<code>names</code>和变量组织成一个Python字典传入<code>tf.train.Saver()</code>中来保存下来，<code>keys</code>是我们使用的<code>names</code>，<code>values</code>是我们的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(..., name=<span class="string">'v1'</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">'v2'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Pass the variables as a dict:</span></div><div class="line">saver = tf.train.Saver(&#123;<span class="string">'v1'</span>: v1, <span class="string">'v2'</span>: v2&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Or pass them as a list.</span></div><div class="line">saver = tf.train.Saver([v1, v2])</div><div class="line"><span class="comment"># Passing a list is equivalent to passing a dict with the variable op names</span></div><div class="line"><span class="comment"># as keys:</span></div><div class="line">saver = tf.train.Saver(&#123;v.op.name: v <span class="keyword">for</span> v <span class="keyword">in</span> [v1, v2]&#125;)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ol>
<li>如果我们想保存不同模型的几组变量，我们可以创建很多个<code>saver</code>对象。而且相同的变量可以存储在不同的<code>saver</code>对象中，它们的值只有在<code>restore()</code>函数执行后才改变。</li>
<li>如果我们想恢复一组变量到模型中，我们必须事先初始化其他变量。</li>
</ol>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="external">tf.train.Saver  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" rel="external">Variables: Creation, Initialization, Saving, and Loading  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文学习了TensorFlow的一些常用的基础函数，主要为以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tf.group&lt;/li&gt;
&lt;li&gt;tf.Graph.control_dependencies&lt;/li&gt;
&lt;li&gt;tf.train.Saver&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时介绍了训练过程中的模型保存和恢复方法，主要用到&lt;code&gt;tf.train.Saver&lt;/code&gt;类。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
</feed>
