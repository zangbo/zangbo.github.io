<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZangBo&#39;s Home</title>
  <subtitle>永远年轻 永远热泪盈眶</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zangbo.me/"/>
  <updated>2017-07-07T08:58:03.000Z</updated>
  <id>http://zangbo.me/</id>
  
  <author>
    <name>ZangBo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow 笔记（十二）：CNN示例代码CIFAR-10分析（下）</title>
    <link href="http://zangbo.me/2017/07/07/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89/"/>
    <id>http://zangbo.me/2017/07/07/TensorFlow 笔记（十二）/</id>
    <published>2017-07-07T07:48:47.000Z</published>
    <updated>2017-07-07T08:58:03.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文接上文，继续学习TensorFlow在CIFAR-10上的教程，该代码主要由以下五部分组成：</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py" target="_blank" rel="external"><code>cifar10_input.py</code></a></td>
<td>读取原始的 CIFAR-10 二进制格式文件</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py" target="_blank" rel="external"><code>cifar10.py</code></a></td>
<td>建立 CIFAR-10 网络模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py" target="_blank" rel="external"><code>cifar10_train.py</code></a></td>
<td>在单块CPU或者GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py" target="_blank" rel="external"><code>cifar10_multi_gpu_train.py</code></a></td>
<td>在多块GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py" target="_blank" rel="external"><code>cifar10_eval.py</code></a></td>
<td>在测试集上评估 CIFAR-10 模型的表现</td>
</tr>
</tbody>
</table>
<p>本次主要学习<code>cifar10_train.py</code>和<code>cifar10.eval.py</code>两个文件，内容分别为训练模型和评估模型，并最终给出实验过程。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">https://www.tensorflow.org/tutorials/deep_cnn</a></p>
<p>代码地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>
<p><br></p>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>这部分代码在<code>cifar10_train.py</code>文件中，实现了用单块GPU训练模型，具体训练过程设计为：</p>
<ul>
<li>共计100万次迭代（自己实验时改成了10万次）</li>
<li>batch_size为128</li>
<li>每10次迭代打印一次训练数据（损失、样本/秒、秒/batch）</li>
<li>每600s保存一次<em>checkpoint</em>文件</li>
<li>每300s对最新的<em>checkpoint</em>文件执行一次评估</li>
<li>每100次迭代保存一次summary</li>
</ul>
<p><br></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""用单块GPU训练CIFAR-10 </span></div><div class="line"></div><div class="line">准确率:</div><div class="line">cifar10_train.py 在10万次迭代后达到 ~86% 准确率 (256 epochs of</div><div class="line">data) 并且用在 cifar10_eval.py </div><div class="line"></div><div class="line">batch_size: 128</div><div class="line"></div><div class="line">System        | Step Time (sec/batch)  |     Accuracy</div><div class="line">------------------------------------------------------------------</div><div class="line">1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)</div><div class="line">1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)</div><div class="line"></div><div class="line">"""</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10</div><div class="line"></div><div class="line"><span class="comment">#作用类似于argparse，通过命令行传参改变训练数据</span></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'train_dir'</span>, <span class="string">'/tmp/cifar10_train'</span>,</div><div class="line">                           <span class="string">"""Directory where to write event logs """</span></div><div class="line">                           <span class="string">"""and checkpoint."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'max_steps'</span>, <span class="number">1000000</span>,</div><div class="line">                            <span class="string">"""Number of batches to run."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'log_device_placement'</span>, <span class="keyword">False</span>,</div><div class="line">                            <span class="string">"""Whether to log device placement."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'log_frequency'</span>, <span class="number">10</span>,</div><div class="line">                            <span class="string">"""How often to log results to the console."""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""训练 CIFAR-10 数据集."""</span></div><div class="line">  <span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">    <span class="comment"># 返回或创建全局迭代张量（是一个不会被训练的变量）</span></div><div class="line">    global_step = tf.contrib.framework.get_or_create_global_step()</div><div class="line"></div><div class="line">    <span class="comment"># 获得CIFAR-10的训练数据和标签</span></div><div class="line">    <span class="comment"># 强迫输入管道在 CPU:0 上操作避免有时候操作在GPU上会停止并导致运行变慢</span></div><div class="line">    <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</div><div class="line">      images, labels = cifar10.distorted_inputs()</div><div class="line"></div><div class="line">    <span class="comment"># 用模型的接口函数inference()建立Graph并且计算logits</span></div><div class="line">    logits = cifar10.inference(images)</div><div class="line"></div><div class="line">    <span class="comment"># 计算损失</span></div><div class="line">    loss = cifar10.loss(logits, labels)</div><div class="line"></div><div class="line">    <span class="comment"># 建立 Graph 并用一个batch的数据来训练模型并更新参数</span></div><div class="line">    train_op = cifar10.train(loss, global_step)</div><div class="line"></div><div class="line">    <span class="class"><span class="keyword">class</span> <span class="title">_LoggerHook</span><span class="params">(tf.train.SessionRunHook)</span>:</span></div><div class="line">      <span class="string">"""打印损失和运行时间"""</span></div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">begin</span><span class="params">(self)</span>:</span></div><div class="line">        self._step = <span class="number">-1</span></div><div class="line">        self._start_time = time.time()</div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">before_run</span><span class="params">(self, run_context)</span>:</span></div><div class="line">        self._step += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> tf.train.SessionRunArgs(loss)  <span class="comment"># 计算损失</span></div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">after_run</span><span class="params">(self, run_context, run_values)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._step % FLAGS.log_frequency == <span class="number">0</span>:</div><div class="line">          current_time = time.time()</div><div class="line">          duration = current_time - self._start_time</div><div class="line">          self._start_time = current_time</div><div class="line"></div><div class="line">          loss_value = run_values.results</div><div class="line">          <span class="comment"># 计算每秒钟训练了多少个样本</span></div><div class="line">          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration </div><div class="line">          <span class="comment"># 计算每次迭代用了多长时间</span></div><div class="line">          sec_per_batch = float(duration / FLAGS.log_frequency) </div><div class="line"></div><div class="line">          format_str = (<span class="string">'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '</span></div><div class="line">                        <span class="string">'sec/batch)'</span>)</div><div class="line">          <span class="keyword">print</span> (format_str % (datetime.now(), self._step, loss_value,</div><div class="line">                               examples_per_sec, sec_per_batch))</div><div class="line"></div><div class="line">    <span class="comment"># 开启一个会话执行训练过程</span></div><div class="line">    <span class="comment"># tf.train.NanTensorHook(loss)：监控loss，如果loss为NaN则停止训练</span></div><div class="line">    <span class="keyword">with</span> tf.train.MonitoredTrainingSession(</div><div class="line">        checkpoint_dir=FLAGS.train_dir,</div><div class="line">        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),</div><div class="line">               tf.train.NanTensorHook(loss),</div><div class="line">               _LoggerHook()],</div><div class="line">        config=tf.ConfigProto(</div><div class="line">            log_device_placement=FLAGS.log_device_placement)) <span class="keyword">as</span> mon_sess:</div><div class="line">      <span class="keyword">while</span> <span class="keyword">not</span> mon_sess.should_stop(): <span class="comment"># 如果没有到最大迭代次数</span></div><div class="line">        mon_sess.run(train_op) <span class="comment"># 执行训练过程</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span> </div><div class="line">  cifar10.maybe_download_and_extract() <span class="comment"># 下载数据并解压缩</span></div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(FLAGS.train_dir):</div><div class="line">    tf.gfile.DeleteRecursively(FLAGS.train_dir)</div><div class="line">  tf.gfile.MakeDirs(FLAGS.train_dir)</div><div class="line">  train()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  tf.app.run()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">tf.train.MonitoredTrainingSession(</div><div class="line">    master=<span class="string">''</span>,</div><div class="line">    is_chief=<span class="keyword">True</span>,</div><div class="line">    checkpoint_dir=<span class="keyword">None</span>,</div><div class="line">    scaffold=<span class="keyword">None</span>,</div><div class="line">    hooks=<span class="keyword">None</span>,</div><div class="line">    chief_only_hooks=<span class="keyword">None</span>,</div><div class="line">    save_checkpoint_secs=<span class="number">600</span>,</div><div class="line">    save_summaries_steps=<span class="number">100</span>,</div><div class="line">    save_summaries_secs=<span class="keyword">None</span>,</div><div class="line">    config=<span class="keyword">None</span>,</div><div class="line">    stop_grace_period_secs=<span class="number">120</span>,</div><div class="line">    log_step_count_steps=<span class="number">100</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>每600s保存一次<em>checkpoint</em>，每100s保存一次<em>summary</em>。</p>
<p><br></p>
<h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>这部分代码在<code>cifar10_eval.py</code>中，默认每300s执行一次评估，具体流程：</p>
<ul>
<li><code>evaluate()</code>负责创建和维护整个评估过程： </li>
</ul>
<ol>
<li>获得测试数据</li>
<li>搭建神经网络模型（和训练过程一样） </li>
<li>创建<code>saver</code> ，<code>saver</code>负责恢复<code>shadow variable</code>的值并赋给<code>variable</code></li>
<li>每隔固定的间隔(300s)，运行一次<code>eval_once()</code></li>
</ol>
<ul>
<li><code>eval_once()</code>负责完成一次评估，步骤是： </li>
</ul>
<ol>
<li>从checkpoint中取出最新模型 </li>
<li>运行<code>saver.restore</code>从<code>checkpoint</code>中恢复<code>shadow variable</code>的值并赋给<code>variable</code>。</li>
<li>运行神经网络，对测试集的数据按批次进行预测</li>
<li>计算整个测试集的预测精度</li>
</ol>
<p><br></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""Evaluation for CIFAR-10.</span></div><div class="line"></div><div class="line">准确率:</div><div class="line">cifar10_train.py 在10万次迭代后达到 ~86% 准确率 (256 epochs of</div><div class="line">data) 并且用在 cifar10_eval.py </div><div class="line"></div><div class="line">速度:</div><div class="line">在单块 Tesla K40 中, cifar10_train.py 运行一个batch有128张图片在 0.25-0.35 sec (i.e. 350 - 600 images /sec). </div><div class="line">该模型在10万次迭代后达到 ~86% 准确率，训练过程耗时8小时。</div><div class="line">"""</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'eval_dir'</span>, <span class="string">'/tmp/cifar10_eval'</span>,</div><div class="line">                           <span class="string">"""Directory where to write event logs."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'eval_data'</span>, <span class="string">'test'</span>,</div><div class="line">                           <span class="string">"""Either 'test' or 'train_eval'."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'checkpoint_dir'</span>, <span class="string">'/tmp/cifar10_train'</span>,</div><div class="line">                           <span class="string">"""Directory where to read model checkpoints."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'eval_interval_secs'</span>, <span class="number">60</span> * <span class="number">5</span>,</div><div class="line">                            <span class="string">"""How often to run the eval."""</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'num_examples'</span>, <span class="number">10000</span>,</div><div class="line">                            <span class="string">"""Number of examples to run."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'run_once'</span>, <span class="keyword">False</span>,</div><div class="line">                         <span class="string">"""Whether to run eval only once."""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_once</span><span class="params">(saver, summary_writer, top_k_op, summary_op)</span>:</span></div><div class="line">  <span class="string">"""运行一次评估</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    saver: Saver.</div><div class="line">    summary_writer: Summary writer.</div><div class="line">    top_k_op: Top K op.</div><div class="line">    summary_op: Summary op.</div><div class="line">  """</div><div class="line">  <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</div><div class="line">      <span class="comment"># 从checkpoint恢复变量的值</span></div><div class="line">      saver.restore(sess, ckpt.model_checkpoint_path)</div><div class="line">      <span class="comment"># model_checkpoint_path提取最新的checkpoint文件名，看起来如下:</span></div><div class="line">      <span class="comment"># /my-favorite-path/cifar10_train/model.ckpt-0</span></div><div class="line">      <span class="comment"># 从中提取出global_step </span></div><div class="line">      global_step = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      print(<span class="string">'No checkpoint file found'</span>)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    <span class="comment"># 开始队列</span></div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">      threads = []</div><div class="line">      <span class="keyword">for</span> qr <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):</div><div class="line">        threads.extend(qr.create_threads(sess, coord=coord, daemon=<span class="keyword">True</span>,</div><div class="line">                                         start=<span class="keyword">True</span>))</div><div class="line"></div><div class="line">      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size)) <span class="comment"># 总的迭代数目</span></div><div class="line">      true_count = <span class="number">0</span>  <span class="comment"># 统计预测正确的数目</span></div><div class="line">      total_sample_count = num_iter * FLAGS.batch_size <span class="comment"># 总的样本数目</span></div><div class="line">      step = <span class="number">0</span></div><div class="line">      <span class="keyword">while</span> step &lt; num_iter <span class="keyword">and</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">        predictions = sess.run([top_k_op])</div><div class="line">        true_count += np.sum(predictions)</div><div class="line">        step += <span class="number">1</span></div><div class="line"></div><div class="line">      <span class="comment"># 计算准确率 @ 1.</span></div><div class="line">      precision = true_count / total_sample_count</div><div class="line">      print(<span class="string">'%s: precision @ 1 = %.3f'</span> % (datetime.now(), precision))</div><div class="line"></div><div class="line">      summary = tf.Summary()</div><div class="line">      summary.ParseFromString(sess.run(summary_op))</div><div class="line">      summary.value.add(tag=<span class="string">'Precision @ 1'</span>, simple_value=precision)</div><div class="line">      summary_writer.add_summary(summary, global_step)</div><div class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:  </div><div class="line">      coord.request_stop(e)</div><div class="line"></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads, stop_grace_period_secs=<span class="number">10</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""Eval CIFAR-10 for a number of steps."""</span></div><div class="line">  <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">    <span class="comment"># 从CIFAR-10中获取图像数据和标签数据</span></div><div class="line">    eval_data = FLAGS.eval_data == <span class="string">'test'</span></div><div class="line">    images, labels = cifar10.inputs(eval_data=eval_data)</div><div class="line"></div><div class="line">    <span class="comment"># 建立一个Graph来计算logits</span></div><div class="line">    logits = cifar10.inference(images)</div><div class="line"></div><div class="line">    <span class="comment"># 计算预测值，输出一个batch_size大小的bool数组</span></div><div class="line">    top_k_op = tf.nn.in_top_k(logits, labels, <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 恢复训练变量的滑动平均值来评估模型</span></div><div class="line">    variable_averages = tf.train.ExponentialMovingAverage(</div><div class="line">        cifar10.MOVING_AVERAGE_DECAY)</div><div class="line">    variables_to_restore = variable_averages.variables_to_restore()</div><div class="line">    saver = tf.train.Saver(variables_to_restore)</div><div class="line"></div><div class="line">    summary_op = tf.summary.merge_all()</div><div class="line"></div><div class="line">    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)</div><div class="line"></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">      eval_once(saver, summary_writer, top_k_op, summary_op)</div><div class="line">      <span class="keyword">if</span> FLAGS.run_once:</div><div class="line">        <span class="keyword">break</span></div><div class="line">      time.sleep(FLAGS.eval_interval_secs)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span>  </div><div class="line">  cifar10.maybe_download_and_extract()</div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(FLAGS.eval_dir):</div><div class="line">    tf.gfile.DeleteRecursively(FLAGS.eval_dir)</div><div class="line">  tf.gfile.MakeDirs(FLAGS.eval_dir)</div><div class="line">  evaluate()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  tf.app.run()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tf.nn.in_top_k(</div><div class="line">    predictions,</div><div class="line">    targets,</div><div class="line">    k,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>判断<code>targets</code>是否在top k的预测之中。输出<code>batch_size</code>大小的bool数组，如果对目标累的预测在所有预测的top k中，则<code>out[i]=True</code>。</p>
<p><br></p>
<h1 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h1><p>作者在单块Tesla K40中训练了10万次用了8小时（350 - 600 images/sec），我在单快Quadro M5000上只用了46分钟（4800～5000 images/sec），下面是训练过程：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">2017-07-07 15:30:08.459355: step 0, loss = 4.68 (317.5 examples/sec; 0.403 sec/batch)</div><div class="line">2017-07-07 15:30:08.794469: step 10, loss = 4.62 (3819.4 examples/sec; 0.034 sec/batch)</div><div class="line">2017-07-07 15:30:09.067413: step 20, loss = 4.49 (4689.6 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:09.343335: step 30, loss = 4.45 (4638.9 examples/sec; 0.028 sec/batch)</div><div class="line">2017-07-07 15:30:09.618720: step 40, loss = 4.31 (4648.0 examples/sec; 0.028 sec/batch)</div><div class="line">2017-07-07 15:30:09.889763: step 50, loss = 4.32 (4722.5 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.162925: step 60, loss = 4.26 (4685.9 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.436191: step 70, loss = 4.07 (4684.1 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.702081: step 80, loss = 4.20 (4814.0 examples/sec; 0.027 sec/batch)</div><div class="line">2017-07-07 15:30:10.963494: step 90, loss = 4.26 (4896.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 15:30:11.442152: step 100, loss = 4.08 (2674.1 examples/sec; 0.048 sec/batch)</div><div class="line">...</div><div class="line">2017-07-07 16:16:08.694992: step 99900, loss = 0.67 (3468.2 examples/sec; 0.037 sec/batch)</div><div class="line">2017-07-07 16:16:08.952094: step 99910, loss = 0.71 (4978.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.211538: step 99920, loss = 0.65 (4933.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.472046: step 99930, loss = 0.76 (4913.5 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.728118: step 99940, loss = 0.81 (4998.6 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:09.986376: step 99950, loss = 0.77 (4956.3 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:10.241033: step 99960, loss = 0.56 (5026.4 examples/sec; 0.025 sec/batch)</div><div class="line">2017-07-07 16:16:10.496853: step 99970, loss = 0.71 (5003.5 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:10.760321: step 99980, loss = 0.64 (4858.3 examples/sec; 0.026 sec/batch)</div><div class="line">2017-07-07 16:16:11.018312: step 99990, loss = 0.76 (4961.4 examples/sec; 0.026 sec/batch)</div></pre></td></tr></table></figure>
<p>训练和评估过程是放在两个程序分开进行的，具体的实现方法是，在训练过程中，为每个训练变量添加指数滑动平均变量，然后每600s就将模型训练到的变量值保存在<em>checkpoint</em>中，评估过程运行时，从最新存储的<em>checkpoint</em>中取出模型的<code>shadow variable</code>，赋值给对应的变量，然后进行评估。</p>
<p>我们需要同时运行两个程序才能实时的对训练过程进行评估，否则得到的永远只是最新的<em>checkpoint</em>文件中的评估结果。具体可以先运行<code>python cifar_train.py</code> ，再打开另一个窗口运行<code>python cifar_eval.py</code> 。</p>
<p>官方给的代码最大迭代次数是100万，我运行的时候改成了10万。</p>
<p>因为我的迭代速度太快了，到600s时第一次保存<em>checkpoint</em>就已经是两万多次迭代了，可以通过修改<code>tf.train.MonitoredTrainingSession()</code>函数的<code>save_checkpoint_secs</code>参数来修改保存<em>checkpoint</em>的时间间隔，默认600s。</p>
<p>最终10万次迭代后的评估准确率是86.2%，和官方给出的数据还是吻合的。</p>
<p>最后来张TensorBoard的图：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170707164106_Jfe0c9_total_loss.jpeg" alt="Total Loss"></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks  |  TensorFlow</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10 and CIFAR-100 datasets</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/image" target="_blank" rel="external">Images  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/strided_slice" target="_blank" rel="external">tf.strided_slice  |  TensorFlow</a></li>
<li><a href="https://segmentfault.com/a/1190000008793389" target="_blank" rel="external">TensorFlow学习笔记（11）：数据操作指南 - 数据实验室 - SegmentFault</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="external">tf.nn.local_response_normalization  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession" target="_blank" rel="external">tf.train.MonitoredTrainingSession  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文接上文，继续学习TensorFlow在CIFAR-10上的教程，该代码主要由以下五部分组成：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_input.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;读取原始的 CIFAR-10 二进制格式文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;建立 CIFAR-10 网络模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_train.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在单块CPU或者GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_multi_gpu_train.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在多块GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_eval.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在测试集上评估 CIFAR-10 模型的表现&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;本次主要学习&lt;code&gt;cifar10_train.py&lt;/code&gt;和&lt;code&gt;cifar10.eval.py&lt;/code&gt;两个文件，内容分别为训练模型和评估模型，并最终给出实验过程。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十一）：CNN示例代码CIFAR-10分析（上）</title>
    <link href="http://zangbo.me/2017/07/05/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89/"/>
    <id>http://zangbo.me/2017/07/05/TensorFlow 笔记（十一）/</id>
    <published>2017-07-05T12:28:12.000Z</published>
    <updated>2017-07-07T08:46:14.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>TensorFlow官方网站关于卷积神经网络的教程有具体实例，该实例在CIFAR-10数据集上实现，我对这部分代码进行了学习，该代码主要由以下五部分组成：</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py" target="_blank" rel="external"><code>cifar10_input.py</code></a></td>
<td>读取原始的 CIFAR-10 二进制格式文件</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py" target="_blank" rel="external"><code>cifar10.py</code></a></td>
<td>建立 CIFAR-10 网络模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py" target="_blank" rel="external"><code>cifar10_train.py</code></a></td>
<td>在单块CPU或者GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py" target="_blank" rel="external"><code>cifar10_multi_gpu_train.py</code></a></td>
<td>在多块GPU上训练 CIFAR-10 模型</td>
</tr>
<tr>
<td><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py" target="_blank" rel="external"><code>cifar10_eval.py</code></a></td>
<td>在测试集上评估 CIFAR-10 模型的表现</td>
</tr>
</tbody>
</table>
<p>本次只对单GPU情况进行学习，对多GPU不做学习。本次学习分上下两部分，本文首先介绍<code>cifar10_input.py</code>、<code>cifar10.py</code>两个函数，内容分别为数据的获取和模型的建立，同时我们还介绍了本次教程的重点和CIFAR-10数据集。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>教程地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">https://www.tensorflow.org/tutorials/deep_cnn</a></p>
<p>代码地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>
<p><br></p>
<h1 id="教程重点"><a href="#教程重点" class="headerlink" title="教程重点"></a>教程重点</h1><p>该教程主要实现了以下几个用TensorFlow设计大型且复杂网络模型时的重要构造：</p>
<ul>
<li>核心的运算包括卷积(convolution)、relu激活(rectified linear activations)、最大池化(max poolnig)和局部响应归一化(LRN)。</li>
<li>网络训练过程中的可视化操作</li>
<li>对学习到的参数计算滑动平均值(moving average)，并且在评估模型表现时使用它们。</li>
<li>实现学习率衰减策略来训练，采用指数衰减(exponential_decay)方式。</li>
<li>使用队列(queues)操作获取输入数据。</li>
</ul>
<p><br></p>
<h1 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h1><p>CIFAR-10 数据集分类是机器学习领域很经典的任务，该任务旨在把32x32的RGB图像分成十类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.</div></pre></td></tr></table></figure>
<p>我们这里用到的是二进制数据，该数据共有六个文件，其中五个训练数据文件，文件名为：<code>data_batch_1.bin</code>,…, <code>data_batch_5.bin</code>，一个测试数据文件，名为<code>test_batch.bin</code>。每个文件里包含10000个样本，共计50000个训练样本，10000个测试样本。</p>
<p>文件中数据结构如下（文件中并没具体的划分行）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;1 x label&gt;&lt;3072 x pixel&gt;</div><div class="line">...</div><div class="line">&lt;1 x label&gt;&lt;3072 x pixel&gt;</div></pre></td></tr></table></figure>
<p>第一个字节代表着标签，范围0-9分别代表十类。接下来的3072个字节代表着图像像素值，前1024个字节是red通道的值，接着1024个字节是green通道值，最后1024个字节是blue通道值。字节排列是以行为主的顺序，也就是说前32个字节代表red通道下第一行的图像数据。每个文件由10000个3073字节组成，理论上来说每个文件包含30730000字节长的数据。</p>
<p><br></p>
<h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><p>这部分的代码在<code>cifar10_input.py</code>文件中，该代码主要由四个函数组成：</p>
<ul>
<li><code>read_cifar10()</code>：从文件名队列读取二进制数据并提取出单张图片数据。</li>
<li><code>_generate_image_and_label_batch()</code>：利用单张图片数据生成<code>batch</code>数据。</li>
<li><code>distorted_inputs()</code>：构建训练数据并进行预处理。</li>
<li><code>inputs()</code>：构建测试数据并进行预处理（也可以用在训练集上）。</li>
</ul>
<p>给外部调用的主要是后两个函数，分别生成训练数据和测试数据。</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 如果是python2的代码，会有不兼容，这里把python3的特性引入，使得python2也可以运行该代码。</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import <span class="comment"># 绝对引用</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division <span class="comment"># 精确除法</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function <span class="comment"># print函数</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="comment"># xrange返回一个生成器，range返回一个列表，xrange在生成大范围数据的时候更节省内存。</span></div><div class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">IMAGE_SIZE = <span class="number">24</span> <span class="comment"># 图像尺寸</span></div><div class="line"></div><div class="line"><span class="comment"># 描述 CIFAR-10 数据的全局常量。</span></div><div class="line">NUM_CLASSES = <span class="number">10</span> <span class="comment"># 类别数目</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = <span class="number">50000</span> <span class="comment"># 训练样本数目</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = <span class="number">10000</span> <span class="comment"># 测试样本数目</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_cifar10</span><span class="params">(filename_queue)</span>:</span></div><div class="line">    </div><div class="line">  <span class="string">"""从文件名队列读取二进制数据并提取出单张图像数据</span></div><div class="line"></div><div class="line">  建议: </div><div class="line">    如果想N个线程同步读取, 调用这个函数N次即可。</div><div class="line">    这将会产生N个独立的Readers从不同文件不同位置读取数据，进而产生更好的样本混合效果。</div><div class="line">    </div><div class="line">  输入参数:</div><div class="line">    filename_queue: 一个包含文件名列表的字符串队列</div><div class="line"></div><div class="line">  返回一个类，包含了单张图像的各种数据。</div><div class="line">  """</div><div class="line"></div><div class="line">  <span class="class"><span class="keyword">class</span> <span class="title">CIFAR10Record</span><span class="params">(object)</span>:</span> <span class="comment"># 定义一个类</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line">  result = CIFAR10Record() <span class="comment"># 建立类的实体对象</span></div><div class="line"></div><div class="line">  </div><div class="line">  <span class="comment"># 输入数据格式</span></div><div class="line">  label_bytes = <span class="number">1</span>  </div><div class="line">  result.height = <span class="number">32</span></div><div class="line">  result.width = <span class="number">32</span></div><div class="line">  result.depth = <span class="number">3</span></div><div class="line">  image_bytes = result.height * result.width * result.depth</div><div class="line">  </div><div class="line">  record_bytes = label_bytes + image_bytes</div><div class="line"></div><div class="line">  <span class="comment"># 定义固定长度阅读器读取长度为record_bytes的数据，从文件名队列中获取文件并读出单张图像数据。</span></div><div class="line">  <span class="comment"># CIFAR-10格式数据没有头数据和尾数据，所以我们令 header_bytes 和 footer_bytes 保持默认值0。</span></div><div class="line">  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)</div><div class="line">  result.key, value = reader.read(filename_queue)</div><div class="line"></div><div class="line">  <span class="comment"># 使用解码器把字符串类型转化为uint8类型的数据</span></div><div class="line">  record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line"></div><div class="line">  <span class="comment"># 第一个字节代表着标签数据，所以我们把它的格式从uint8转化为int32。</span></div><div class="line">  <span class="comment"># tf.strided_slice(input_, begin, end, strides)</span></div><div class="line">  result.label = tf.cast(</div><div class="line">      tf.strided_slice(record_bytes, [<span class="number">0</span>], [label_bytes]), tf.int32)</div><div class="line"></div><div class="line">  <span class="comment"># 剩下的字节表示图像数据，我们首先根据CIFAR-10的数据排列[depth * height * width] </span></div><div class="line">  <span class="comment"># 把它转化为 [depth, height, width] 形状的张量。</span></div><div class="line">  depth_major = tf.reshape(</div><div class="line">      tf.strided_slice(record_bytes, [label_bytes],</div><div class="line">                       [label_bytes + image_bytes]),</div><div class="line">      [result.depth, result.height, result.width])</div><div class="line">  <span class="comment"># 把 [depth, height, width] 转化为 [height, width, depth] 形状的张量，这是TensorFlow处理图像的格式。</span></div><div class="line">  result.uint8image = tf.transpose(depth_major, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</div><div class="line"></div><div class="line">  <span class="keyword">return</span> result</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_generate_image_and_label_batch</span><span class="params">(image, label, min_queue_examples, batch_size, shuffle)</span>:</span></div><div class="line">  <span class="string">"""生成图像和标签的batch数据</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    image: 3-D Tensor of [height, width, 3] of type.float32.</div><div class="line">    label: 1-D Tensor of type.int32</div><div class="line">    min_queue_examples: int32, 每次出队后队伍中剩下的样本数量的最小值。</div><div class="line">    batch_size: 每个batch的图像数量。</div><div class="line">    shuffle: boolean值表明是否对图像队列随机排序。</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, height, width, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  <span class="comment"># 创造一个样本队列，根据需求决定是否对样本随机排序，每次从队列中出队batch_size个图像数据和标签数据</span></div><div class="line">  num_preprocess_threads = <span class="number">16</span> </div><div class="line">  <span class="comment">#16个Reader平行读取，每个Reader读不同的文件或者位置，可以充分的混合样本数据</span></div><div class="line">  <span class="keyword">if</span> shuffle: <span class="comment"># 对样本队列随机排序</span></div><div class="line">    images, label_batch = tf.train.shuffle_batch(</div><div class="line">        [image, label],</div><div class="line">        batch_size=batch_size,</div><div class="line">        num_threads=num_preprocess_threads,</div><div class="line">        capacity=min_queue_examples + <span class="number">3</span> * batch_size,</div><div class="line">        min_after_dequeue=min_queue_examples)</div><div class="line">  <span class="keyword">else</span>: <span class="comment"># 不对样本队列随机排序</span></div><div class="line">    images, label_batch = tf.train.batch(</div><div class="line">        [image, label],</div><div class="line">        batch_size=batch_size,</div><div class="line">        num_threads=num_preprocess_threads,</div><div class="line">        capacity=min_queue_examples + <span class="number">3</span> * batch_size)</div><div class="line"></div><div class="line">  <span class="comment"># 添加summary节点以便在TensorBoard中对图像信息进行可视化</span></div><div class="line">  tf.summary.image(<span class="string">'images'</span>, images)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> images, tf.reshape(label_batch, [batch_size])</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">distorted_inputs</span><span class="params">(data_dir, batch_size)</span>:</span></div><div class="line">  <span class="string">""" 构造训练数据并对其进行失真处理。</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    data_dir: CIFAR-10数据的存放路径.</div><div class="line">    batch_size: 每个batch中图像的数目.</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  filenames = [os.path.join(data_dir, <span class="string">'data_batch_%d.bin'</span> % i)</div><div class="line">               <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">6</span>)]</div><div class="line">  <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line"></div><div class="line">  <span class="comment"># 创建一个文件名队列</span></div><div class="line">  filename_queue = tf.train.string_input_producer(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># 调用read_cifar10函数从文件名队列中读取文件，并得到单张图像信息</span></div><div class="line">  read_input = read_cifar10(filename_queue)</div><div class="line">  reshaped_image = tf.cast(read_input.uint8image, tf.float32)</div><div class="line"></div><div class="line">  height = IMAGE_SIZE <span class="comment"># 24</span></div><div class="line">  width = IMAGE_SIZE <span class="comment"># 24</span></div><div class="line"></div><div class="line">  <span class="comment"># 对训练数据图像预处理，包括多个随机失真操作。</span></div><div class="line"></div><div class="line">  <span class="comment"># 把原始的32*32的图像随机裁剪为24*24</span></div><div class="line">  distorted_image = tf.random_crop(reshaped_image, [height, width, <span class="number">3</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 随机左右翻转</span></div><div class="line">  distorted_image = tf.image.random_flip_left_right(distorted_image)</div><div class="line"></div><div class="line">  <span class="comment"># 随机改变图像亮度和对比度</span></div><div class="line">  distorted_image = tf.image.random_brightness(distorted_image,</div><div class="line">                                               max_delta=<span class="number">63</span>)</div><div class="line">  distorted_image = tf.image.random_contrast(distorted_image,</div><div class="line">                                             lower=<span class="number">0.2</span>, upper=<span class="number">1.8</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 把图像进行归一化处理，变为0均值和1方差。</span></div><div class="line">  float_image = tf.image.per_image_standardization(distorted_image)</div><div class="line"></div><div class="line">  <span class="comment"># 有时候graph没法推断出tensors的形状，我们可以手动保存tensors的形状信息</span></div><div class="line">  float_image.set_shape([height, width, <span class="number">3</span>]) </div><div class="line">  read_input.label.set_shape([<span class="number">1</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 当采用随机生成batch操作时设定min_after_dequeue的值为50000*0.4=20000，保证足够的随机性。</span></div><div class="line">  min_fraction_of_examples_in_queue = <span class="number">0.4</span></div><div class="line">  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *</div><div class="line">                           min_fraction_of_examples_in_queue)</div><div class="line">  <span class="keyword">print</span> (<span class="string">'Filling queue with %d CIFAR images before starting to train. '</span></div><div class="line">         <span class="string">'This will take a few minutes.'</span> % min_queue_examples)</div><div class="line"></div><div class="line">  <span class="comment"># 通过建立样本队列来生成batch图像数据和batch标签数据</span></div><div class="line">  <span class="keyword">return</span> _generate_image_and_label_batch(float_image, read_input.label, </div><div class="line">                                         min_queue_examples, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(eval_data, data_dir, batch_size)</span>:</span></div><div class="line">  <span class="string">"""构造测试数据.</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    eval_data: bool型，表明使用训练数据还是测试数据，True为测试集</div><div class="line">    data_dir: CIFAR-10数据集的存放路径</div><div class="line">    batch_size: 每个batch的图片数量</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> eval_data: <span class="comment"># 训练数据</span></div><div class="line">    filenames = [os.path.join(data_dir, <span class="string">'data_batch_%d.bin'</span> % i)</div><div class="line">                 <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">6</span>)]</div><div class="line">    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN</div><div class="line">  <span class="keyword">else</span>: <span class="comment"># 测试数据</span></div><div class="line">    filenames = [os.path.join(data_dir, <span class="string">'test_batch.bin'</span>)]</div><div class="line">    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL</div><div class="line"></div><div class="line">  <span class="keyword">for</span> f <span class="keyword">in</span> filenames: <span class="comment"># 检查文件是否存在</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line"></div><div class="line">  <span class="comment"># 创建一个文件名队列</span></div><div class="line">  filename_queue = tf.train.string_input_producer(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># 从文件名队列中的文件中读取单个样本</span></div><div class="line">  read_input = read_cifar10(filename_queue)</div><div class="line">  reshaped_image = tf.cast(read_input.uint8image, tf.float32)</div><div class="line"></div><div class="line">  height = IMAGE_SIZE</div><div class="line">  width = IMAGE_SIZE</div><div class="line"></div><div class="line">  <span class="comment"># 测试时的图像预处理</span></div><div class="line">  <span class="comment"># 沿中心裁剪28*28大小的图像</span></div><div class="line">  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, height, width)</div><div class="line"></div><div class="line">  <span class="comment"># 图像归一化处理</span></div><div class="line">  float_image = tf.image.per_image_standardization(resized_image)</div><div class="line"></div><div class="line">  <span class="comment"># 设置张量的形状</span></div><div class="line">  float_image.set_shape([height, width, <span class="number">3</span>])</div><div class="line">  read_input.label.set_shape([<span class="number">1</span>])</div><div class="line"></div><div class="line">  <span class="comment"># 设置min_after_dequeue参数为20000(训练数据)，4000(测试数据)，保证足够随机性。</span></div><div class="line">  min_fraction_of_examples_in_queue = <span class="number">0.4</span></div><div class="line">  min_queue_examples = int(num_examples_per_epoch * min_fraction_of_examples_in_queue)</div><div class="line"></div><div class="line">  <span class="comment"># 通过建立一个样本队列生成batch图像数据和batch标签数据</span></div><div class="line">  <span class="keyword">return</span> _generate_image_and_label_batch(float_image, read_input.label, </div><div class="line">                                         min_queue_examples, batch_size, shuffle=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><ol>
<li><p><code>tf.strided_slice(input_, begin, end, strides)</code></p>
<p>用法示例(注意和<code>tf.slice()</code>的区分)：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 'input' is [[[1, 1, 1], [2, 2, 2]],</div><div class="line">#             [[3, 3, 3], [4, 4, 4]],</div><div class="line">#             [[5, 5, 5], [6, 6, 6]]]</div><div class="line">tf.strided_slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3]]]</div><div class="line">tf.strided_slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3],</div><div class="line">                                                               [4, 4, 4]]]</div><div class="line">tf.strided_slice(input, [1, -1, 0], [2, -3, 3], [1, -1, 1]) ==&gt;[[[4, 4, 4],</div><div class="line">                                                              [3, 3, 3]]]</div></pre></td></tr></table></figure>
</li>
<li><p>TensorFlow提供两种类型的拼接：</p>
<ul>
<li><code>tf.concat(values, axis, name=&#39;concat&#39;)</code>：按照指定的<strong>已经存在</strong>的轴进行拼接</li>
</ul>
<ul>
<li><code>tf.stack(values, axis=0, name=&#39;stack&#39;)</code>：按照指定的<strong>新建</strong>的轴进行拼接</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">t1 = [[1, 2, 3], [4, 5, 6]]</div><div class="line">t2 = [[7, 8, 9], [10, 11, 12]]</div><div class="line">tf.concat([t1, t2], 0) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</div><div class="line">tf.concat([t1, t2], 1) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</div><div class="line">tf.stack([t1, t2], 0)  ==&gt; [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]</div><div class="line">tf.stack([t1, t2], 1)  ==&gt; [[[1, 2, 3], [7, 8, 9]], [[4, 5, 6], [10, 11, 12]]]</div><div class="line">tf.stack([t1, t2], 2)  ==&gt; [[[1, 7], [2, 8], [3, 9]], [[4, 10], [5, 11], [6, 12]]]</div></pre></td></tr></table></figure>
<p>上面的结果读起来不太直观，我们从shape角度看一下就很容易明白了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</div><div class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</div><div class="line">tf.concat([t1, t2], <span class="number">0</span>)  <span class="comment"># [2,3] + [2,3] ==&gt; [4, 3]</span></div><div class="line">tf.concat([t1, t2], <span class="number">1</span>)  <span class="comment"># [2,3] + [2,3] ==&gt; [2, 6]</span></div><div class="line">tf.stack([t1, t2], <span class="number">0</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2*,2,3]</span></div><div class="line">tf.stack([t1, t2], <span class="number">1</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2,2*,3]</span></div><div class="line">tf.stack([t1, t2], <span class="number">2</span>)   <span class="comment"># [2,3] + [2,3] ==&gt; [2,3,2*]</span></div></pre></td></tr></table></figure>
</li>
</ol>
<p><br></p>
<h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><p>这部分代码在<code>cifar10.py</code>文件中，该代码主要由以下函数组成：</p>
<ul>
<li><code>_activation_summary()</code>：为激活函数创造可视化summary</li>
<li><code>_variable_on_cpu()</code>：新建一个变量存储在CPU上</li>
<li><code>_variable_with_weight_decay()</code>：新建一个已经初始化的变量并计算正则化损失</li>
<li><code>distorted_inputs()</code>：获得训练数据</li>
<li><code>inputs()</code>：获得测试数据</li>
<li><code>inference()</code>：搭建神经网络模型，输出Logits</li>
<li><code>loss()</code>：计算总体损失=交叉熵+正则化</li>
<li><code>_add_loss_summaries()</code>：为损失添加可视化summary</li>
<li><code>train()</code>：创建一个optimizer并给所有训练变量添加滑动平均</li>
<li><code>maybe_download_and_extract()</code>：下载并解压训练数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""搭建 CIFAR-10 网络"""</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> tarfile <span class="comment"># 实现文件的压缩与解压缩</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> cifar10_input</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line"><span class="comment"># 基本的模型参数，默认batch_size128</span></div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">'batch_size'</span>, <span class="number">128</span>,</div><div class="line">                            <span class="string">"""Number of images to process in a batch."""</span>)</div><div class="line">tf.app.flags.DEFINE_string(<span class="string">'data_dir'</span>, <span class="string">'/tmp/cifar10_data'</span>,</div><div class="line">                           <span class="string">"""Path to the CIFAR-10 data directory."""</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(<span class="string">'use_fp16'</span>, <span class="keyword">False</span>,</div><div class="line">                            <span class="string">"""Train the model using fp16."""</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设置描述CIFAR-10数据的全局常量</span></div><div class="line">IMAGE_SIZE = cifar10_input.IMAGE_SIZE <span class="comment"># 28</span></div><div class="line">NUM_CLASSES = cifar10_input.NUM_CLASSES <span class="comment"># 10</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN <span class="comment"># 50000</span></div><div class="line">NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL <span class="comment"># 10000</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 训练过程中用到的常量</span></div><div class="line">MOVING_AVERAGE_DECAY = <span class="number">0.9999</span>     <span class="comment"># 计算滑动平均(moving average)时的衰减(decay)</span></div><div class="line">NUM_EPOCHS_PER_DECAY = <span class="number">350.0</span>      <span class="comment"># 学习率衰减的epochs</span></div><div class="line">LEARNING_RATE_DECAY_FACTOR = <span class="number">0.1</span>  <span class="comment"># 学习率衰减因子</span></div><div class="line">INITIAL_LEARNING_RATE = <span class="number">0.1</span>       <span class="comment"># 初始学习率</span></div><div class="line"></div><div class="line"><span class="comment"># 如果一个模型在多GPU上训练, 在Op名字上加上前缀tower_name来区分操作</span></div><div class="line"><span class="comment"># 注意当我们可视化一个模型的时候该前缀会被移去。</span></div><div class="line">TOWER_NAME = <span class="string">'tower'</span></div><div class="line"></div><div class="line">DATA_URL = <span class="string">'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_activation_summary</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="string">"""为激活函数创建summary</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    x: Tensor</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    nothing</div><div class="line">  """</div><div class="line">  <span class="comment"># 如果是一个多GPU训练，就把'tower_[0-9]/'移除，这将会帮助我们更清晰的在TensorBoard上展示。</span></div><div class="line">  tensor_name = re.sub(<span class="string">'%s_[0-9]*/'</span> % TOWER_NAME, <span class="string">''</span>, x.op.name)</div><div class="line">  tf.summary.histogram(tensor_name + <span class="string">'/activations'</span>, x)</div><div class="line">  tf.summary.scalar(tensor_name + <span class="string">'/sparsity'</span>,</div><div class="line">                                       tf.nn.zero_fraction(x)) <span class="comment"># 统计0的比例反映稀疏度</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_variable_on_cpu</span><span class="params">(name, shape, initializer)</span>:</span></div><div class="line">  <span class="string">"""帮助新建一个存储在CPU上的变量</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    name: 变量名</div><div class="line">    shape: 变量形状</div><div class="line">    initializer: 变量的初始化器</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    变量Tensor</div><div class="line">  """</div><div class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</div><div class="line">    dtype = tf.float16 <span class="keyword">if</span> FLAGS.use_fp16 <span class="keyword">else</span> tf.float32</div><div class="line">    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)</div><div class="line">  <span class="keyword">return</span> var</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_variable_with_weight_decay</span><span class="params">(name, shape, stddev, wd)</span>:</span></div><div class="line">  <span class="string">"""帮助新建一个已经初始化的变量并且计算正则化损失</span></div><div class="line"></div><div class="line">  变量初始化采用的 truncated normal 分布</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    name: 变量名</div><div class="line">    shape: 变量形状</div><div class="line">    stddev: truncated Gaussian 分布的标准差</div><div class="line">    wd: 正则化系数，添加正则化损失乘以该系数，如果是None则不添加。</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    Variable Tensor</div><div class="line">  """</div><div class="line">  dtype = tf.float16 <span class="keyword">if</span> FLAGS.use_fp16 <span class="keyword">else</span> tf.float32</div><div class="line">  var = _variable_on_cpu(</div><div class="line">      name,</div><div class="line">      shape,</div><div class="line">      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))</div><div class="line">  <span class="keyword">if</span> wd <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=<span class="string">'weight_loss'</span>)</div><div class="line">    tf.add_to_collection(<span class="string">'losses'</span>, weight_decay) <span class="comment"># 把正则化损失存储为全局变量</span></div><div class="line">  <span class="keyword">return</span> var</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">distorted_inputs</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""获得训练数据，对cifar10_input.py文件里distorted_inputs()进行封装</span></div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line"></div><div class="line">  引发:</div><div class="line">    ValueError: 如果没有data_dir</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.data_dir:</div><div class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Please supply a data_dir'</span>)</div><div class="line">  data_dir = os.path.join(FLAGS.data_dir, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,</div><div class="line">                                                  batch_size=FLAGS.batch_size)</div><div class="line">  <span class="keyword">if</span> FLAGS.use_fp16:</div><div class="line">    images = tf.cast(images, tf.float16)</div><div class="line">    labels = tf.cast(labels, tf.float16)</div><div class="line">  <span class="keyword">return</span> images, labels</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(eval_data)</span>:</span></div><div class="line">  <span class="string">"""获得测试数据，对cifar10_input.py文件里inputs()进行封装</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    eval_data: bool, 指出是否使用测试数据还是训练数据（一般是True）</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    images: batch图像数据. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.</div><div class="line">    labels: batch标签数据. 1D tensor of [batch_size] size.</div><div class="line"></div><div class="line">  引发:</div><div class="line">    ValueError: 如果没有data_dir</div><div class="line">  """</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.data_dir:</div><div class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Please supply a data_dir'</span>)</div><div class="line">  data_dir = os.path.join(FLAGS.data_dir, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  images, labels = cifar10_input.inputs(eval_data=eval_data,</div><div class="line">                                        data_dir=data_dir,</div><div class="line">                                        batch_size=FLAGS.batch_size)</div><div class="line">  <span class="keyword">if</span> FLAGS.use_fp16:</div><div class="line">    images = tf.cast(images, tf.float16)</div><div class="line">    labels = tf.cast(labels, tf.float16)</div><div class="line">  <span class="keyword">return</span> images, labels</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(images)</span>:</span></div><div class="line">  <span class="string">"""搭建 CIFAR-10 模型.</span></div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    images: 从 distorted_inputs() 或 inputs() 中返回的图像数据</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    Logits</div><div class="line">  """</div><div class="line">  <span class="comment"># 我们是用 tf.get_variable() 而不是 tf.Variable() 来新建变量以便在多GPU训练中共享变量</span></div><div class="line">  <span class="comment"># 如果我们只是在单块GPU上训练，可以简化 tf.get_variable() 变成tf.Variable()</span></div><div class="line"></div><div class="line">  <span class="comment"># conv1</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv1'</span>) <span class="keyword">as</span> scope:</div><div class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</div><div class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>],</div><div class="line">                                         stddev=<span class="number">5e-2</span>,</div><div class="line">                                         wd=<span class="number">0.0</span>)</div><div class="line">    conv = tf.nn.conv2d(images, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    pre_activation = tf.nn.bias_add(conv, biases) <span class="comment"># tf.add()的特例，该函数中biases只能是1-D维度的</span></div><div class="line">    conv1 = tf.nn.relu(pre_activation, name=scope.name)</div><div class="line">    _activation_summary(conv1)</div><div class="line"></div><div class="line">  <span class="comment"># pool1</span></div><div class="line">  pool1 = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                         padding=<span class="string">'SAME'</span>, name=<span class="string">'pool1'</span>)</div><div class="line">  <span class="comment"># norm1</span></div><div class="line">  norm1 = tf.nn.lrn(pool1, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>,</div><div class="line">                    name=<span class="string">'norm1'</span>)</div><div class="line"></div><div class="line">  <span class="comment"># conv2</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv2'</span>) <span class="keyword">as</span> scope:</div><div class="line">    kernel = _variable_with_weight_decay(<span class="string">'weights'</span>,</div><div class="line">                                         shape=[<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>],</div><div class="line">                                         stddev=<span class="number">5e-2</span>,</div><div class="line">                                         wd=<span class="number">0.0</span>)</div><div class="line">    conv = tf.nn.conv2d(norm1, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">64</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    pre_activation = tf.nn.bias_add(conv, biases)</div><div class="line">    conv2 = tf.nn.relu(pre_activation, name=scope.name)</div><div class="line">    _activation_summary(conv2)</div><div class="line"></div><div class="line">  <span class="comment"># norm2</span></div><div class="line">  norm2 = tf.nn.lrn(conv2, <span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>,</div><div class="line">                    name=<span class="string">'norm2'</span>)</div><div class="line">  <span class="comment"># pool2</span></div><div class="line">  pool2 = tf.nn.max_pool(norm2, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</div><div class="line">                         strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=<span class="string">'pool2'</span>)</div><div class="line"></div><div class="line">  <span class="comment"># local3</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local3'</span>) <span class="keyword">as</span> scope:</div><div class="line">    <span class="comment"># Move everything into depth so we can perform a single matrix multiply.</span></div><div class="line">    reshape = tf.reshape(pool2, [FLAGS.batch_size, <span class="number">-1</span>])</div><div class="line">    dim = reshape.get_shape()[<span class="number">1</span>].value</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[dim, <span class="number">384</span>],</div><div class="line">                                          stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">384</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)</div><div class="line">    _activation_summary(local3)</div><div class="line"></div><div class="line">  <span class="comment"># local4</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'local4'</span>) <span class="keyword">as</span> scope:</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, shape=[<span class="number">384</span>, <span class="number">192</span>],</div><div class="line">                                          stddev=<span class="number">0.04</span>, wd=<span class="number">0.004</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [<span class="number">192</span>], tf.constant_initializer(<span class="number">0.1</span>))</div><div class="line">    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)</div><div class="line">    _activation_summary(local4)</div><div class="line"></div><div class="line">  <span class="comment"># 线性层(WX + b)</span></div><div class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax_linear'</span>) <span class="keyword">as</span> scope:</div><div class="line">    weights = _variable_with_weight_decay(<span class="string">'weights'</span>, [<span class="number">192</span>, NUM_CLASSES],</div><div class="line">                                          stddev=<span class="number">1</span>/<span class="number">192.0</span>, wd=<span class="number">0.0</span>)</div><div class="line">    biases = _variable_on_cpu(<span class="string">'biases'</span>, [NUM_CLASSES],</div><div class="line">                              tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)</div><div class="line">    _activation_summary(softmax_linear)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> softmax_linear</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(logits, labels)</span>:</span></div><div class="line">  <span class="string">"""添加L2正则化损失到所有的训练变量中</span></div><div class="line"></div><div class="line">  为 "Loss" 和 "Loss/avg" 添加可视化summary</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    logits: 从 inference() 得到的logits</div><div class="line">    labels: 从 distorted_inputs() 或 inputs() 得到的标签数据. 1-D tensor 形状为 [batch_size]</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    float类型的损失tensor.</div><div class="line">  """</div><div class="line">  <span class="comment"># 计算batch的平均交叉熵损失</span></div><div class="line">  <span class="comment"># 如果是label是one-hot码则用tf.nn.softmax_cross_entropy_with_logits()</span></div><div class="line">  <span class="comment"># 这里label是从0-9的数字来表示，则用tf.nn.sparse_softmax_cross_entropy_with_logits()</span></div><div class="line">  labels = tf.cast(labels, tf.int64)</div><div class="line">  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</div><div class="line">      labels=labels, logits=logits, name=<span class="string">'cross_entropy_per_example'</span>)</div><div class="line">  cross_entropy_mean = tf.reduce_mean(cross_entropy, name=<span class="string">'cross_entropy'</span>)</div><div class="line">  tf.add_to_collection(<span class="string">'losses'</span>, cross_entropy_mean)</div><div class="line"></div><div class="line">  <span class="comment"># 总的损失是交叉熵损失加上所有变量的L2正则化损失</span></div><div class="line">  <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">'losses'</span>), name=<span class="string">'total_loss'</span>) </div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_loss_summaries</span><span class="params">(total_loss)</span>:</span></div><div class="line">  <span class="string">"""为损失添加可视化summary</span></div><div class="line"></div><div class="line">  为所有的损失生成滑动平均并且关联summaries来可视化网络表现。</div><div class="line"></div><div class="line">  输入参数:</div><div class="line">    total_loss: 从loss()函数得到的总损失.</div><div class="line"></div><div class="line">  返回数据:</div><div class="line">    loss_averages_op: 生成损失滑动平均的op</div><div class="line">  """</div><div class="line">  <span class="comment"># 为所有的独立损失和总的损失计算滑动平均</span></div><div class="line">  loss_averages = tf.train.ExponentialMovingAverage(<span class="number">0.9</span>, name=<span class="string">'avg'</span>)</div><div class="line">  losses = tf.get_collection(<span class="string">'losses'</span>) <span class="comment"># 得到包含独立loss的列表</span></div><div class="line">  <span class="comment"># 把每个独立loss和总的loss合并成一个列表，并计算滑动平均</span></div><div class="line">  loss_averages_op = loss_averages.apply(losses + [total_loss])</div><div class="line"></div><div class="line">  <span class="comment"># 添加一个scalar summary给所有的独立损失和总体损失以及它们的滑动平均</span></div><div class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> losses + [total_loss]:</div><div class="line">    <span class="comment"># 给每个损失命名'(raw)'同时给滑动平均损失原始的命名</span></div><div class="line">    tf.summary.scalar(l.op.name + <span class="string">' (raw)'</span>, l)</div><div class="line">    tf.summary.scalar(l.op.name, loss_averages.average(l))</div><div class="line"></div><div class="line">  <span class="keyword">return</span> loss_averages_op</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(total_loss, global_step)</span>:</span></div><div class="line">  <span class="string">"""训练 CIFAR-10 模型.</span></div><div class="line"></div><div class="line">  新建一个优化器并且应用到所有的训练变量上，给所有的训练变量添加滑动平均</div><div class="line">  用tf.control_dependencies控制着执行的先后顺序</div><div class="line">  执行顺序如下：</div><div class="line">  计算损失的滑动平均——&gt;计算学习率——&gt;计算梯度——&gt;更新参数——&gt;计算训练变量的滑动平均——&gt;train_op(返回的值)</div><div class="line">  通过执行train_op来依次执行之前的所有步骤</div><div class="line">  </div><div class="line">  输入参数:</div><div class="line">    total_loss: 从loss()得到的总损失.</div><div class="line">    global_step: 整型Variable来计算迭代次数</div><div class="line"></div><div class="line">  输出数据:</div><div class="line">    train_op: 训练的op.</div><div class="line">  """</div><div class="line">  <span class="comment"># 影响学习率的变量.</span></div><div class="line">  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size</div><div class="line">  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)</div><div class="line"></div><div class="line">  <span class="comment"># 学习率随着迭代次数指数衰减</span></div><div class="line">  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,</div><div class="line">                                  global_step,</div><div class="line">                                  decay_steps,</div><div class="line">                                  LEARNING_RATE_DECAY_FACTOR,</div><div class="line">                                  staircase=<span class="keyword">True</span>)</div><div class="line">  tf.summary.scalar(<span class="string">'learning_rate'</span>, lr)</div><div class="line"></div><div class="line">  <span class="comment"># 对所有损失生成滑动平均并且关联可视化summaries.</span></div><div class="line">  loss_averages_op = _add_loss_summaries(total_loss)</div><div class="line"></div><div class="line">  <span class="comment"># 计算梯度（先执行生成滑动损失的操作loss_averages_op，再计算梯度）。</span></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([loss_averages_op]):</div><div class="line">    opt = tf.train.GradientDescentOptimizer(lr)</div><div class="line">    grads = opt.compute_gradients(total_loss)</div><div class="line"></div><div class="line">  <span class="comment"># 应用梯度更新参数</span></div><div class="line">  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有训练变量添加可视化histograms</span></div><div class="line">  <span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</div><div class="line">    tf.summary.histogram(var.op.name, var)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有梯度添加可视化histograms</span></div><div class="line">  <span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</div><div class="line">    <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      tf.summary.histogram(var.op.name + <span class="string">'/gradients'</span>, grad)</div><div class="line"></div><div class="line">  <span class="comment"># 为所有训练变量添加滑动平均</span></div><div class="line">  variable_averages = tf.train.ExponentialMovingAverage(</div><div class="line">      MOVING_AVERAGE_DECAY, global_step)</div><div class="line">  variables_averages_op = variable_averages.apply(tf.trainable_variables())</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([apply_gradient_op, variables_averages_op]):</div><div class="line">    train_op = tf.no_op(name=<span class="string">'train'</span>) <span class="comment"># 什么也不做，只是为了确保上面两个op的执行</span></div><div class="line"></div><div class="line">  <span class="keyword">return</span> train_op</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download_and_extract</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""下载并解压缩数据"""</span></div><div class="line">  dest_directory = FLAGS.data_dir</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dest_directory):</div><div class="line">    os.makedirs(dest_directory)</div><div class="line">  filename = DATA_URL.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</div><div class="line">  filepath = os.path.join(dest_directory, filename)</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filepath):</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_progress</span><span class="params">(count, block_size, total_size)</span>:</span></div><div class="line">      sys.stdout.write(<span class="string">'\r&gt;&gt; Downloading %s %.1f%%'</span> % (filename,</div><div class="line">          float(count * block_size) / float(total_size) * <span class="number">100.0</span>))</div><div class="line">      sys.stdout.flush()</div><div class="line">    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)</div><div class="line">    print()</div><div class="line">    statinfo = os.stat(filepath)</div><div class="line">    print(<span class="string">'Successfully downloaded'</span>, filename, statinfo.st_size, <span class="string">'bytes.'</span>)</div><div class="line">  extracted_dir_path = os.path.join(dest_directory, <span class="string">'cifar-10-batches-bin'</span>)</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(extracted_dir_path):</div><div class="line">    tarfile.open(filepath, <span class="string">'r:gz'</span>).extractall(dest_directory)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>这里介绍下LRN层，也就是<code>local response normalization</code>，局部响应归一化。最早是由Krizhevsky和Hinton在论文《ImageNet Classification with Deep Convolutional Neural Networks》里面使用的一种数据标准化方法。这种方法是受到神经科学的启发，激活的神经元会抑制其邻近神经元的活动（侧抑制现象），至于为什么使用这种正则手段，以及它为什么有效，查阅了很多文献似乎也没有详细的解释。加上后来BN层太火，LRN用的就比较少了。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks  |  TensorFlow</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10 and CIFAR-100 datasets</a></li>
<li><a href="https://www.tensorflow.org/api_guides/python/image" target="_blank" rel="external">Images  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/strided_slice" target="_blank" rel="external">tf.strided_slice  |  TensorFlow</a></li>
<li><a href="https://segmentfault.com/a/1190000008793389" target="_blank" rel="external">TensorFlow学习笔记（11）：数据操作指南 - 数据实验室 - SegmentFault</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="external">tf.nn.local_response_normalization  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession" target="_blank" rel="external">tf.train.MonitoredTrainingSession  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;TensorFlow官方网站关于卷积神经网络的教程有具体实例，该实例在CIFAR-10数据集上实现，我对这部分代码进行了学习，该代码主要由以下五部分组成：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_input.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;读取原始的 CIFAR-10 二进制格式文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;建立 CIFAR-10 网络模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_train.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在单块CPU或者GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_multi_gpu_train.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在多块GPU上训练 CIFAR-10 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;cifar10_eval.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在测试集上评估 CIFAR-10 模型的表现&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;本次只对单GPU情况进行学习，对多GPU不做学习。本次学习分上下两部分，本文首先介绍&lt;code&gt;cifar10_input.py&lt;/code&gt;、&lt;code&gt;cifar10.py&lt;/code&gt;两个函数，内容分别为数据的获取和模型的建立，同时我们还介绍了本次教程的重点和CIFAR-10数据集。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（十）：张量的阶、形状和数据类型</title>
    <link href="http://zangbo.me/2017/07/05/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89/"/>
    <id>http://zangbo.me/2017/07/05/TensorFlow 笔记（十）/</id>
    <published>2017-07-05T09:00:56.000Z</published>
    <updated>2017-07-05T09:23:17.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文整理了TensorFlow中<strong>张量(Tensor)</strong>相关的一些概念：</p>
<ul>
<li>阶(Rank)</li>
<li>形状(Shape)</li>
<li>数据类型(Type)</li>
</ul>
<p>TensorFlow用张量这种数据结构来表示所有的数据，我们可以把一个张量想象成一个n维的数组或列表。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="阶-Rank"><a href="#阶-Rank" class="headerlink" title="阶(Rank)"></a>阶(Rank)</h1><p>在TensorFlow系统中，张量的维数来被描述为阶(Rank)。张量的阶和矩阵的阶并不是同一个概念，张量的阶是张量维数的一个数量描述。比如下面的张量（使用Python中list定义的）就是2阶。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">t = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</div></pre></td></tr></table></figure>
<p>可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量。对于一个二阶张量我们可以用语句<code>t[i, j]</code>来访问其中的任何元素。而对于三阶张量你可以用<code>t[i, j, k]</code>来访问其中的任何元素。</p>
<p>维数越靠后，位置越靠里。比如上面的二阶张量，<code>t[1, 3] = 3</code>。维数越靠前，位置越靠外，比如第一维度的数据，是最外层中括号的数据。</p>
<table>
<thead>
<tr>
<th>阶</th>
<th>数学实例</th>
<th>Python 例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>纯量 (只有大小)</td>
<td><code>s = 483</code></td>
</tr>
<tr>
<td>1</td>
<td>向量(大小和方向)</td>
<td><code>v = [1.1, 2.2, 3.3]</code></td>
</tr>
<tr>
<td>2</td>
<td>矩阵(数据表)</td>
<td><code>m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</code></td>
</tr>
<tr>
<td>3</td>
<td>3阶张量 (数据立体)</td>
<td><code>t = [[[2], [4], [6]], [[8], [10], [12]], [[14], [16], [18]]]</code></td>
</tr>
<tr>
<td>n</td>
<td>n阶 (自己想想看)</td>
<td><code>....</code></td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="形状-Shape"><a href="#形状-Shape" class="headerlink" title="形状(Shape)"></a>形状(Shape)</h1><p>TensorFlow中使用了三种记号来方便地描述张量的维度：阶，形状以及维数。下表展示了它们之间的关系：</p>
<table>
<thead>
<tr>
<th>阶</th>
<th>形状</th>
<th>维数</th>
<th>实例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[ ]</td>
<td>0-D</td>
<td>一个0维张量，一个纯量。</td>
</tr>
<tr>
<td>1</td>
<td>[D0]</td>
<td>1-D</td>
<td>一个1维张量的形式[5]</td>
</tr>
<tr>
<td>2</td>
<td>[D0, D1]</td>
<td>2-D</td>
<td>一个2维张量的形式[3, 4]</td>
</tr>
<tr>
<td>3</td>
<td>[D0, D1, D2]</td>
<td>3-D</td>
<td>一个3维张量的形式 [1, 4, 3]</td>
</tr>
<tr>
<td>n</td>
<td>[D0, D1, … Dn]</td>
<td>n-D</td>
<td>一个n维张量的形式 [D0, D1, … Dn]</td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="数据类型-Type"><a href="#数据类型-Type" class="headerlink" title="数据类型(Type)"></a>数据类型(Type)</h1><p>除了维度，Tensors有一个数据类型属性.你可以为一个张量指定下列数据类型中的任意一个类型：</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>Python 类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DT_FLOAT</code></td>
<td><code>tf.float32</code></td>
<td>32 位浮点数.</td>
</tr>
<tr>
<td><code>DT_DOUBLE</code></td>
<td><code>tf.float64</code></td>
<td>64 位浮点数.</td>
</tr>
<tr>
<td><code>DT_INT64</code></td>
<td><code>tf.int64</code></td>
<td>64 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT32</code></td>
<td><code>tf.int32</code></td>
<td>32 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT16</code></td>
<td><code>tf.int16</code></td>
<td>16 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_INT8</code></td>
<td><code>tf.int8</code></td>
<td>8 位有符号整型.</td>
</tr>
<tr>
<td><code>DT_UINT8</code></td>
<td><code>tf.uint8</code></td>
<td>8 位无符号整型.</td>
</tr>
<tr>
<td><code>DT_STRING</code></td>
<td><code>tf.string</code></td>
<td>可变长度的字节数组，每一个张量元素都是一个字节数组。</td>
</tr>
<tr>
<td><code>DT_BOOL</code></td>
<td><code>tf.bool</code></td>
<td>布尔型</td>
</tr>
<tr>
<td><code>DT_COMPLEX64</code></td>
<td><code>tf.complex64</code></td>
<td>由两个32位浮点数组成的复数：实数和虚数。</td>
</tr>
<tr>
<td><code>DT_QINT32</code></td>
<td><code>tf.qint32</code></td>
<td>用于量化Ops的32位有符号整型</td>
</tr>
<tr>
<td><code>DT_QINT8</code></td>
<td><code>tf.qint8</code></td>
<td>用于量化Ops的8位有符号整型</td>
</tr>
<tr>
<td><code>DT_QUINT8</code></td>
<td><code>tf.quint8</code></td>
<td>用于量化Ops的8位无符号整型</td>
</tr>
</tbody>
</table>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/dims_types" target="_blank" rel="external">Tensor Ranks, Shapes, and Types  |  TensorFlow</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/resources/dims_types.html" target="_blank" rel="external">张量的阶、形状、数据类型 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文整理了TensorFlow中&lt;strong&gt;张量(Tensor)&lt;/strong&gt;相关的一些概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阶(Rank)&lt;/li&gt;
&lt;li&gt;形状(Shape)&lt;/li&gt;
&lt;li&gt;数据类型(Type)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TensorFlow用张量这种数据结构来表示所有的数据，我们可以把一个张量想象成一个n维的数组或列表。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（九）：数据读取</title>
    <link href="http://zangbo.me/2017/07/05/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89/"/>
    <id>http://zangbo.me/2017/07/05/TensorFlow 笔记（九）/</id>
    <published>2017-07-05T02:04:11.000Z</published>
    <updated>2017-07-06T08:06:24.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文整理了TensorFlow中的数据读取方法，在TensorFlow中主要有三种方法读取数据：</p>
<ol>
<li>Feeding：由Python提供数据。</li>
<li>Preloaded data：预加载数据。</li>
<li>Reading from files：从文件读取。</li>
</ol>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Feeding"><a href="#Feeding" class="headerlink" title="Feeding"></a>Feeding</h1><p>我们一般用<code>tf.placeholder</code>节点来<code>feed</code>数据，该节点不需要初始化也不包含任何数据，我们在执行<code>run()</code>或者<code>eval()</code>指令时通过<code>feed_dict</code>参数把数据传入<code>graph</code>中来计算。如果在运行过程中没有对<code>tf.placeholder</code>节点传入数据，程序会报错。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 设计Graph</span></div><div class="line">x1 = tf.placeholder(tf.int16)</div><div class="line">x2 = tf.placeholder(tf.int16)</div><div class="line">y = tf.add(x1, x2)</div><div class="line"><span class="comment"># 用Python产生数据</span></div><div class="line">li1 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</div><div class="line">li2 = [<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>]</div><div class="line"><span class="comment"># 打开一个session --&gt; 喂数据 --&gt; 计算y</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="keyword">print</span> sess.run(y, feed_dict=&#123;x1: li1, x2: li2&#125;)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Preloaded-data"><a href="#Preloaded-data" class="headerlink" title="Preloaded data"></a>Preloaded data</h1><p>预加载数据方法仅限于用在可以完全加载到内存中的小数据集上，主要有两种方法：</p>
<ol>
<li>把数据存在常量（constant）中。</li>
<li>把数据存在变量（variable）中，我们初始化并且永不改变它的值。</li>
</ol>
<p>用常量更简单些，但会占用更多的内存，因为常量存储在<code>graph</code>数据结构内部。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">training_data = ...</div><div class="line">training_labels = ...</div><div class="line"><span class="keyword">with</span> tf.Session():</div><div class="line">  input_data = tf.constant(training_data)</div><div class="line">  input_labels = tf.constant(training_labels)</div><div class="line">  ...</div></pre></td></tr></table></figure>
<p>如果用变量的话，我们需要在<code>graph</code>构建好之后初始化该变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">training_data = ...</div><div class="line">training_labels = ...</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  data_initializer = tf.placeholder(dtype=training_data.dtype,</div><div class="line">                                    shape=training_data.shape)</div><div class="line">  label_initializer = tf.placeholder(dtype=training_labels.dtype,</div><div class="line">                                     shape=training_labels.shape)</div><div class="line">  input_data = tf.Variable(data_initializer, trainable=<span class="keyword">False</span>, collections=[])</div><div class="line">  input_labels = tf.Variable(label_initializer, trainable=<span class="keyword">False</span>, collections=[])</div><div class="line">  ...</div><div class="line">  sess.run(input_data.initializer,</div><div class="line">           feed_dict=&#123;data_initializer: training_data&#125;)</div><div class="line">  sess.run(input_labels.initializer,</div><div class="line">           feed_dict=&#123;label_initializer: training_labels&#125;)</div></pre></td></tr></table></figure>
<p>设定<code>trainable=False</code> 可以防止该变量被数据流图的 <code>GraphKeys.TRAINABLE_VARIABLES</code> 收集, 这样我们就不会在训练的时候尝试更新它的值； 设定 <code>collections=[]</code> 可以防止<code>GraphKeys.VARIABLES</code> 把它收集后做为保存和恢复的中断点。</p>
<p>无论哪种方式，我们可以用<code>tf.train.slice_input_producer</code>函数每次产生一个切片。这样就会让样本在整个迭代中被打乱，所以在使用批处理的时候不需要再次打乱样本。所以我们不使用<code>shuffle_batch</code>函数，取而代之的是纯<code>tf.train.batch</code> 函数。 如果要使用多个线程进行预处理，需要将<code>num_threads</code>参数设置为大于1的数字。</p>
<p><br></p>
<h1 id="Reading-from-files"><a href="#Reading-from-files" class="headerlink" title="Reading from files"></a>Reading from files</h1><p>从文件中读取数据一般包含以下步骤：</p>
<ol>
<li>文件名列表</li>
<li>文件名随机排序（可选的）</li>
<li>迭代控制（可选的）</li>
<li>文件名队列</li>
<li>针对输入文件格式的阅读器</li>
<li>记录解析器</li>
<li>预处理器（可选的）</li>
<li>样本队列</li>
</ol>
<p><br></p>
<h2 id="文件名、随机排序和迭代控制"><a href="#文件名、随机排序和迭代控制" class="headerlink" title="文件名、随机排序和迭代控制"></a>文件名、随机排序和迭代控制</h2><p>我们首先要有个文件名列表，为了产生文件名列表，我们可以手动用Python输入字符串，例如：</p>
<ul>
<li><code>[&quot;file0&quot;, &quot;file1&quot;]</code></li>
<li><code>[(&quot;file%d&quot; % i) for i in range(2)]</code></li>
<li><code>[(&quot;file%d&quot; % i) for i in range(2)]</code></li>
</ul>
<p>我们也可以用<code>tf.train.match_filenames_once</code>函数来生成文件名列表。</p>
<p>有了文件名列表后，我们需要把它送入<code>tf.train.string_input_producer</code>函数中生成一个先入先出的文件名队列，文件阅读器需要从该队列中读取文件名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">string_input_producer(</div><div class="line">    string_tensor,</div><div class="line">    num_epochs=<span class="keyword">None</span>,</div><div class="line">    shuffle=<span class="keyword">True</span>,</div><div class="line">    seed=<span class="keyword">None</span>,</div><div class="line">    capacity=<span class="number">32</span>,</div><div class="line">    shared_name=<span class="keyword">None</span>,</div><div class="line">    name=<span class="keyword">None</span>,</div><div class="line">    cancel_op=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>一个<code>QueueRunner</code>每次会把每批次的所有文件名送入队列中，可以通过设置<code>string_input_producer</code>函数的<code>shuffle</code>参数来对文件名随机排序，或者通过设置<code>num_epochs</code>来决定对<code>string_tensor</code>里的文件使用多少次，类型为整型，如果想要迭代控制则需要设置了<code>num_epochs</code>参数，同时需要添加<code>tf.local_variables_initializer()</code>进行初始化，如果不初始化会报错。</p>
<p>这个<code>QueueRunner</code>的工作线程独立于文件阅读器的线程， 因此随机排序和将文件名送入到文件名队列这些过程不会阻碍文件阅读器的运行。</p>
<p><br></p>
<h2 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h2><p>根据不同的文件格式， 应该选择对应的文件阅读器， 然后将文件名队列提供给阅读器的<code>read</code>方法。阅读器每次从队列中读取一个文件，它的<code>read</code>方法会输出一个<code>key</code>来表征读入的文件和其中的纪录(对于调试非常有用)，同时得到一个字符串标量， 这个字符串标量可以被一个或多个解析器，或者转换操作将其解码为张量并且构造成为样本。</p>
<p>根据不同的文件类型，有三种不同的文件阅读器：</p>
<ul>
<li><code>tf.TextLineReader</code></li>
<li><code>tf.FixedLengthRecordReader</code></li>
<li><code>tf.TFRecordReader</code></li>
</ul>
<p>它们分别用于单行读取(如CSV文件)、固定长度读取(如CIFAR-10的.bin二进制文件)、TensorFlow标准格式读取。</p>
<p>根据不同的文件阅读器，有三种不同的解析器，它们分别对应上面三种阅读器：</p>
<ul>
<li><code>tf.decode_csv</code></li>
<li><code>tf.decode_raw</code></li>
<li><code>tf.parse_single_example</code>和<code>tf.parse_example</code></li>
</ul>
<p><br></p>
<h3 id="CSV文件"><a href="#CSV文件" class="headerlink" title="CSV文件"></a>CSV文件</h3><p>当我们读入CSV格式的文件时，我们可以使用<code>tf.TextLineReader</code>阅读器和<code>tf.decode_csv</code>解析器。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">filename_queue = tf.train.string_input_producer([<span class="string">"file0.csv"</span>, <span class="string">"file1.csv"</span>]) </div><div class="line"><span class="comment"># 创建一个Filename Queue</span></div><div class="line"><span class="comment"># 该例csv文件中共有5列数据，前四列为features，最后一列为label</span></div><div class="line"></div><div class="line">reader = tf.TextLineReader() <span class="comment"># 文件阅读器</span></div><div class="line">key, value = reader.read(filename_queue) <span class="comment"># 每次执行阅读器都从文件读一行内容</span></div><div class="line"></div><div class="line"><span class="comment"># Default values, in case of empty columns. Also specifies the type of the decoded result.</span></div><div class="line">record_defaults = [[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]] <span class="comment"># 文件数据皆为整数</span></div><div class="line">col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line">features = tf.stack([col1, col2, col3, col4])</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Start populating the filename queue.</span></div><div class="line">  coord = tf.train.Coordinator() <span class="comment">#创建一个协调器，管理线程</span></div><div class="line">  threads = tf.train.start_queue_runners(coord=coord) <span class="comment">#启动QueueRunner, 此时文件名队列已经进队。</span></div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1200</span>):</div><div class="line">    <span class="comment"># Retrieve a single instance:</span></div><div class="line">    example, label = sess.run([features, col5])</div><div class="line"></div><div class="line">  coord.request_stop()</div><div class="line">  coord.join(threads)</div></pre></td></tr></table></figure>
<p>每次<code>read</code>的执行都会从文件中读取一行内容， <code>decode_csv</code> 操作会解析这一行内容并将其转为张量列表。在调用<code>run</code>或者<code>eval</code>去执行<code>read</code>之前， 必须先调用<code>tf.train.start_queue_runners</code>来将文件名填充到队列。否则<code>read</code>操作会被阻塞到文件名队列中有值为止。</p>
<p><code>record_defaults = [[1], [1], [1], [1], [1]]</code>代表了解析的摸版，默认用<code>,</code>隔开，是用于指定矩阵格式以及数据类型的，CSV文件中的矩阵是NXM的，则此处为1XM，例如上例中M=5。[1]表示解析为整型，如果矩阵中有小数，则应为float型，[1]应该变为[1.0]，[‘null’]解析为string类型。</p>
<p><code>col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults = record_defaults)</code>， 矩阵中有几列，这里就要写几个参数，比如5列，就要写到col5,不管你到底用多少。否则报错。</p>
<p><br></p>
<h3 id="固定长度记录"><a href="#固定长度记录" class="headerlink" title="固定长度记录"></a>固定长度记录</h3><p>我们也可以从<strong>二进制文件(.bin)</strong>中读取固定长度的数据，使用的是<code>tf.FixedLengthRecordReader</code>阅读器和<code>tf.decode_raw</code>解析器。<code>decode_raw</code>节点会把<code>string</code>转化为<code>uint8</code>类型的张量。</p>
<p>例如CIFAR-10数据集就采用的固定长度的数据，1字节的标签，后面跟着3072字节的图像数据。使用<code>uint8</code>类型张量的标准操作可以把每个图像的片段截取下来并且按照需要重组。下面有一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">reader = tf.FixedLengthRecordReader(record_bytes = record_bytes)</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line">label = tf.cast(tf.slice(record_bytes, [<span class="number">0</span>], [label_bytes]), tf.int32)</div><div class="line">image_raw = tf.slice(record_bytes, [label_bytes], [image_bytes])</div><div class="line">image_raw = tf.reshape(image_raw, [depth, height, width])</div><div class="line">image = tf.transpose(image_raw, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)) <span class="comment"># 图像形状为[height, width, channels]     </span></div><div class="line">image = tf.cast(image, tf.float32)</div></pre></td></tr></table></figure>
<p>这里介绍上述代码中出现的函数：<code>tf.slice()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">slice(</div><div class="line">    input_,</div><div class="line">    begin,</div><div class="line">    size,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>从一个张量<code>input</code>中提取出长度为<code>size</code>的一部分，提取的起点由<code>begin</code>定义。<code>size</code>是一个向量，它代表着在每个维度提取出的<code>tensor</code>的大小。<code>begin</code>表示提取的位置，它表示的是<code>input</code>的起点偏离值，也就是从每个维度第几个值开始提取。</p>
<p><code>begin</code>从0开始，<code>size</code>从1开始，如果<code>size[i]</code>的值为-1，则第i个维度从<code>begin</code>处到余下的所有值都被提取出来。</p>
<p>例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 'input' is [[[1, 1, 1], [2, 2, 2]],</div><div class="line">#             [[3, 3, 3], [4, 4, 4]],</div><div class="line">#             [[5, 5, 5], [6, 6, 6]]]</div><div class="line">tf.slice(input, [1, 0, 0], [1, 1, 3]) ==&gt; [[[3, 3, 3]]]</div><div class="line">tf.slice(input, [1, 0, 0], [1, 2, 3]) ==&gt; [[[3, 3, 3],</div><div class="line">                                            [4, 4, 4]]]</div><div class="line">tf.slice(input, [1, 0, 0], [2, 1, 3]) ==&gt; [[[3, 3, 3]],</div><div class="line">                                           [[5, 5, 5]]]</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="标准TensorFlow格式"><a href="#标准TensorFlow格式" class="headerlink" title="标准TensorFlow格式"></a>标准TensorFlow格式</h3><p>我们也可以把任意的数据转换为TensorFlow所支持的格式， 这种方法使TensorFlow的数据集更容易与网络应用架构相匹配。这种方法就是使用TFRecords文件，TFRecords文件包含了<code>tf.train.Example</code>的<em>protocol buffer</em>（里面包含了名为 <code>Features</code>的字段）。你可以写一段代码获取你的数据， 将数据填入到<code>Example</code>的<em>protocol buffer</em>，将<em>protocol buffer</em>序列化为一个字符串， 并且通过<code>tf.python_io.TFRecordWriter</code>类写入到TFRecords文件。</p>
<p>从TFRecords文件中读取数据， 可以使用<code>tf.TFRecordReader</code>阅读器以及<code>tf.parse_single_example</code>解析器。<code>parse_single_example</code>操作可以将<code>Example</code><em>protocol buffer</em>解析为张量。 具体可以参考如下例子，把MNIST数据集转化为TFRecords格式：</p>
<ul>
<li><a href="https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/convert_to_records.py" target="_blank" rel="external"><code>tensorflow/examples/how_tos/reading_data/convert_to_records.py</code></a> .</li>
</ul>
<p>SparseTensors这种稀疏输入数据类型使用队列来处理不是太好。如果要使用SparseTensors你就必须在批处理<strong>之后</strong>使用<code>tf.parse_example</code>去解析字符串记录 (而不是在批处理<strong>之前</strong>使用<code>tf.parse_single_example</code>) 。</p>
<p><br></p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>我们可以对输入的样本数据进行任意的预处理， 这些预处理不依赖于训练参数， 比如数据归一化， 提取随机数据片，增加噪声或失真等等。具体可以参考如下对CIFAR-10处理的例子：</p>
<ul>
<li><a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10.py" target="_blank" rel="external"><code>tensorflow/models/image/cifar10/cifar10.py</code></a></li>
</ul>
<p><br></p>
<h2 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h2><p>经过了之前的步骤，在数据读取流程的最后， 我们需要有另一个队列来批量执行输入样本的训练，评估或者推断。根据要不要打乱顺序，我们常用的有两个函数：</p>
<ul>
<li><code>tf.train.batch()</code></li>
<li><code>tf.train.shuffle_batch()</code></li>
</ul>
<p>下面来分别介绍：</p>
<h3 id="tf-train-batch"><a href="#tf-train-batch" class="headerlink" title="tf.train.batch()"></a>tf.train.batch()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">tf.train.batch(</div><div class="line">    tensors,</div><div class="line">    batch_size,</div><div class="line">    num_threads=<span class="number">1</span>,</div><div class="line">    capacity=<span class="number">32</span>,</div><div class="line">    enqueue_many=<span class="keyword">False</span>,</div><div class="line">    shapes=<span class="keyword">None</span>,</div><div class="line">    dynamic_pad=<span class="keyword">False</span>,</div><div class="line">    allow_smaller_final_batch=<span class="keyword">False</span>,</div><div class="line">    shared_name=<span class="keyword">None</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>该函数将会使用一个队列，函数读取一定数量的<code>tensors</code>送入队列，然后每次从中选取<code>batch_size</code>个<code>tensors</code>组成一个新的<code>tensors</code>返回出来。</p>
<p><code>capacity</code>参数决定了队列的长度。</p>
<p><code>num_threads</code>决定了有多少个线程进行入队操作，如果设置的超过一个线程，它们将从不同文件不同位置同时读取，可以更加充分的混合训练样本。</p>
<p>如果<code>enqueue_many</code>参数为<code>False</code>，则输入参数<code>tensors</code>为一个形状为<code>[x, y, z]</code>的张量，输出为一个形状为<code>[batch_size, x, y, z]</code>的张量。如果<code>enqueue_many</code>参数为<code>True</code>，则输入参数<code>tensors</code>为一个形状为<code>[*, x, y, z]</code>的张量，其中所有<code>*</code>的数值相同，输出为一个形状为<code>[batch_size, x, y, z]</code>的张量。</p>
<p>当<code>allow_smaller_final_batch</code>为<code>True</code>时，如果队列中的张量数量不足<code>batch_size</code>，将会返回小于<code>batch_size</code>长度的张量，如果为<code>False</code>，剩下的张量会被丢弃。</p>
<h3 id="tf-train-shuffle-batch"><a href="#tf-train-shuffle-batch" class="headerlink" title="tf.train.shuffle_batch()"></a>tf.train.shuffle_batch()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">tf.train.shuffle_batch(</div><div class="line">    tensors,</div><div class="line">    batch_size,</div><div class="line">    capacity,</div><div class="line">    min_after_dequeue,</div><div class="line">    num_threads=1,</div><div class="line">    seed=None,</div><div class="line">    enqueue_many=False,</div><div class="line">    shapes=None,</div><div class="line">    allow_smaller_final_batch=False,</div><div class="line">    shared_name=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>该函数类似于上面的<code>tf.train.batch()</code>，同样创建一个队列，主要区别是会首先把队列中的张量进行乱序处理，然后再选取其中的<code>batch_size</code>个张量组成一个新的张量返回。但是新增加了几个参数。</p>
<p><code>capacity</code>参数依然为队列的长度，建议<code>capacity</code>的取值如下：</p>
<p><code>min_after_dequeue + (num_threads + a small safety margin) * batch_size</code></p>
<p><code>min_after_dequeue</code>这个参数的意思是队列中，做dequeue（取数据）的操作后，线程要保证队列中至少剩下<code>min_after_dequeue</code>个数据。如果<code>min_after_dequeue</code>设置的过少，则即使<code>shuffle</code>为<code>True</code>，也达不到好的混合效果。</p>
<blockquote>
<p>假设你有一个队列，现在里面有m个数据，你想要每次随机从队列中取n个数据，则代表先混合了m个数据，再从中取走n个。 </p>
<p>当第一次取走n个后，队列就变为m-n个数据；当你下次再想要取n个时，假设队列在此期间入队进来了k个数据，则现在的队列中有(m-n+k)个数据，则此时会从混合的(m-n+k)个数据中随机取走n个。</p>
<p>如果队列填充的速度比较慢，k就比较小，那你取出来的n个数据只是与周围很小的一部分(m-n+k)个数据进行了混合。</p>
<p>因为我们的目的肯定是想尽最大可能的混合数据，因此设置<code>min_after_dequeue</code>，可以保证每次dequeue后都有足够量的数据填充尽队列，保证下次dequeue时可以很充分的混合数据。</p>
<p>但是<code>min_after_dequeue</code>也不能设置的太大，这样会导致队列填充的时间变长，尤其是在最初的装载阶段，会花费比较长的时间。</p>
</blockquote>
<p>其他参数和<code>tf.train.batch()</code>相同。</p>
<p><br></p>
<p>这里我们使用<code>tf.train.shuffle_batch</code>函数来对队列中的样本进行乱序处理。如下的模版：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_my_file_format</span><span class="params">(filename_queue)</span>:</span></div><div class="line">  reader = tf.SomeReader()</div><div class="line">  key, record_string = reader.read(filename_queue)</div><div class="line">  example, label = tf.some_decoder(record_string)</div><div class="line">  processed_example = some_processing(example)</div><div class="line">  <span class="keyword">return</span> processed_example, label</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_pipeline</span><span class="params">(filenames, batch_size, num_epochs=None)</span>:</span></div><div class="line">  filename_queue = tf.train.string_input_producer(</div><div class="line">      filenames, num_epochs=num_epochs, shuffle=<span class="keyword">True</span>)</div><div class="line">  example, label = read_my_file_format(filename_queue)</div><div class="line">  <span class="comment"># min_after_dequeue 越大意味着随机效果越好但是也会占用更多的时间和内存</span></div><div class="line">  <span class="comment"># capacity 必须比 min_after_dequeue 大</span></div><div class="line">  <span class="comment"># 建议capacity的取值如下：</span></div><div class="line">  <span class="comment"># min_after_dequeue + (num_threads + a small safety margin) * batch_size</span></div><div class="line">  min_after_dequeue = <span class="number">10000</span></div><div class="line">  capacity = min_after_dequeue + <span class="number">3</span> * batch_size</div><div class="line">  example_batch, label_batch = tf.train.shuffle_batch(</div><div class="line">      [example, label], batch_size=batch_size, capacity=capacity,</div><div class="line">      min_after_dequeue=min_after_dequeue)</div><div class="line">  <span class="keyword">return</span> example_batch, label_batch</div></pre></td></tr></table></figure>
<p>一个具体的例子如下，该例采用了CIFAR-10数据集，采用了固定长度读取的<code>tf.FixedLengthRecordReader</code>阅读器和<code>tf.decode_raw</code>解析器，同时进行了数据预处理操作中的标准化操作，最后使用<code>tf.train.shuffle_batch</code>函数批量执行数据的乱序处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">cifar10_data</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filename_queue)</span>:</span></div><div class="line">        self.height = <span class="number">32</span></div><div class="line">        self.width = <span class="number">32</span></div><div class="line">        self.depth = <span class="number">3</span></div><div class="line">        self.label_bytes = <span class="number">1</span></div><div class="line">        self.image_bytes = self.height * self.width * self.depth</div><div class="line">        self.record_bytes = self.label_bytes + self.image_bytes</div><div class="line">        self.label, self.image = self.read_cifar10(filename_queue)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_cifar10</span><span class="params">(self, filename_queue)</span>:</span></div><div class="line">        reader = tf.FixedLengthRecordReader(record_bytes = self.record_bytes)</div><div class="line">        key, value = reader.read(filename_queue)</div><div class="line">        record_bytes = tf.decode_raw(value, tf.uint8)</div><div class="line">        label = tf.cast(tf.slice(record_bytes, [<span class="number">0</span>], [self.label_bytes]), tf.int32)</div><div class="line">        image_raw = tf.slice(record_bytes, [self.label_bytes], [self.image_bytes])</div><div class="line">        image_raw = tf.reshape(image_raw, [self.depth, self.height, self.width])</div><div class="line">        image = tf.transpose(image_raw, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))        </div><div class="line">        image = tf.cast(image, tf.float32)</div><div class="line">        <span class="keyword">return</span> label, image</div><div class="line">    </div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputs</span><span class="params">(data_dir, batch_size, train = True, name = <span class="string">'input'</span>)</span>:</span></div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">        <span class="keyword">if</span> train:    </div><div class="line">            filenames = [os.path.join(data_dir,<span class="string">'data_batch_%d.bin'</span> % ii) </div><div class="line">                        <span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>)]</div><div class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line">                    </div><div class="line">            filename_queue = tf.train.string_input_producer(filenames)</div><div class="line">            read_input = cifar10_data(filename_queue)</div><div class="line">            images = read_input.image</div><div class="line">            images = tf.image.per_image_standardization(images)</div><div class="line">            labels = read_input.label</div><div class="line">            image, label = tf.train.shuffle_batch(</div><div class="line">                                    [images,labels], batch_size = batch_size, </div><div class="line">                                    min_after_dequeue = <span class="number">20000</span>, capacity = <span class="number">20192</span>)</div><div class="line">        </div><div class="line">            <span class="keyword">return</span> image, tf.reshape(label, [batch_size])</div><div class="line">            </div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            filenames = [os.path.join(data_dir,<span class="string">'test_batch.bin'</span>)]</div><div class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(f):</div><div class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">'Failed to find file: '</span> + f)</div><div class="line">                    </div><div class="line">            filename_queue = tf.train.string_input_producer(filenames)</div><div class="line">            read_input = cifar10_data(filename_queue)</div><div class="line">            images = read_input.image</div><div class="line">            images = tf.image.per_image_standardization(images)</div><div class="line">            labels = read_input.label</div><div class="line">            image, label = tf.train.shuffle_batch(</div><div class="line">                                    [images,labels], batch_size = batch_size, </div><div class="line">                                    min_after_dequeue = <span class="number">20000</span>, capacity = <span class="number">20192</span>)</div><div class="line">        </div><div class="line">            <span class="keyword">return</span> image, tf.reshape(label, [batch_size])</div></pre></td></tr></table></figure>
<p>这里介绍下函数<code>tf.image.per_image_standardization(image)</code>，该函数对图像进行线性变换使它具有零均值和单位方差，即规范化。其中参数<code>image</code>是一个3-D的张量，形状为<code>[height, width, channels]</code>。</p>
<p><br></p>
<h2 id="多个样本和多个阅读器"><a href="#多个样本和多个阅读器" class="headerlink" title="多个样本和多个阅读器"></a>多个样本和多个阅读器</h2><p>下面讲分别展示三个不同Reader数目和不同样本数的代码示例：</p>
<h3 id="文件准备"><a href="#文件准备" class="headerlink" title="文件准备"></a>文件准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Alpha1,A1\nAlpha2,A2\nAlpha3,A3"</span> &gt; A.csv</div><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Bee1,B1\nBee2,B2\nBee3,B3"</span> &gt; B.csv</div><div class="line">$ <span class="built_in">echo</span> -e <span class="string">"Sea1,C1\nSea2,C2\nSea3,C3"</span> &gt; C.csv</div><div class="line">$ cat A.csv</div><div class="line">Alpha1,A1</div><div class="line">Alpha2,A2</div><div class="line">Alpha3,A3</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="单个Reader，单个样本"><a href="#单个Reader，单个样本" class="headerlink" title="单个Reader，单个样本"></a>单个Reader，单个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 生成一个先入先出队列和一个QueueRunner</span></div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># 定义Reader</span></div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line"><span class="comment"># 定义Decoder</span></div><div class="line">example, label = tf.decode_csv(value, record_defaults=[[<span class="string">'null'</span>], [<span class="string">'null'</span>]])</div><div class="line"><span class="comment"># 运行Graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()  <span class="comment">#创建一个协调器，管理线程</span></div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)  <span class="comment">#启动QueueRunner, 此时文件名队列已经进队。</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example.eval()   <span class="comment">#取样本的时候，一个Reader先从文件名队列中取出文件名，读出数据，Decoder解析后进入样本队列。</span></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line"><span class="comment"># outpt</span></div><div class="line">Alpha1</div><div class="line">Alpha2</div><div class="line">Alpha3</div><div class="line">Bee1</div><div class="line">Bee2</div><div class="line">Bee3</div><div class="line">Sea1</div><div class="line">Sea2</div><div class="line">Sea3</div><div class="line">Alpha1</div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="单个Reader，多个样本"><a href="#单个Reader，多个样本" class="headerlink" title="单个Reader，多个样本"></a>单个Reader，多个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">example, label = tf.decode_csv(value, record_defaults=[[<span class="string">'null'</span>], [<span class="string">'null'</span>]])</div><div class="line"><span class="comment"># 使用tf.train.batch()会多加了一个样本队列和一个QueueRunner。Decoder解后数据会进入这个队列，再批量出队。</span></div><div class="line"><span class="comment"># 虽然这里只有一个Reader，但可以设置多线程，通过在tf.train.batch()中添加“num_threads="，相应增加线程数会提高读取速度，但并不是线程越多越好。</span></div><div class="line">example_batch, label_batch = tf.train.batch(</div><div class="line">      [example, label], batch_size=<span class="number">5</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example_batch.eval()</div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line"><span class="comment"># output</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div><div class="line"><span class="comment"># ['Bee3' 'Sea1' 'Sea2' 'Sea3' 'Alpha1']</span></div><div class="line"><span class="comment"># ['Alpha2' 'Alpha3' 'Bee1' 'Bee2' 'Bee3']</span></div><div class="line"><span class="comment"># ['Sea1' 'Sea2' 'Sea3' 'Alpha1' 'Alpha2']</span></div><div class="line"><span class="comment"># ['Alpha3' 'Bee1' 'Bee2' 'Bee3' 'Sea1']</span></div><div class="line"><span class="comment"># ['Sea2' 'Sea3' 'Alpha1' 'Alpha2' 'Alpha3']</span></div><div class="line"><span class="comment"># ['Bee1' 'Bee2' 'Bee3' 'Sea1' 'Sea2']</span></div><div class="line"><span class="comment"># ['Sea3' 'Alpha1' 'Alpha2' 'Alpha3' 'Bee1']</span></div><div class="line"><span class="comment"># ['Bee2' 'Bee3' 'Sea1' 'Sea2' 'Sea3']</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="多个Reader，多个样本"><a href="#多个Reader，多个样本" class="headerlink" title="多个Reader，多个样本"></a>多个Reader，多个样本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">filenames = [<span class="string">'A.csv'</span>, <span class="string">'B.csv'</span>, <span class="string">'C.csv'</span>]</div><div class="line">filename_queue = tf.train.string_input_producer(filenames, shuffle=<span class="keyword">False</span>)</div><div class="line">reader = tf.TextLineReader()</div><div class="line">key, value = reader.read(filename_queue)</div><div class="line">record_defaults = [[<span class="string">'null'</span>], [<span class="string">'null'</span>]]</div><div class="line">example_list = [tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line">                  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]  <span class="comment"># Reader设置为2</span></div><div class="line"><span class="comment"># 使用tf.train.batch_join()，可以使用多个reader，并行读取数据。每个Reader使用一个线程。</span></div><div class="line">example_batch, label_batch = tf.train.batch_join(</div><div class="line">      example_list, batch_size=<span class="number">5</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    coord = tf.train.Coordinator()</div><div class="line">    threads = tf.train.start_queue_runners(coord=coord)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">print</span> example_batch.eval()</div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div><div class="line">    </div><div class="line"><span class="comment"># output</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div><div class="line"><span class="comment"># ['Bee3' 'Sea1' 'Sea2' 'Sea3' 'Alpha1']</span></div><div class="line"><span class="comment"># ['Alpha2' 'Alpha3' 'Bee1' 'Bee2' 'Bee3']</span></div><div class="line"><span class="comment"># ['Sea1' 'Sea2' 'Sea3' 'Alpha1' 'Alpha2']</span></div><div class="line"><span class="comment"># ['Alpha3' 'Bee1' 'Bee2' 'Bee3' 'Sea1']</span></div><div class="line"><span class="comment"># ['Sea2' 'Sea3' 'Alpha1' 'Alpha2' 'Alpha3']</span></div><div class="line"><span class="comment"># ['Bee1' 'Bee2' 'Bee3' 'Sea1' 'Sea2']</span></div><div class="line"><span class="comment"># ['Sea3' 'Alpha1' 'Alpha2' 'Alpha3' 'Bee1']</span></div><div class="line"><span class="comment"># ['Bee2' 'Bee3' 'Sea1' 'Sea2' 'Sea3']</span></div><div class="line"><span class="comment"># ['Alpha1' 'Alpha2' 'Alpha3' 'Bee1' 'Bee2']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p><code>tf.train.batch</code>与<code>tf.train.shuffle_batch</code>函数是单个Reader读取，但是可以多线程，通过设置<code>num_threads</code>参数来设置多线程。<code>tf.train.batch_join</code>与<code>tf.train.shuffle_batch_join</code>可设置多Reader读取，每个Reader使用一个线程。至于两种方法的效率，单Reader时，2个线程就达到了速度的极限。多Reader时，2个Reader就达到了极限。所以并不是线程越多越快，甚至更多的线程反而会使效率下降。</p>
<p>上述两种方法，前者相比于后者的好处是：</p>
<ul>
<li>避免了两个不同的线程从同一个文件中读取同一个样本。</li>
<li>避免了过多的磁盘搜索操作。</li>
</ul>
<p>那么具体需要多少个读取线程呢？ 函数<code>tf.train.shuffle_batch*</code>为<code>graph</code>提供了获取文件名队列中的元素个数之和的方法。 如果你有足够多的读取线程， 文件名队列中的元素个数之和应该一直是一个略高于0的数。具体可以参考TensorBoard的教程。</p>
<p><br></p>
<h2 id="创建线程并使用QueueRunner对象来获取"><a href="#创建线程并使用QueueRunner对象来获取" class="headerlink" title="创建线程并使用QueueRunner对象来获取"></a>创建线程并使用QueueRunner对象来获取</h2><p>我们要添加<code>tf.train.QueueRunner</code>对象到数据流图中，在运行任何训练步骤之前，需要调用<code>tf.train.start_queue_runners</code>函数，否则数据流图将一直挂起，该函数将会启动输入管道的线程，填充样本到队列中，以便出队操作可以从队列中拿到样本。这种情况下最好配合使用一个<code>tf.train.Coordinator</code>，这样可以在发生错误的情况下正确地关闭这些线程。如果我们对训练迭代数做了限制，那么需要使用一个训练迭代数计数器，并且需要初始化它。推荐的代码模板如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create the graph, etc.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Create a session for running operations in the Graph.</span></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line"><span class="comment"># Initialize the variables (like the epoch counter).</span></div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"><span class="comment"># Start input enqueue threads.</span></div><div class="line">coord = tf.train.Coordinator()</div><div class="line">threads = tf.train.start_queue_runners(sess=sess, coord=coord)</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">        <span class="comment"># Run training steps or whatever</span></div><div class="line">        sess.run(train_op)</div><div class="line"></div><div class="line"><span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">    print(<span class="string">'Done training -- epoch limit reached'</span>)</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    <span class="comment"># When done, ask the threads to stop.</span></div><div class="line">    coord.request_stop()</div><div class="line"></div><div class="line"><span class="comment"># Wait for threads to finish.</span></div><div class="line">coord.join(threads)</div><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p><br></p>
<center><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170704205652_Agg5Zf_AnimatedFileQueues.gif" alt="Animated File Queues" width="900"></center>

<p>如上图所示，每个<code>QueueRunner</code>负责一个阶段，处理那些需要在线程中运行的入队操作的列表。一旦数据流图构造成功，<code>tf.train.start_queue_runners</code>函数就会要求数据流图中每个<code>QueueRunner</code>去开始它的线程运行入队操作。</p>
<p>如果一切顺利的话，我们可以执行训练步骤，同时队列也会被后台线程来填充。如果设置了最大训练迭代数，在某些时候，样本出队的操作可能会得到一个<code>tf.OutOfRangeError</code>的错误。这其实是TensorFlow的“文件结束”（EOF）——这就意味着已经达到了最大训练迭代数，已经没有更多可用的样本了。</p>
<p>最后一个因素是<code>Coordinator</code>。这是负责在收到任何关闭信号的时候，让所有的线程都知道。最常见的情况是在发生异常时，比如说其中一个线程在运行某些操作时出现错误（或一个普通的Python异常）。</p>
<p><br></p>
<h3 id="疑问：在达到最大训练迭代数的时候如何关闭线程"><a href="#疑问：在达到最大训练迭代数的时候如何关闭线程" class="headerlink" title="疑问：在达到最大训练迭代数的时候如何关闭线程?"></a>疑问：在达到最大训练迭代数的时候如何关闭线程?</h3><p>想象一下，我们有一个模型并且设置了最大训练迭代数。这意味着，生成文件的那个线程只会在产生<code>OutOfRange</code>错误之前运行。<code>QueueRunner</code>会捕获该错误，并且关闭文件名的队列，最后退出线程。关闭队列做了两件事情：</p>
<ul>
<li>如果试着对文件名队列执行入队操作将发生错误。</li>
<li>当前或将来的出队操作要么成功（如果队列中还有足够的元素）或立即失败（发生<code>OutOfRange</code>错误）。它们不会等待更多的元素被添加到队列中，因为上面的一点已经保证了这种情况不会发生。</li>
</ul>
<p>关键是，当在文件名队列被关闭时候，有可能还有许多文件名在该队列中，这样下一阶段的流水线（包括reader和其它预处理）还可以继续运行一段时间。 一旦文件名队列空了之后，如果后面的流水线还要尝试从文件名队列中取出一个文件名，这将会触发<code>OutOfRange</code>错误。在这种情况下，即使你可能有一个<code>QueueRunner</code>关联着多个线程，如果该出错线程不是<code>QueueRunner</code>中最后的那个线程，那么<code>OutOfRange</code>错误只会使得这一个线程退出。而其他那些正处理自己的最后一个文件的线程继续运行，直至他们完成为止。（但如果你使用的是<code>tf.train.Coordinator</code>来管理所有的线程，那么其他类型的错误将导致所有线程停止）。一旦所有的reader线程触发<code>OutOfRange</code>错误，样本队列才会被关闭。</p>
<p>同样，样本队列中会有一些已经入队的元素，所以样本训练将一直持续直到样本队列中再没有样本为止。如果样本队列是一个<code>RandomShuffleQueue</code>，因为你使用了<code>shuffle_batch</code> 或者 <code>shuffle_batch_join</code>，所以通常不会出现以往那种队列中的元素会比<code>min_after_dequeue</code> 定义的更少的情况。 然而，一旦该队列被关闭，<code>min_after_dequeue</code>设置的限定值将失效，最终队列将为空。在这一点来说，当实际训练线程尝试从样本队列中取出数据时，将会触发<code>OutOfRange</code>错误，然后训练线程会退出。一旦所有的训练线程完成，<code>tf.train.Coordinator.join</code>会返回，你就可以正常退出了。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/reading_data" target="_blank" rel="external">Reading data  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/extend/new_data_formats" target="_blank" rel="external">Custom Data Readers  |  TensorFlow</a></li>
<li><a href="http://www.cnblogs.com/Charles-Wan/p/6197019.html" target="_blank" rel="external">TF Boys (TensorFlow Boys ) 养成记（二）： TensorFlow 数据读取 - Charles-Wan - 博客园</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer" target="_blank" rel="external">tf.train.string_input_producer  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/batch" target="_blank" rel="external">tf.train.batch  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch" target="_blank" rel="external">tf.train.shuffle_batch  |  TensorFlow</a></li>
<li><a href="http://honggang.io" target="_blank" rel="external">honggang.io</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/reading_data.html#AUTOGENERATED-reading-from-files" target="_blank" rel="external">数据读取 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
<li><a href="http://blog.csdn.net/masa_fish/article/details/52624459" target="_blank" rel="external">TensorFlow官网教程Convolutional Neural Networks 难点详解 - 玛莎鱼的博客 - CSDN博客</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文整理了TensorFlow中的数据读取方法，在TensorFlow中主要有三种方法读取数据：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feeding：由Python提供数据。&lt;/li&gt;
&lt;li&gt;Preloaded data：预加载数据。&lt;/li&gt;
&lt;li&gt;Reading from files：从文件读取。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（八）：线程和队列</title>
    <link href="http://zangbo.me/2017/07/03/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89/"/>
    <id>http://zangbo.me/2017/07/03/TensorFlow 笔记（八）/</id>
    <published>2017-07-03T02:11:56.000Z</published>
    <updated>2017-07-05T04:25:39.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文介绍了TensorFlow的线程和队列。在使用TensorFlow进行异步计算时，队列是一种强大的机制。正如TensorFlow中的其他组件一样，队列就是TensorFlow图中的节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，其他节点可以把新元素插入到队列后端(rear)，也可以把队列前端(front)的元素删除。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p>为了感受一下队列，先来看一个简单的例子。我们先创建一个“先入先出”的队列（FIFOQueue），并将其内部所有元素初始化为零。然后，我们构建一个TensorFlow图，它从队列前端取走一个元素，加上1之后，放回队列的后端。慢慢地，队列的元素的值就会增加。</p>
<center><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/IncremeterFifoQueue.gif" alt="Incremeter Fifo Queue" width="800"></center>

<p><code>Enqueue</code>、 <code>EnqueueMany</code>和<code>Dequeue</code>都是特殊的节点，在Python API中，它们都是队列对象的方法（例如<code>q.enqueue(...)</code>）。</p>
<p>下面我们深入了解下细节。</p>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>诸如<code>FIFOQueue</code>和<code>RandomShuffleQueue</code>这样的队列，在TensorFlow的<code>tensor</code>异步计算时非常重要。</p>
<p>例如，一个典型的输入结构：使用一个<code>RandomShuffleQueue</code>来作为模型训练的输入：</p>
<ul>
<li>多个线程准备训练样本，并且把这些样本推入队列。</li>
<li>一个训练线程执行一个训练操作，此操作会从队列中移除最小批次的样本（mini-batches)。</li>
</ul>
<p>TensorFlow的<code>Session</code>对象是可以支持多线程的，因此多个线程可以很方便地使用同一个会话（Session）并且并行地执行操作。然而，在Python程序实现这样的并行运算却并不容易。所有线程都必须能被同步终止，异常必须能被正确捕获并报告，会话终止的时候， 队列必须能被正确地关闭。</p>
<p>TensorFlow提供了两个类来帮助多线程的实现：<code>tf.Coordinator</code>和 <code>tf.QueueRunner</code>，通常来说这两个类必须被一起使用。<code>Coordinator</code>类用来同时停止多个工作线程并且向那个在等待所有工作线程终止的程序报告异常。<code>QueueRunner</code>类用来协调多个工作线程并将多个张量推入同一个队列中。</p>
<p><br></p>
<h1 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h1><p><code>Coordinator</code>类用来帮助多个线程协同工作，多个线程同步终止。 其主要方法有：</p>
<ul>
<li><code>should_stop()</code>：如果线程应该停止则返回True。</li>
<li><code>request_stop(&lt;exception&gt;)</code>：请求该线程停止。</li>
<li><code>join(&lt;list of threads&gt;)</code>：等待被指定的线程终止。</li>
</ul>
<p>首先创建一个<code>Coordinator</code>对象，然后建立一些使用<code>Coordinator</code>对象的线程。这些线程通常一直循环运行，每次循环前首先判断<code>should_stop()</code>是否返回<code>True</code>，如果是的话就停止。 任何线程都可以决定什么时候应该停止，它只需要调用<code>request_stop()</code>，同时其他线程的<code>should_stop()</code>将会返回<code>True</code>，然后就都停下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Thread body: loop until the coordinator indicates a stop was requested.</span></div><div class="line"><span class="comment"># If some condition becomes true, ask the coordinator to stop.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">MyLoop</span><span class="params">(coord)</span>:</span></div><div class="line">  <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</div><div class="line">    ...do something...</div><div class="line">    <span class="keyword">if</span> ...some condition...:</div><div class="line">      coord.request_stop()</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="comment"># ...</span></div><div class="line">  coord = tf.train.Coordinator()</div><div class="line">  <span class="comment"># Start a number of threads, passing the coordinator to each of them.</span></div><div class="line">  ...start thread <span class="number">1</span> MyLoop(coord)</div><div class="line">  ...start thread N MyLoop(coord)</div><div class="line">  <span class="comment"># Wait for all the threads to terminate.</span></div><div class="line">  coord.join(threads)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">  coord.request_stop()</div><div class="line">coord.join(threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="QueueRunner"><a href="#QueueRunner" class="headerlink" title="QueueRunner"></a>QueueRunner</h1><p><code>QueueRunner</code>类会创建一组线程， 这些线程可以重复的执行Enquene操作， 他们使用同一个<code>Coordinator</code>来处理线程同步终止。此外，一个<code>QueueRunner</code>会运行一个用于异常处理的<em>closer thread</em>，当<code>Coordinator</code>收到异常报告时，这个<em>closer thread</em>会自动关闭队列。</p>
<p>我们可以使用一个一个<code>QueueRunner</code>来实现上述结构。 首先建立一个TensorFlow图表，这个图表使用队列来输入样本，处理样本并将样本推入队列中，用training操作来移除队列中的样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">example = ...ops to create one example...</div><div class="line"><span class="comment"># Create a queue, and an op that enqueues examples one at a time in the queue.</span></div><div class="line">queue = tf.RandomShuffleQueue(...)</div><div class="line">enqueue_op = queue.enqueue(example)</div><div class="line"><span class="comment"># Create a training graph that starts by dequeuing a batch of examples.</span></div><div class="line">inputs = queue.dequeue_many(batch_size)</div><div class="line">train_op = ...use <span class="string">'inputs'</span> to build the training part of the graph...</div></pre></td></tr></table></figure>
<p>在Python的训练程序中，创建一个<code>QueueRunner</code>来运行几个线程， 这几个线程处理样本，并且将样本推入队列。创建一个<code>Coordinator</code>，让queue runner使用<code>Coordinator</code>来开始它的线程，同时创建一个训练的循环， 并且使用<code>Coordinator</code>来控制<code>QueueRunner</code>这些线程的终止。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a queue runner that will run 4 threads in parallel to enqueue</span></div><div class="line"><span class="comment"># examples.</span></div><div class="line">qr = tf.train.QueueRunner(queue, [enqueue_op] * <span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph.</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># Create a coordinator, launch the queue runner threads.</span></div><div class="line">coord = tf.train.Coordinator()</div><div class="line">enqueue_threads = qr.create_threads(sess, coord=coord, start=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Run the training loop, controlling termination with the coordinator.</span></div><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</div><div class="line">    <span class="keyword">if</span> coord.should_stop():</div><div class="line">        <span class="keyword">break</span></div><div class="line">    sess.run(train_op)</div><div class="line"><span class="comment"># When done, ask the threads to stop.</span></div><div class="line">coord.request_stop()</div><div class="line"><span class="comment"># And wait for them to actually do it.</span></div><div class="line">coord.join(enqueue_threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h1><p>通过queue runners启动的线程不仅仅推送样本到队列。它们还捕捉和处理由队列产生的异常，包括<code>OutOfRangeError</code>异常，这个异常是用于报告队列被关闭。 使用<code>Coordinator</code>训练时在主循环中必须同时捕捉和报告异常。 下面是对上面训练循环的改进版本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</div><div class="line">        <span class="keyword">if</span> coord.should_stop():</div><div class="line">            <span class="keyword">break</span></div><div class="line">        sess.run(train_op)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">    <span class="comment"># Report exceptions to the coordinator.</span></div><div class="line">    coord.request_stop(e)</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    <span class="comment"># Terminate as usual. It is safe to call `coord.request_stop()` twice.</span></div><div class="line">    coord.request_stop()</div><div class="line">    coord.join(threads)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/programmers_guide/threading_and_queues" target="_blank" rel="external">Threading and Queues  |  TensorFlow</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html" target="_blank" rel="external">线程和队列 - TensorFlow 官方文档中文版 - 极客学院Wiki</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator" target="_blank" rel="external">tf.train.Coordinator  |  TensorFlow</a> </li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文介绍了TensorFlow的线程和队列。在使用TensorFlow进行异步计算时，队列是一种强大的机制。正如TensorFlow中的其他组件一样，队列就是TensorFlow图中的节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，其他节点可以把新元素插入到队列后端(rear)，也可以把队列前端(front)的元素删除。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（七）：Exponential_decay</title>
    <link href="http://zangbo.me/2017/07/01/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89/"/>
    <id>http://zangbo.me/2017/07/01/TensorFlow 笔记（七）/</id>
    <published>2017-07-01T07:17:57.000Z</published>
    <updated>2017-07-01T08:23:00.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>在神经网络的训练过程中，<strong>学习率(learning rate)</strong>控制着参数的更新速度，<code>tf.train</code>类下面的五种不同的学习速率的衰减方法。</p>
<ul>
<li><code>tf.train.exponential_decay</code></li>
<li><code>tf.train.inverse_time_decay</code></li>
<li><code>tf.train.natural_exp_decay</code></li>
<li><code>tf.train.piecewise_constant</code></li>
<li><code>tf.train.polynomial_decay</code></li>
</ul>
<p>本文只对<code>exponential_decay</code>做整理。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="exponential-decay"><a href="#exponential-decay" class="headerlink" title="exponential_decay"></a>exponential_decay</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tf.train.exponential_decay(</div><div class="line">    learning_rate,</div><div class="line">    global_step,</div><div class="line">    decay_steps,</div><div class="line">    decay_rate,</div><div class="line">    staircase=<span class="keyword">False</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<ul>
<li><code>learning_rate</code>：初始学习率</li>
<li><code>global_step</code>：当前迭代次数</li>
<li><code>decay_steps</code>：衰减迭代次数（在迭代到该次数时学习率衰减为<code>earning_rate * decay_rate</code>）</li>
<li><code>decay_rate</code>：学习率衰减率，通常介于0-1之间。</li>
</ul>
<p>学习率会按照以下公式变化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</div></pre></td></tr></table></figure>
<p>直观解释：假设给定初始学习率<code>learning_rate</code>为0.1，学习率衰减率为0.1，<code>decay_steps</code>为10000，则随着迭代次数从1到10000，当前的学习率<code>decayed_learning_rate</code>慢慢的从0.1降低为<code>0.1*0.1=0.01</code>，当迭代次数到20000，当前的学习率慢慢的从0.01降低为<code>0.1*0.1^2=0.001</code>，以此类推。也就是说每10000次迭代，学习率衰减为前10000次的十分之一，该衰减是连续的，这是在<code>staircase</code>为<code>False</code>的情况下。</p>
<p>如果<code>staircase</code>为<code>True</code>，则<code>global_step / decay_steps</code>始终取整数，也就是说衰减是突变的，每<code>decay_steps</code>次变化一次，变化曲线是阶梯状。</p>
<p>例子：每100000次迭代衰减一次，学习率衰减率为0.96。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### ...</span></div><div class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</div><div class="line">starter_learning_rate = <span class="number">0.1</span></div><div class="line">learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, <span class="number">100000</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Passing global_step to minimize() will increment it at each step.</span></div><div class="line">learning_step = (</div><div class="line">    tf.train.GradientDescentOptimizer(learning_rate)</div><div class="line">    .minimize(...my loss..., global_step=global_step)</div><div class="line">)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong>这里的<code>global_step</code>变量的<code>trainable</code>要设置为<code>False</code>，它代表着当前的迭代次数，我们不能对它进行训练，系统会自动更新它的值，初始化为0，从1开始。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay" target="_blank" rel="external">tf.train.exponential_decay  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;在神经网络的训练过程中，&lt;strong&gt;学习率(learning rate)&lt;/strong&gt;控制着参数的更新速度，&lt;code&gt;tf.train&lt;/code&gt;类下面的五种不同的学习速率的衰减方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.train.exponential_decay&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.inverse_time_decay&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.natural_exp_decay&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.piecewise_constant&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.train.polynomial_decay&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文只对&lt;code&gt;exponential_decay&lt;/code&gt;做整理。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（六）：Moving Average</title>
    <link href="http://zangbo.me/2017/07/01/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://zangbo.me/2017/07/01/TensorFlow 笔记（六）/</id>
    <published>2017-07-01T03:08:19.000Z</published>
    <updated>2017-07-01T05:48:04.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了TensorFlow训练模型过程中对参数的<strong>滑动平均(moving average)</strong>的计算，在测试数据上评估模型性能时用这些平均值总会提升预测结果表现，用到的类主要为<code>tf.train.ExponentialMovingAverage</code>。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="MovingAverage"><a href="#MovingAverage" class="headerlink" title="MovingAverage"></a>MovingAverage</h1><p>常规的滑动平均的计算方法十分简单，对于一个给定的数列，首先设定一个固定的值k，然后分别计算第1项到第k项，第2项到第k+1项，第3项到第k+2项的平均值，依次类推。</p>
<p>以<code>1、2、3、4、5</code>共5个数为例，window为3，计算过程为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(1+2+3)/3=2</div><div class="line">(2+3+4)/3=3</div><div class="line">(3+4+5)/3=4</div></pre></td></tr></table></figure>
<p>下图很好的反映了原始数据和滑动平均之间的关系，其中绿线为原始数据，红线为MovingAverage：</p>
<ul>
<li>当window为3:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_TyB8hT_MovingAverage1.jpeg" alt="window=3" width="700"></p>
<ul>
<li>当window为10:</li>
</ul>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630211204_UWRLDA_MovingAverage2.jpeg" alt="window=10" width="700"></p>
<p>可以发现当我们使用滑动平均时，会十分有效的提升模型在测试数据上的<strong>健壮性(robustness)</strong>。</p>
<p><br></p>
<h1 id="ExponentialMovingAverage"><a href="#ExponentialMovingAverage" class="headerlink" title="ExponentialMovingAverage"></a>ExponentialMovingAverage</h1><p>在TensorFlow中，我们计算的是<strong>指数滑动平均(ExponentialMovingAverage)</strong>，我们通过使用一个<strong>指数衰减(exponential decay)</strong>来维持着变量的滑动平均。</p>
<p>当我们训练一个模型时，计算训练参数的滑动平均经常是十分有利的，当我们用这些平均后的参数来评估模型时有时会得到比使用常规的训练参数好很多的结果。</p>
<p>我们用一个<code>apply()</code>函数返回一个<code>ops</code>来添加变量的一个副本同时得到原变量的滑动平均，它在我们训练模型的时候使用。该<code>ops</code>得到原变量的滑动平均始终是在每一次训练迭代结束后。</p>
<p><code>average()</code>和<code>average_name()</code>函数返回影子变量和它们的名字，它们在我们对测试数据进行模型评估时使用，它们用参数的滑动平均值来代替最终的训练值来对模型进行评估。它们也可以在我们从一个<code>checkpoint file</code>继续开始训练模型时使用。</p>
<p>滑动平均值用一个指数衰减来计算，当我们创建<code>ExponentialMovingAverage</code>对象时会把该<code>decay</code>值输入进去。影子变量的初始化值和原始变量初始化值相同。每个影子变量计算滑动平均值的公式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">shadow_variable = decay * shadow_variable + (1 - decay) * variable</div></pre></td></tr></table></figure>
<p>通常我们定义<code>decay</code>时会让它尽可能接近于1.0，一般来说我们会让它为0.999、0.9999等。</p>
<p>如下是我们训练一个模型时的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create variables.</span></div><div class="line">var0 = tf.Variable(...)</div><div class="line">var1 = tf.Variable(...)</div><div class="line"><span class="comment"># ... use the variables to build a training model...</span></div><div class="line">...</div><div class="line"><span class="comment"># Create an op that applies the optimizer.  This is what we usually</span></div><div class="line"><span class="comment"># would use as a training op.</span></div><div class="line">opt_op = opt.minimize(my_loss, [var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an ExponentialMovingAverage object</span></div><div class="line">ema = tf.train.ExponentialMovingAverage(decay=<span class="number">0.9999</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create the shadow variables, and add ops to maintain moving averages</span></div><div class="line"><span class="comment"># of var0 and var1.</span></div><div class="line">maintain_averages_op = ema.apply([var0, var1])</div><div class="line"></div><div class="line"><span class="comment"># Create an op that will update the moving averages after each training</span></div><div class="line"><span class="comment"># step.  This is what we will use in place of the usual training op.</span></div><div class="line"><span class="keyword">with</span> tf.control_dependencies([opt_op]):</div><div class="line">    training_op = tf.group(maintain_averages_op)</div><div class="line"></div><div class="line"><span class="comment"># ...train the model by running training_op...</span></div></pre></td></tr></table></figure>
<p>当我们使用滑动平均来预测时，有两种用法：</p>
<ol>
<li>用影子变量代替原始变量，使用<code>average()</code>函数来返回给定变量的影子变量。</li>
<li>通过使用影子变量的<code>name</code>来载入<code>checkpoint files</code>，我们在这里使用<code>average_name()</code>函数。对于这种用法有如下例子：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a Saver that loads variables from their saved shadow values.</span></div><div class="line">shadow_var0_name = ema.average_name(var0)</div><div class="line">shadow_var1_name = ema.average_name(var1)</div><div class="line">saver = tf.train.Saver(&#123;shadow_var0_name: var0, shadow_var1_name: var1&#125;)</div><div class="line">saver.restore(...checkpoint filename...)</div><div class="line"><span class="comment"># var0 and var1 now hold the moving average values</span></div></pre></td></tr></table></figure>
<p>详情可以查看<code>tf.train.Saver</code>，下面介绍<code>ExponentialMovingAverage</code>的相关函数。</p>
<p><br></p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul>
<li><code>__init__</code></li>
<li><code>apply</code></li>
<li><code>average</code></li>
<li><code>average_name</code></li>
<li><code>variables_to_restore</code></li>
</ul>
<p><br></p>
<h2 id="init"><a href="#init" class="headerlink" title="__init__"></a>__init__</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">__init__(</div><div class="line">    decay,</div><div class="line">    num_updates=<span class="keyword">None</span>,</div><div class="line">    zero_debias=<span class="keyword">False</span>,</div><div class="line">    name=<span class="string">'ExponentialMovingAverage'</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>创建一个<code>ExponentialMovingAverage</code>对象。</p>
<ol>
<li><p><code>decay</code>一般取值接近于1.0。</p>
</li>
<li><p><code>num_updates</code>允许<code>dacay</code>值动态的变化，在训练开端<code>dacay</code>速率较低，这使得滑动均值更快，如果有值的话，实际<code>decay</code>速率为以下公式：</p>
<p><code>min(decay, (1 + num_updates) / (10 + num_updates))</code></p>
</li>
<li><p><code>name</code>将会给<code>apply()</code>中的<code>ops</code>添加一个额外的前置名字。</p>
</li>
</ol>
<p><br></p>
<h2 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apply(var_list=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>该函数维护变量的滑动平均，返回一个<code>op</code>来更新所有影子变量。<code>var_list</code>必须是一个变量或者<code>Tensor</code>对象的列表，这个函数创造<code>var_list</code>中所有变量的副本，对于变量副本，初始化值和原变量初始化值相同。变量类型必须是<code>float</code>相关的类型。</p>
<p><code>apply()</code>函数对于不同的<code>var_list</code>可以被调用多次。</p>
<p><br></p>
<h2 id="average"><a href="#average" class="headerlink" title="average"></a>average</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>的滑动平均影子变量，返回类型为<code>Variable</code>。前提是该<code>var</code>使用了<code>apply()</code>函数来维护。</p>
<p><br></p>
<h2 id="average-name"><a href="#average-name" class="headerlink" title="average_name"></a>average_name</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average_name(var)</div></pre></td></tr></table></figure>
<p>返回<code>var</code>变量的滑动平均影子变量的<code>name</code>。该函数一个典型的应用是在训练过程中计算原始变量的滑动平均，并且在测试时根据影子变量的<code>name</code>恢复出原始变量。</p>
<p>为了恢复原始变量，我们必须知道影子变量的<code>name</code>，影子变量的<code>name</code>和原始变量可以在训练阶段利用<code>Saver()</code>对象来保存，操作为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.train.Saver(&#123;ema.average_name(var): var&#125;)</div></pre></td></tr></table></figure>
<p><code>average_name()</code>函数在<code>apply()</code>函数调用之前或之后都可以使用。</p>
<p><br></p>
<h2 id="variables-to-restore"><a href="#variables-to-restore" class="headerlink" title="variables_to_restore"></a>variables_to_restore</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">variables_to_restore(moving_avg_variables=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>返回一个从<code>restore_name</code>到<code>Variables</code>的映射，如果一个变量有滑动平均值，那么就用该滑动平均影子变量的<code>name</code>来作为<code>restore name</code>，否则，就用原始变量的<code>name</code>。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">variables_to_restore = ema.variables_to_restore()</div><div class="line">saver = tf.train.Saver(variables_to_restore)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage" target="_blank" rel="external">tf.train.ExponentialMovingAverage  |  TensorFlow</a></li>
<li><a href="http://blog.csdn.net/u014365862/article/details/54380313" target="_blank" rel="external">MovingAverage-滑动平均 - 小鹏的专栏 - 博客频道 - CSDN.NET</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文记录了TensorFlow训练模型过程中对参数的&lt;strong&gt;滑动平均(moving average)&lt;/strong&gt;的计算，在测试数据上评估模型性能时用这些平均值总会提升预测结果表现，用到的类主要为&lt;code&gt;tf.train.ExponentialMovingAverage&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（五）：常用函数和模型的保存与恢复</title>
    <link href="http://zangbo.me/2017/06/30/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://zangbo.me/2017/06/30/TensorFlow 笔记（五）/</id>
    <published>2017-06-30T12:58:59.000Z</published>
    <updated>2017-07-01T05:53:17.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文学习了TensorFlow的一些常用的基础函数，主要为以下几种：</p>
<ul>
<li>tf.group</li>
<li>tf.Graph.control_dependencies</li>
<li>tf.train.Saver</li>
</ul>
<p>同时介绍了训练过程中的模型保存和恢复方法，主要用到<code>tf.train.Saver</code>类。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a>tf.group</h1><p><code>tf.group(inputs)</code>创建一个<code>op</code>把多个<code>ops</code>给组合起来，该<code>op</code>无输出。当该<code>op</code>结束操作，其中的所有<code>ops</code>都会结束。</p>
<p>其中<code>inputs</code>为空或者很多<code>tensors</code>。</p>
<p><br></p>
<h1 id="tf-Graph-control-dependencies"><a href="#tf-Graph-control-dependencies" class="headerlink" title="tf.Graph.control_dependencies"></a>tf.Graph.control_dependencies</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">control_dependencies(control_inputs)</div></pre></td></tr></table></figure>
<p>参数<code>control_inputs</code>是一个包含<code>op</code>或者<code>tensor</code>的列表，该列表内的对象必须在控制区域内的<code>ops</code>之前执行。可以为<code>None</code>来清空控制依赖。</p>
<p>通常用<code>with</code>操作来定义一个区域，在该区域下所有的<code>ops</code>都要在<code>control_inputs</code>执行结束后才能执行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</div><div class="line">  <span class="comment"># `d` and `e` will only run after `a`, `b`, and `c` have executed.</span></div><div class="line">  d = ...</div><div class="line">  e = ...</div></pre></td></tr></table></figure>
<p>多次用<code>with</code>调用该函数会得到叠加的依赖，区域内的<code>ops</code>将会在以上所有层次的<code>control_inputs</code>运行结束后才能得到运行。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">    <span class="comment"># Ops constructed here run after `a`, `b`, `c`, and `d`.</span></div></pre></td></tr></table></figure>
<p>我们可以用<code>None</code>来清空控制所有依赖。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</div><div class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></div><div class="line">  <span class="keyword">with</span> g.control_dependencies(<span class="keyword">None</span>):</div><div class="line">    <span class="comment"># Ops constructed here run normally, not waiting for either `a` or `b`.</span></div><div class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</div><div class="line">      <span class="comment"># Ops constructed here run after `c` and `d`, also not waiting</span></div><div class="line">      <span class="comment"># for either `a` or `b`.</span></div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p>控制依赖起作用的区域内只有<code>ops</code>会被执行，仅仅把一个节点放在该区域是不起作用的。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># WRONG</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  t = tf.matmul(tensor, tensor)</div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created outside the context, so no control</span></div><div class="line">    <span class="comment"># dependency will be added.</span></div><div class="line">    <span class="keyword">return</span> t</div><div class="line"></div><div class="line"><span class="comment"># RIGHT</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</div><div class="line">    <span class="comment"># The matmul op is created in the context, so a control dependency</span></div><div class="line">    <span class="comment"># will be added.</span></div><div class="line">    <span class="keyword">return</span> tf.matmul(tensor, tensor)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="tf-train-Saver"><a href="#tf-train-Saver" class="headerlink" title="tf.train.Saver"></a>tf.train.Saver</h1><p>在训练过程中我们可能会想到每隔一段训练保存一次模型，一方面为了防止过拟合，另一方面如果训练过程被意外打断还可以从某个保存点继续开始训练。之前已经学习过<code>Variable</code>的概念，今天来学习下如何用<code>tf.train.Saver</code>来保存和恢复一个模型。关于<code>tf.train.Saver</code>更详细的内容请参考官方文档。</p>
<p><code>Saver</code>类添加<code>ops</code>来保存变量到<em>checkpoints</em>或者从<em>checkpoints</em>中恢复变量，它同时提供了一些函数方法来运行这些<code>ops</code>。<em>checkpoints</em>是一种二进制文件，它把<code>variable name</code>和<code>tensor</code>值联系起来，默认的为<code>tf.Variable.name</code>。使用<em>checkpoints</em>最好的方法就是用<code>Saver</code>来载入它。</p>
<p><code>Saver</code>可以自动的给<em>checkpoints</em>文件命名，这使得我们在不同的训练阶段可以保存不同的<em>checkpoints</em>。例如我们可以用训练迭代次数来给<em>checkpoints</em>命名，为了防止训练阶段磁盘空间占用量过多，我们还可以选择只保存最近N个文件，或者每N个小时保存一次文件。</p>
<p><br></p>
<h2 id="保存变量"><a href="#保存变量" class="headerlink" title="保存变量"></a>保存变量</h2><p>用<code>tf.train.Saver()</code>创建一个<code>Saver</code>对象来控制模型中所有的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add an op to initialize the variables.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, initialize the variables, do some work, save the</span></div><div class="line"><span class="comment"># variables to disk.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  sess.run(init_op)</div><div class="line">  <span class="comment"># Do some work with the model.</span></div><div class="line">  <span class="comment"># ..</span></div><div class="line">  <span class="comment"># Save the variables to disk.</span></div><div class="line">  save_path = saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model saved in file: %s"</span> % save_path)</div></pre></td></tr></table></figure>
<p>为了自动给<em>checkpoints</em>文件命名我们可以传入一个<code>global_step</code>值给<code>save()</code>函数。例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">saver.save(sess, 'my-model', global_step=0) ==&gt; filename: 'my-model-0'</div><div class="line">...</div><div class="line">saver.save(sess, 'my-model', global_step=1000) ==&gt; filename: 'my-model-1000'</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="恢复变量"><a href="#恢复变量" class="headerlink" title="恢复变量"></a>恢复变量</h2><p>我们用同一个<code>Saver</code>对象来恢复变量，注意当我们从一个文件恢复变量时我们没必要提前对变量初始化。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some variables.</span></div><div class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># Add ops to save and restore all the variables.</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and</span></div><div class="line"><span class="comment"># do some work with the model.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Restore variables from disk.</span></div><div class="line">  saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</div><div class="line">  print(<span class="string">"Model restored."</span>)</div><div class="line">  <span class="comment"># Do some work with the model</span></div><div class="line">  <span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="保存和恢复部分变量"><a href="#保存和恢复部分变量" class="headerlink" title="保存和恢复部分变量"></a>保存和恢复部分变量</h2><p>如果我们没有给<code>tf.train.Saver()</code>任何参数，它默认保存所有变量，每个变量都和它们的<code>name</code>联系起来。</p>
<p>有时候我们想要在<em>checkpoint</em>文件中给某个变量定义一个具体的<code>name</code>，例如我们想把一个变量取名为<code>&quot;weights&quot;</code>，我们想恢复它的值到一个新的名字叫<code>&quot;param&quot;</code>的变量中。</p>
<p>有时候我们想保存和恢复模型的某一组变量，例如我们训练了一个5层的神经网络，但我们想训练一个新的6层的神经网络，并且从5层的神经网络中恢复参数到新的6层模型的前5层中。</p>
<p>我们可以传入一个变量列表到<code>tf.train.Saver()</code>中，变量在<code>checkpoint</code>文件中的<code>name</code>就是<code>op</code>的<code>name</code>。我们也可以很简单的把<code>names</code>和变量组织成一个Python字典传入<code>tf.train.Saver()</code>中来保存下来，<code>keys</code>是我们使用的<code>names</code>，<code>values</code>是我们的变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(..., name=<span class="string">'v1'</span>)</div><div class="line">v2 = tf.Variable(..., name=<span class="string">'v2'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Pass the variables as a dict:</span></div><div class="line">saver = tf.train.Saver(&#123;<span class="string">'v1'</span>: v1, <span class="string">'v2'</span>: v2&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Or pass them as a list.</span></div><div class="line">saver = tf.train.Saver([v1, v2])</div><div class="line"><span class="comment"># Passing a list is equivalent to passing a dict with the variable op names</span></div><div class="line"><span class="comment"># as keys:</span></div><div class="line">saver = tf.train.Saver(&#123;v.op.name: v <span class="keyword">for</span> v <span class="keyword">in</span> [v1, v2]&#125;)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ol>
<li>如果我们想保存不同模型的几组变量，我们可以创建很多个<code>saver</code>对象。而且相同的变量可以存储在不同的<code>saver</code>对象中，它们的值只有在<code>restore()</code>函数执行后才改变。</li>
<li>如果我们想恢复一组变量到模型中，我们必须事先初始化其他变量。</li>
</ol>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="external">tf.train.Saver  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" rel="external">Variables: Creation, Initialization, Saving, and Loading  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文学习了TensorFlow的一些常用的基础函数，主要为以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tf.group&lt;/li&gt;
&lt;li&gt;tf.Graph.control_dependencies&lt;/li&gt;
&lt;li&gt;tf.train.Saver&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时介绍了训练过程中的模型保存和恢复方法，主要用到&lt;code&gt;tf.train.Saver&lt;/code&gt;类。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（四）：TensorBoard可视化</title>
    <link href="http://zangbo.me/2017/06/27/TensorFlow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://zangbo.me/2017/06/27/TensorFlow 笔记（四）/</id>
    <published>2017-06-27T13:50:54.000Z</published>
    <updated>2017-06-30T04:45:29.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文对TensorFlow的重要可视化工具TensorBoard进行学习。</p>
<p>当我们训练庞大且复杂的神经网络时，为了便于我们理解和调试TensorFlow程序，可视化工具显得尤为重要。而TensorFlow有一个很方便的可视化工具名为TensorBoard，我们可以利用它来可视化我们的<code>graph</code>以及训练的细节。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<hr>
<p>TensorBoard的界面如下图所示：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627153131_zidI6p_tensorboard.jpeg" alt="MNIST TensorBoard" width="700"></p>
<p>界面主要提供以下几种不同的数据可视化类型：</p>
<ul>
<li>SCALARS</li>
<li>IMAGES</li>
<li>AUDIO</li>
<li>GRAPHS</li>
<li>DISTRIBUTIONS</li>
<li>HISTOGRAMS</li>
<li>EMBEDDINGS</li>
<li>TEXT</li>
</ul>
<p><br></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在TensorFlow运行过程中，我们可以通过<code>summary operations</code>操作来保存它的运行日志，TensorBoard通过读取这些运行日志来实现可视化功能。下面是使用TensorBoard的生命周期：</p>
<ol>
<li><p>创建一个<code>TensorFlow graph</code>，用<code>summary operations</code>来保存我们需要了解的节点的信息。</p>
</li>
<li><p>我们需要整合所有节点的信息，如果一个一个提取太耗费精力和时间，我们可以选择<code>tf.summary.merge_all()</code>函数来把所有的<code>op</code>节点整合成一个单一节点，然后通过运行该节点来提取出所有<code>summary op</code>的信息。</p>
</li>
<li><p>用<code>tf.summary.FileWriter()</code>函数把提取出的所有的<code>summary op</code>信息存入磁盘中。我们同时可以用该函数把<code>graph</code>的信息存入磁盘，用来可视化神经网络的结构。</p>
</li>
<li><p>在训练过程中用<code>add_summary</code>命令把训练信息存入磁盘，训练结束后记得把<code>FileWriter</code>对象<code>close</code>。</p>
</li>
<li><p>打开终端，键入以下命令运行TensorBoard：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure>
<p>得到提示信息<code>“Starting TensorBoard...”</code>则运行成功，我们可以打开浏览器，在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板。</p>
<p><br></p>
</li>
</ol>
<h1 id="Summary-Operations"><a href="#Summary-Operations" class="headerlink" title="Summary Operations"></a>Summary Operations</h1><p>我们利用该操作来从TensorFlow运行过程中获得信息，然后在TensorBoard上展示出来，<code>summary ops</code>同样是<code>ops</code>的一种，它们和<code>tf.matmul</code>或者<code>tf.nn.relu</code>等操作一样。这意味着它们和其他<code>ops</code>相同，包含在一个<code>graph</code>中，读入一个<code>tensors</code>并且输出一个<code>tensors</code>。但是它们也有不同的地方，<code>summary ops</code>输出的向量还包含着连续的<code>protobufs</code>信息，它们可以被保存到磁盘上并且被TensorBoard读取进行可视化。</p>
<p>常见的<code>summary ops</code>如下：</p>
<ul>
<li>tf.summary.scalar</li>
<li>tf.summary.image</li>
<li>tf.summary.audio</li>
<li>tf.summary.text</li>
<li>tf.summary.histogram</li>
</ul>
<p><br></p>
<p>我们在这里主要介绍三种：</p>
<h2 id="Scalar"><a href="#Scalar" class="headerlink" title="Scalar"></a>Scalar</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, accuracy)</div></pre></td></tr></table></figure>
<p><code>Scalar</code>存储一些非向量的单一数值，通常会随着迭代次数变化，例如<code>accuracy</code>或者<code>cross_entropy</code>。我们可以在TensorBoard的<code>SCALARS</code>面板看到该类数据的详细信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_XxfLyL_scalar.jpeg" alt="SCALARS" width="700"></p>
<h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.image(<span class="string">'input'</span>, x_image, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<p>我们可以输出包含图像的<code>summary protobufs</code>，图像信息保存在<code>tensor</code>中，并且必须是4-D的形式<code>[batch_size, height, width, channels]</code>，其中，<code>channels</code>可以是以下形式：</p>
<ul>
<li>1: <code>tensor</code> is interpreted as Grayscale.</li>
<li>3: <code>tensor</code> is interpreted as RGB.</li>
<li>4: <code>tensor</code> is interpreted as RGBA.</li>
</ul>
<p>我们可以在TensorBoard的<code>IMAGES</code>面板看到图片信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_YAlrJU_images.jpeg" alt="IMAGES" width="700"></p>
<h2 id="Histogram"><a href="#Histogram" class="headerlink" title="Histogram"></a>Histogram</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.histogram(<span class="string">"weights"</span>, w)</div></pre></td></tr></table></figure>
<p>很多时候我们想要知道某些向量的分布，比如参数<code>weight</code>、<code>bias</code>或者某一层的输出向量，以及它们随着时间的变化规律，这时可以采用Histogram。</p>
<p>在TensorBoard中有两种显示向量分布的方式，分别是面板中的<code>DISTRIBUTIONS</code>和<code>HISTOGRAMS</code>，如下两图所示。</p>
<p><strong>DISTRIBUTIONS：</strong></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_w23F61_distribution.jpeg" alt="DISTRIBUTIONS" width="700"></p>
<p><strong>HISTOGRAMS：</strong></p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180058_uvshkp_histogram.jpeg" alt="HISTOGRAMS" width="700"></p>
<h2 id="Merge-all"><a href="#Merge-all" class="headerlink" title="Merge_all"></a>Merge_all</h2><p>当我们定义好所有ops，我们可以用以下函数来把所有的<code>op</code>节点整合成一个单一节点，然后通过运行该节点来提取出所有<code>summary op</code>的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum = tf.summary.merge_all()</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h1><p>当我们训练无比庞大且复杂的神经网络时，我们一般会希望对整个网络的结构进行可视化，本文开头的图片展示的便是对MNIST数据集一个简单的五层神经网络的可视化。在提取了所有<code>summary_op</code>信息后，我们可以用以下函数把它们写进给定的磁盘目录下，在此同时可以把<code>graph</code>的信息一起写进去，这样我们便可以在TensorBoard的<code>GRAPHS</code>下对整个神经网络的结构进行可视化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">file_writer = tf.summary.FileWriter(&apos;/path/to/logs&apos;, sess.graph)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="Name-scoping-and-nodes"><a href="#Name-scoping-and-nodes" class="headerlink" title="Name scoping and nodes"></a>Name scoping and nodes</h2><p>然而直接保存得到的结构往往无比晦涩难懂，这时我们就可以考虑用前一个笔记中学到的<code>name_scope</code>对变量和<code>ops</code>进行分层。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'layer2'</span>):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'weight'</span>):</div><div class="line">        W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'bias'</span>):</div><div class="line">        b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'conv'</span>):</div><div class="line">        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'pool'</span>):</div><div class="line">        h_pool2 = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<p>这样我们就可以得到本文开端处的图片中展示的结构图，通常默认只显示最高层，双击每一个模块或者单机模块右上方的<code>+</code>，可以进一步展开，我们可以得到进一步详细的结构信息。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170627180059_C4hmAd_graph.jpeg" alt="GRAPHS" width="700"></p>
<p>TensorBoard的<code>graph</code>有两种依赖，<strong>数据依赖</strong>和<strong>控制依赖</strong>。数据依赖展示了<code>tensors</code>在两个<code>ops</code>之间的流动以及流动方向；控制依赖运用了虚线来表示。TensorBoard显示<code>graph</code>有两个区域，<strong>主区域</strong>和<strong>辅助区域</strong>，为了便于观察我们可以把一些高层次的依赖项多的节点从主区域移出到辅助区域，只需要在节点右键点击<code>Remove from main graph</code>即可，如上图所示我们把<code>train</code>移到辅助区域。</p>
<p><br></p>
<h2 id="Runtime-statistics"><a href="#Runtime-statistics" class="headerlink" title="Runtime statistics"></a>Runtime statistics</h2><p>通常来说搜集运行时间的统计数据对我们是很有帮助的，例如总的内存使用，总的计算时间，以及节点的<code>tensor</code>形状。我们可以在<code>GRAPHS</code>的侧边栏处通过选择<code>Compute time</code>和<code>Memory</code>来看每个节点处响应的信息，颜色越深表明相应的值越大。</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170629223648_L7I2IB_Run metadata graph.jpeg" alt="Run metadata graph" width="700"></p>
<p>如下是一段代码示例，我们在第99、199、299……1999处保存<code>Runtime</code>统计数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000</span>):</div><div class="line">    <span class="comment">#...</span></div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:  <span class="comment"># Record execution statistics</span></div><div class="line">        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</div><div class="line">        run_metadata = tf.RunMetadata()</div><div class="line">        summary, _ = sess.run([merged, train_step],</div><div class="line">                              feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;,</div><div class="line">                              options=run_options,</div><div class="line">                              run_metadata=run_metadata)</div><div class="line">        train_writer.add_run_metadata(run_metadata, <span class="string">'step%d'</span> % i)</div><div class="line">        train_writer.add_summary(summary, i)</div><div class="line">        print(<span class="string">'Adding run metadata for'</span>, i)</div><div class="line">    <span class="comment">#...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="运行-TensorBoard"><a href="#运行-TensorBoard" class="headerlink" title="运行 TensorBoard"></a>运行 TensorBoard</h1><p>打开终端，键入以下命令运行TensorBoard：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure>
<p>这里的路径是我们用<code>tf.summary.FileWriter()</code>写入的路径。这里注意我们用<code>FileWriter</code>写入时可能会分为<code>train</code>和<code>test</code>两个路径，这时的TensorBoard路径应该为它们两个的上层路径，例如：</p>
<p><strong>.py文件:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_writer = tf.summary.FileWriter(<span class="string">'/tmp/tensorflow/mnist/my_example/train'</span>, sess.graph)</div><div class="line">test_writer = tf.summary.FileWriter(<span class="string">'/tmp/tensorflow/mnist/my_example/test'</span>)</div></pre></td></tr></table></figure>
<p><strong>终端命令：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=<span class="string">'/tmp/tensorflow/mnist/my_example/'</span></div></pre></td></tr></table></figure>
<p>如果我们想比较不同的网络模型的运行情况，例如我们想改超参数，想知道哪个值能获得更高的准确率，TensorBoard允许我们打开不同的模型保存的<code>log</code>，当TensorBoard打开一个路径时，它会遍历此路径下包括子目录下的所有的事件文件，每次它进入一个子目录，它都会载入它作为一个新的<code>run</code>，前段界面也会通过这个路径来组织数据。例如如下有一个已经组织好的TensorBoard <code>log</code>目录，有两个<code>runs</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/some/path/mnist_experiments/run1/events.out.tfevents.1456525581.name</div><div class="line">/some/path/mnist_experiments/run2/events.out.tfevents.1456525385.name</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=/some/path/mnist_experiments</div></pre></td></tr></table></figure>
<p>然后打开浏览器，在地址栏输入<code>localhost:6006</code>进入TensorBoard的操作面板即可看到可视化结果。</p>
<p>如下是一个不同模型结构训练结果可视化的例子（代码在下面）：</p>
<p><img src="http://orwbuystz.bkt.clouddn.com/Tensorflow学习笔记/20170630113432_HnboU8_more model.jpeg" alt="Different models" width="700"></p>
<p><br></p>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><p>我在MNIST上进行了简单的实现，为了更好的调试超参数，我实验了不同的卷积层、全连接层和是否开启<code>dropout</code>，完整代码可以在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/TensorFlow/TensorBoard" target="_blank" rel="external">GitHub</a>上下载，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Copyright 2017 Zangbo. All Rights Reserved.</span></div><div class="line"><span class="comment"># A simple MNIST classifier which displays summaries in TensorBoard.</span></div><div class="line"><span class="comment"># This is an unimpressive MNIST model, but it is a good example of using</span></div><div class="line"><span class="comment"># tf.name_scope to make a graph legible in the TensorBoard graph explorer, and of</span></div><div class="line"><span class="comment"># naming summary tags so that they are grouped meaningfully in TensorBoard.</span></div><div class="line"><span class="comment"># ==============================================================================</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment">### MNIST datasets ###</span></div><div class="line">LOGDIR = <span class="string">'/tmp/mnist_tutorial/'</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(train_dir=LOGDIR + <span class="string">'data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_layer</span><span class="params">(input, size_in, size_out, name=<span class="string">"conv"</span>)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">    w = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, size_in, size_out], stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div><div class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[size_out]), name=<span class="string">"B"</span>)</div><div class="line">    conv = tf.nn.conv2d(input, w, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line">    act = tf.nn.relu(conv + b)</div><div class="line">    tf.summary.histogram(<span class="string">"weights"</span>, w)</div><div class="line">    tf.summary.histogram(<span class="string">"biases"</span>, b)</div><div class="line">    tf.summary.histogram(<span class="string">"activations"</span>, act)</div><div class="line">    <span class="keyword">return</span> tf.nn.max_pool(act, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_layer</span><span class="params">(input, size_in, size_out, name=<span class="string">"fc"</span>)</span>:</span></div><div class="line">  <span class="keyword">with</span> tf.name_scope(name):</div><div class="line">    w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div><div class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[size_out]), name=<span class="string">"B"</span>)</div><div class="line">    act = tf.nn.relu(tf.matmul(input, w) + b)</div><div class="line">    tf.summary.histogram(<span class="string">"weights"</span>, w)</div><div class="line">    tf.summary.histogram(<span class="string">"biases"</span>, b)</div><div class="line">    tf.summary.histogram(<span class="string">"activations"</span>, act)</div><div class="line">    <span class="keyword">return</span> act</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnist_model</span><span class="params">(learning_rate, use_two_conv, use_two_fc, use_dropout, hparam)</span>:</span></div><div class="line">  tf.reset_default_graph()</div><div class="line">  sess = tf.Session()</div><div class="line"></div><div class="line">  <span class="comment"># Setup placeholders, and reshape the data</span></div><div class="line">  x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">"x"</span>)</div><div class="line">  x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">  tf.summary.image(<span class="string">'input'</span>, x_image, <span class="number">3</span>)</div><div class="line">  y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">"labels"</span>)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> use_two_conv:</div><div class="line">    conv1 = conv_layer(x_image, <span class="number">1</span>, <span class="number">32</span>, <span class="string">"conv1"</span>)</div><div class="line">    conv_out = conv_layer(conv1, <span class="number">32</span>, <span class="number">64</span>, <span class="string">"conv2"</span>)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    conv1 = conv_layer(x_image, <span class="number">1</span>, <span class="number">64</span>, <span class="string">"conv"</span>)</div><div class="line">    conv_out = tf.nn.max_pool(conv1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">  flattened = tf.reshape(conv_out, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">if</span> use_two_fc:</div><div class="line">    fc1 = fc_layer(flattened, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>, <span class="string">"fc1"</span>)</div><div class="line">    <span class="keyword">if</span> use_dropout:</div><div class="line">      <span class="keyword">with</span> tf.name_scope(<span class="string">'dropout'</span>):</div><div class="line">        keep_prob = tf.constant(<span class="number">0.5</span>, name=<span class="string">'keep_prob'</span>)</div><div class="line">        fc1_dropout = tf.nn.dropout(fc1, keep_prob)</div><div class="line">        logits = fc_layer(fc1_dropout, <span class="number">1024</span>, <span class="number">10</span>, <span class="string">"fc2"</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      logits = fc_layer(fc1, <span class="number">1024</span>, <span class="number">10</span>, <span class="string">"fc2"</span>)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    logits = fc_layer(flattened, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">10</span>, <span class="string">"fc"</span>)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"xent"</span>):</div><div class="line">    xent = tf.reduce_mean(</div><div class="line">        tf.nn.softmax_cross_entropy_with_logits(</div><div class="line">            logits=logits, labels=y), name=<span class="string">"xent"</span>)</div><div class="line">    tf.summary.scalar(<span class="string">"xent"</span>, xent)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"train"</span>):</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)</div><div class="line"></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"accuracy"</span>):</div><div class="line">    correct_prediction = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">    tf.summary.scalar(<span class="string">"accuracy"</span>, accuracy)</div><div class="line"></div><div class="line">  summ = tf.summary.merge_all()</div><div class="line"></div><div class="line">  sess.run(tf.global_variables_initializer())</div><div class="line">  writer = tf.summary.FileWriter(LOGDIR + hparam)</div><div class="line">  writer.add_graph(sess.graph)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2001</span>):</div><div class="line">    batch = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">      [train_accuracy, s] = sess.run([accuracy, summ], feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;)</div><div class="line">      writer.add_summary(s, i)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:  <span class="comment"># Record execution statistics</span></div><div class="line">      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</div><div class="line">      run_metadata = tf.RunMetadata()</div><div class="line">      _, s = sess.run([train_step, summ],</div><div class="line">                      feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;,</div><div class="line">                      options=run_options,</div><div class="line">                      run_metadata=run_metadata)</div><div class="line">      writer.add_run_metadata(run_metadata, <span class="string">'step%03d'</span> % i) <span class="comment">#add_run_metadata(run_metadata, tag, global_step=None)</span></div><div class="line">      writer.add_summary(s, i)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      sess.run(train_step, feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>]&#125;)</div><div class="line">  writer.close()</div><div class="line">  sess.close()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_hparam_string</span><span class="params">(learning_rate, use_two_fc, use_two_conv, use_dropout)</span>:</span></div><div class="line">  conv_param = <span class="string">"conv=2"</span> <span class="keyword">if</span> use_two_conv <span class="keyword">else</span> <span class="string">"conv=1"</span></div><div class="line">  fc_param = <span class="string">"fc=2"</span> <span class="keyword">if</span> use_two_fc <span class="keyword">else</span> <span class="string">"fc=1"</span></div><div class="line">  dropout_param = <span class="string">"dropout"</span> <span class="keyword">if</span> use_dropout <span class="keyword">else</span> <span class="string">"no_dropout"</span></div><div class="line">  <span class="keyword">return</span> <span class="string">"lr_%.0E,%s,%s,%s"</span> % (learning_rate, conv_param, fc_param,dropout_param)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">if</span> tf.gfile.Exists(LOGDIR):</div><div class="line">    tf.gfile.DeleteRecursively(LOGDIR)</div><div class="line">  tf.gfile.MakeDirs(LOGDIR)</div><div class="line">  <span class="comment">#Add other param to try different learning rate</span></div><div class="line">  <span class="keyword">for</span> learning_rate <span class="keyword">in</span> [<span class="number">1E-4</span>]:</div><div class="line">    <span class="comment"># Try different model architectures</span></div><div class="line">    <span class="keyword">for</span> use_two_fc <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">      <span class="keyword">for</span> use_two_conv <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">        <span class="keyword">for</span> use_dropout <span class="keyword">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line">          <span class="comment"># Construct a hyperparameter string for each one (example: "lr_1E-4,fc=2,conv=2,dropout)</span></div><div class="line">          hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv, use_dropout)</div><div class="line">          print(<span class="string">'Starting run for %s'</span> % hparam)</div><div class="line"></div><div class="line">          <span class="comment"># Actually run with the new settings</span></div><div class="line">          mnist_model(learning_rate, use_two_fc, use_two_conv, use_dropout, hparam)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">  main()</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>以下链接均来自于Tensorflow官方网站，且均需要翻墙。</p>
<ul>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/tensorboard/README.md" target="_blank" rel="external">tensorflow/README.md at r1.2 · tensorflow/tensorflow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/summary" target="_blank" rel="external">Module: tf.summary  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard: Visualizing Learning  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/get_started/graph_viz" target="_blank" rel="external">TensorBoard: Graph Visualization  |  TensorFlow</a></li>
<li><a href="https://www.youtube.com/watch?v=eBbEDRsCmv4" target="_blank" rel="external">TensorBoard实践介绍（2017年TensorFlow开发大会） - YouTube</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文对TensorFlow的重要可视化工具TensorBoard进行学习。&lt;/p&gt;
&lt;p&gt;当我们训练庞大且复杂的神经网络时，为了便于我们理解和调试TensorFlow程序，可视化工具显得尤为重要。而TensorFlow有一个很方便的可视化工具名为TensorBoard，我们可以利用它来可视化我们的&lt;code&gt;graph&lt;/code&gt;以及训练的细节。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="可视化" scheme="http://zangbo.me/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 笔记（三）：Name_Scope和变量共享</title>
    <link href="http://zangbo.me/2017/06/24/Tensorflow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://zangbo.me/2017/06/24/Tensorflow 笔记（三）/</id>
    <published>2017-06-24T08:09:34.000Z</published>
    <updated>2017-06-29T14:14:59.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文学习了TensorFlow中的四个基本函数以及<strong>变量共享(Sharing Variables)</strong>机制：</p>
<ul>
<li><code>tf.Variable()</code></li>
<li><code>tf.get_variable()</code></li>
<li><code>tf.name_scope()</code></li>
<li><code>tf.variable_scope()</code></li>
</ul>
<p>其中<code>tf.get_variable()</code>和<code>tf.variable_scope()</code>共同构成了Tensorflow的<strong>变量共享</strong>机制。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Variable-和get-variable"><a href="#Variable-和get-variable" class="headerlink" title="Variable()和get_variable()"></a>Variable()和get_variable()</h1><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable()"></a>Variable()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</div></pre></td></tr></table></figure>
<p>函数创建一个变量，该变量的<code>type</code>和<code>shape</code>由初始化的值而定，可以用<code>assign</code>函数改变变量的<code>shape</code>。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create two variables.</span></div><div class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),</div><div class="line">                      name=<span class="string">"weights"</span>)</div><div class="line">biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=<span class="string">"biases"</span>)</div><div class="line">...</div><div class="line"><span class="comment"># Add an op to initialize the variables.</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Later, when launching the model</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Run the init operation.</span></div><div class="line">  sess.run(init_op)</div><div class="line">  ...</div><div class="line">  <span class="comment"># Use the model</span></div><div class="line">  ...</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="get-variable"><a href="#get-variable" class="headerlink" title="get_variable()"></a>get_variable()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = tf.get_variable(name, shape, dtype, initializer)</div></pre></td></tr></table></figure>
<p>函数创建一个新的变量或者返回一个已经存在的变量，它是<code>variable_scope</code>的重要组成部分。</p>
<ol>
<li><p>创建一个新的变量，这时默认<code>tf.get_variable_scope().reuse == False</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/v:0"</span></div></pre></td></tr></table></figure>
</li>
<li><p>为了重新使用(reuse)已经存在的变量，这时改变<code>tf.get_variable_scope().reuse == True</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>注意：</strong></p>
<ol>
<li>该函数必须指定<code>name</code>，它会在当前的<code>variable_scope</code>进行<code>reuse</code>检查。</li>
<li>该函数必须指定<code>shape</code>。</li>
<li>该函数定义时可以不进行初始化， 这样会使用<code>variable_scope</code>的默认初始化，如果这两者都没有，那么就使用<code>glorot_uniform_initializer</code>，即<code>xavier</code>进行初始化。</li>
</ol>
<p>一个基本的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])  <span class="comment"># v.name == "foo/v:0"</span></div><div class="line">    w = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])  <span class="comment"># w.name == "foo/w:0"</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>)  <span class="comment"># v.name == "foo/v:0"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ul>
<li><code>tf.Variable()</code><ol>
<li>可以不定义<code>name</code>。</li>
<li>检测到<code>name</code>冲突，系统会自己处理，不会报错。</li>
<li>每次都创建新的变量，<code>reuse=True</code>对它无影响。</li>
<li>需要初始化。</li>
</ol>
</li>
<li><code>tf.get_variable()</code><ol>
<li>必须定义<code>name</code>。</li>
<li>检测到<code>name</code>冲突，系统不会处理，会报错。</li>
<li><code>reuse=True</code>对它有影响，此时如果已经创建了变量对象，则把对象返回，如果没有，就创建一个新的。</li>
<li>可以不用初始化。</li>
<li>共享变量时使用。</li>
</ol>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>):</div><div class="line">    w1 = tf.get_variable(<span class="string">"w1"</span>, shape=[])</div><div class="line">    w2 = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"w2"</span>)</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">    w1_p = tf.get_variable(<span class="string">"w1"</span>, shape=[])</div><div class="line">    w2_p = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w2"</span>)</div><div class="line"></div><div class="line">print(w1 <span class="keyword">is</span> w1_p, w2 <span class="keyword">is</span> w2_p)</div><div class="line"></div><div class="line"><span class="comment">#输出</span></div><div class="line"><span class="comment">#True  False</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="name-scope-和variable-scope"><a href="#name-scope-和variable-scope" class="headerlink" title="name_scope()和variable_scope()"></a>name_scope()和variable_scope()</h1><h2 id="name-scope"><a href="#name-scope" class="headerlink" title="name_scope()"></a>name_scope()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">name_scope(name)</div></pre></td></tr></table></figure>
<p>TensorFlow 可以有数以千计的节点，如此多而难以一下全部看到，甚至无法使用标准图表工具来展示。为简单起见，我们为<code>op</code>名划定范围，并且把该信息用于在图表中的节点上定义一个层级。默认情况下， 只有顶层节点会显示。一个<code>graph</code>是无数<code>name_scope</code>堆叠而来，用命令<code>with name_scope(...):</code>可以添加一个新的<code>name_scope</code>。</p>
<ul>
<li>如果输入一个字符串，那么会建立一个以该字符串命名的<code>name_scope</code>，在此下面定义的所有<code>op</code>均属于该<code>name_scope</code>。如果该字符串已经被定义，则会把命名改为<code>字符串_*</code>，<code>*</code>代表从1开始的阿拉伯数字。例如已经存在一个名为<code>&quot;a&quot;</code>的<code>name_scope</code>，再定义一个名为<code>&quot;a&quot;</code>的<code>name_scope</code>时，会自动把该<code>name_scope</code>改名为<code>&quot;a_1&quot;</code>，数字会依次累加。</li>
<li>如果用<code>with name_scope(...) as scope:</code>指令，那么可以利用该<code>name_scope</code>通过命令<code>with name_scope(scope):</code>重新进入已经建立好的<code>name_scope</code>下定义<code>op</code>。</li>
<li>如果<code>with name_scope(&quot;&quot;)</code>参数为空，则会重置当前<code>name_scope</code>为顶层空<code>name_scope</code>。</li>
</ul>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">  c = tf.constant(<span class="number">5.0</span>, name=<span class="string">"c"</span>)</div><div class="line">  <span class="keyword">assert</span> c.op.name == <span class="string">"c"</span></div><div class="line">  c_1 = tf.constant(<span class="number">6.0</span>, name=<span class="string">"c"</span>)</div><div class="line">  <span class="keyword">assert</span> c_1.op.name == <span class="string">"c_1"</span></div><div class="line"></div><div class="line">  <span class="comment"># Creates a scope called "nested"</span></div><div class="line">  <span class="keyword">with</span> g.name_scope(<span class="string">"nested"</span>) <span class="keyword">as</span> scope:</div><div class="line">    nested_c = tf.constant(<span class="number">10.0</span>, name=<span class="string">"c"</span>)</div><div class="line">    <span class="keyword">assert</span> nested_c.op.name == <span class="string">"nested/c"</span></div><div class="line"></div><div class="line">    <span class="comment"># Creates a nested scope called "inner".</span></div><div class="line">    <span class="keyword">with</span> g.name_scope(<span class="string">"inner"</span>):</div><div class="line">      nested_inner_c = tf.constant(<span class="number">20.0</span>, name=<span class="string">"c"</span>)</div><div class="line">      <span class="keyword">assert</span> nested_inner_c.op.name == <span class="string">"nested/inner/c"</span></div><div class="line"></div><div class="line">    <span class="comment"># Create a nested scope called "inner_1".</span></div><div class="line">    <span class="keyword">with</span> g.name_scope(<span class="string">"inner"</span>):</div><div class="line">      nested_inner_1_c = tf.constant(<span class="number">30.0</span>, name=<span class="string">"c"</span>)</div><div class="line">      <span class="keyword">assert</span> nested_inner_1_c.op.name == <span class="string">"nested/inner_1/c"</span></div><div class="line"></div><div class="line">      <span class="comment"># Treats `scope` as an absolute name_scope, and</span></div><div class="line">      <span class="comment"># switches to the "nested/" scope.</span></div><div class="line">      <span class="keyword">with</span> g.name_scope(scope):</div><div class="line">        nested_d = tf.constant(<span class="number">40.0</span>, name=<span class="string">"d"</span>)</div><div class="line">        <span class="keyword">assert</span> nested_d.op.name == <span class="string">"nested/d"</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> g.name_scope(<span class="string">""</span>):</div><div class="line">          e = tf.constant(<span class="number">50.0</span>, name=<span class="string">"e"</span>)</div><div class="line">          <span class="keyword">assert</span> e.op.name == <span class="string">"e"</span></div></pre></td></tr></table></figure>
<p><code>with g.name_scope(...) as scope:</code>中的<code>scope</code>存放了该<code>scope</code>的名字，可以在之后的<code>op</code>中直接使用该<code>scope</code>来给<code>op</code>命名。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</div><div class="line">  <span class="keyword">with</span> g.name_scope(<span class="string">'my_layer'</span>) <span class="keyword">as</span> scope:</div><div class="line">    output = tf.constant(<span class="number">1</span>, name=scope)</div><div class="line"><span class="keyword">assert</span> output.op.name == <span class="string">"my_layer"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="variable-scope"><a href="#variable-scope" class="headerlink" title="variable_scope()"></a>variable_scope()</h2><p><code>variable_scope</code>函数将会在变量名前面加个前缀，同时标注<code>reuse</code>。利用缩进形式会分成有层次的<code>variable_scope</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">        v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/bar/v:0"</span></div></pre></td></tr></table></figure>
<p>我们可以通过<code>tf.get_variable_scope()</code>来得到当前的<code>variable_scope</code>，同时可以利用<code>tf.get_variable_scope().reuse_variables():</code>把当前的<code>variable_scope</code>的<code>reuse</code>改成<code>True</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">    tf.get_variable_scope().reuse_variables()</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div></pre></td></tr></table></figure>
<p>当一个<code>variable_scope</code>的<code>reuse</code>默认为<code>False</code>，但被改成了<code>True</code>以后，是无法再改回<code>False</code>的，而且如果一个<code>variable_scope</code>的<code>reuse</code>被改成了<code>True</code>，它下属的所有<code>variable_scope</code>的<code>reues</code>就都是<code>True</code>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"root"</span>):</div><div class="line">    <span class="comment"># At start, the scope is not reusing.</span></div><div class="line">    <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">        <span class="comment"># Opened a sub-scope, still not reusing.</span></div><div class="line">        <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">        <span class="comment"># Explicitly opened a reusing scope.</span></div><div class="line">        <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">True</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">            <span class="comment"># Now sub-scope inherits the reuse flag.</span></div><div class="line">            <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">True</span></div><div class="line">    <span class="comment"># Exited the reusing scope, back to a non-reusing one.</span></div><div class="line">    <span class="keyword">assert</span> tf.get_variable_scope().reuse == <span class="keyword">False</span></div></pre></td></tr></table></figure>
<p>我们也可以直接用<code>variable_scope</code>来代替参数名字进入该<code>variable_scope</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>) <span class="keyword">as</span> foo_scope:</div><div class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(foo_scope):</div><div class="line">    w = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.variable_scope(foo_scope, reuse=<span class="keyword">True</span>):</div><div class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">    w1 = tf.get_variable(<span class="string">"w"</span>, [<span class="number">1</span>])</div><div class="line"><span class="keyword">assert</span> v1 <span class="keyword">is</span> v</div><div class="line"><span class="keyword">assert</span> w1 <span class="keyword">is</span> w</div></pre></td></tr></table></figure>
<p>当我们用之前存在的<code>variable_scope</code>进入一个<code>variable_scope</code>时，我们会跳出当前的<code>variable_scope</code>而进入一个完全不同的<code>variable_scope</code>，哪怕它在某个<code>variable_scope</code>的缩进下面，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>) <span class="keyword">as</span> foo_scope:</div><div class="line">    <span class="keyword">assert</span> foo_scope.name == <span class="string">"foo"</span></div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"baz"</span>) <span class="keyword">as</span> other_scope:</div><div class="line">        <span class="keyword">assert</span> other_scope.name == <span class="string">"bar/baz"</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(foo_scope) <span class="keyword">as</span> foo_scope2:</div><div class="line">            <span class="keyword">assert</span> foo_scope2.name == <span class="string">"foo"</span>  <span class="comment"># Not changed.</span></div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="区别-1"><a href="#区别-1" class="headerlink" title="区别"></a>区别</h2><p><code>name_scope</code>是给<code>op_name</code>加前缀，<code>variable_scope</code>是给<code>get_variable()</code>创建的变量的名字加前缀。</p>
<p><code>name_scope</code>可以在<code>variable_scope</code>中打开，但是<code>name_scope</code>仅仅会影响<code>op</code>的<code>name</code>，而不会影响<code>variable</code>的<code>name</code>，但是<code>variable_scope</code>的会影响<code>op</code>的<code>name</code>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"bar"</span>):</div><div class="line">        v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</div><div class="line">        x = <span class="number">1.0</span> + v</div><div class="line"><span class="keyword">assert</span> v.name == <span class="string">"foo/v:0"</span></div><div class="line"><span class="keyword">assert</span> x.op.name == <span class="string">"foo/bar/add"</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="Sharing-Variables"><a href="#Sharing-Variables" class="headerlink" title="Sharing Variables"></a>Sharing Variables</h1><p>Tensorflow的<strong>变量共享(Sharing Variables)</strong>机制主要是由<code>tf.get_variable()</code>和<code>tf.variable_scope()</code>构成的。</p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>设想一下，假如我们有一个神经网络，它只有两层卷积层组成，而且两层卷积层的输入输出尺寸相同并且卷积核尺寸也相同，如果我们用<code>tf.Variable()</code>来定义这样一个模型，代码大概是下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></div><div class="line">    conv1_weights = tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>]),</div><div class="line">        name=<span class="string">"conv1_weights"</span>)</div><div class="line">    conv1_biases = tf.Variable(tf.zeros([<span class="number">32</span>]), name=<span class="string">"conv1_biases"</span>)</div><div class="line">    conv1 = tf.nn.conv2d(input_images, conv1_weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    relu1 = tf.nn.relu(conv1 + conv1_biases)</div><div class="line"></div><div class="line">    conv2_weights = tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>]),</div><div class="line">        name=<span class="string">"conv2_weights"</span>)</div><div class="line">    conv2_biases = tf.Variable(tf.zeros([<span class="number">32</span>]), name=<span class="string">"conv2_biases"</span>)</div><div class="line">    conv2 = tf.nn.conv2d(relu1, conv2_weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    <span class="keyword">return</span> tf.nn.relu(conv2 + conv2_biases)</div></pre></td></tr></table></figure>
<p>其中有四个不同的变量：<code>conv1_weights</code>、<code>conv1_biases</code>、<code>conv2_weights</code>、<code>conv2_biases</code>，那么问题出现了，当我们反复使用这样一个模型时，每次都要开辟新的存储空间给四个变量，例如我们有两张不同的图片<code>image1</code>和<code>image2</code>，我们先后把它们送入该模型中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># First call creates one set of 4 variables.</span></div><div class="line">result1 = my_image_filter(image1)</div><div class="line"><span class="comment"># Another set of 4 variables is created in the second call.</span></div><div class="line">result2 = my_image_filter(image2)</div></pre></td></tr></table></figure>
<p>总共创建了8个不同的变量。极大的浪费了资源，如果图片数量增多，模型变得复杂，那么耗费的资源是巨大的。因此为了解决这个问题，Tensorflow出现了变量共享的机制。</p>
<p><br></p>
<h2 id="变量共享方法"><a href="#变量共享方法" class="headerlink" title="变量共享方法"></a>变量共享方法</h2><p>下面，我们用<strong>变量共享</strong>来实现上面的模型：</p>
<p>首先利用<code>tf.get_variable()</code>定义一个实现卷积操作的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></div><div class="line">    <span class="comment"># Create variable named "weights".</span></div><div class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape,</div><div class="line">        initializer=tf.random_normal_initializer())</div><div class="line">    <span class="comment"># Create variable named "biases".</span></div><div class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape,</div><div class="line">        initializer=tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">    conv = tf.nn.conv2d(input, weights,</div><div class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</div></pre></td></tr></table></figure>
<p>然后利用<code>tf.variable_scope()</code>来创建两个在不同<code>variable_scope</code>的卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</div><div class="line">        <span class="comment"># Variables created here will be named "conv1/weights", "conv1/biases".</span></div><div class="line">        relu1 = conv_relu(input_images, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</div><div class="line">        <span class="comment"># Variables created here will be named "conv2/weights", "conv2/biases".</span></div><div class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>如果我们直接调用<code>my_image_filter</code>两次会发生什么呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">result1 = my_image_filter(image1)</div><div class="line">result2 = my_image_filter(image2)</div><div class="line"><span class="comment"># ValueError(... conv1/weights already exists ...)</span></div></pre></td></tr></table></figure>
<p>程序报错，<code>tf.get_variable()</code>会检查变量是否已经存在并且是否共享。如果想共享它们，可以通过设置<code>reuse_variables()</code>来实现，它会把当前的<code>reuse</code>改为<code>True</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"image_filters"</span>) <span class="keyword">as</span> scope:</div><div class="line">    result1 = my_image_filter(image1)</div><div class="line">    scope.reuse_variables()</div><div class="line">    result2 = my_image_filter(image2)</div></pre></td></tr></table></figure>
<p>这样不管我们处理多少张图片，都只会创建四个变量，极大的节省了资源。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>以下链接均来自于Tensorflow官方文档，且均需要翻墙。</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="external">tf.Variable  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/get_variable" target="_blank" rel="external">tf.get_variable  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/name_scope" target="_blank" rel="external">tf.name_scope  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope" target="_blank" rel="external">tf.Graph  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" target="_blank" rel="external">tf.variable_scope  |  TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/variable_scope" target="_blank" rel="external">Sharing Variables  |  TensorFlow</a></li>
</ul>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文学习了TensorFlow中的四个基本函数以及&lt;strong&gt;变量共享(Sharing Variables)&lt;/strong&gt;机制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.Variable()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.get_variable()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.name_scope()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.variable_scope()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中&lt;code&gt;tf.get_variable()&lt;/code&gt;和&lt;code&gt;tf.variable_scope()&lt;/code&gt;共同构成了Tensorflow的&lt;strong&gt;变量共享&lt;/strong&gt;机制。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://zangbo.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>爬虫笔记（四）：获取某地历史天气数据</title>
    <link href="http://zangbo.me/2017/06/22/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://zangbo.me/2017/06/22/爬虫学习笔记（四）/</id>
    <published>2017-06-22T11:28:01.000Z</published>
    <updated>2017-06-22T13:02:02.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>本文记录了爬虫的一个简单实战应用，因为前不久做了个竞赛需要用到历史天气，所以就写了个简单的爬虫程序，利用Requests库和Beautiful Soup库爬取某地的历史天气记录。</p>
<p>中间的数据整合方便起见调用了numpy库和pandas库，关于两者的详细使用方法大家可以去查阅相关资料。</p>
<p>所有代码基于python3.5版本。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>最近参加了天池的一个大数据竞赛，因为需要用到江苏省扬中市的历史天气信息，所以就写了个简单的爬虫程序来爬取。有很多网站都提供了历史天气，这里我们选择的网站是：<a href="http://lishi.tianqi.com/yangzhong/index.html" target="_blank" rel="external">扬中市历史天气-历史天气网</a>。</p>
<p><br></p>
<h1 id="分析网站"><a href="#分析网站" class="headerlink" title="分析网站"></a>分析网站</h1><ol>
<li><p>打开上述网站，往下滑动，可以看到有一栏记录了该地每个月的天气信息，右键打开审查元素，查看该信息所在的标签，发现每个月的天气信息都在标签<code>&lt;div class=&#39;tqtongji1&#39;&gt;</code>下面的<code>&lt;a&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_p0q980_2.jpeg" alt="历史月份信息"></center>

<p><br></p>
</li>
<li><p>用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;tqtongji1&#39;).find_all(&#39;a&#39;)</code>命令得到所有月份的信息，提取出其中的链接，这里只提取最近30个月的信息。</p>
<p><br></p>
</li>
<li><p>打开其中一个月份链接，右键审查元素，发现该月份每天的天气信息都在标签<code>&lt;div class=&#39;tqtongji2&#39;&gt;</code>下面的<code>&lt;li&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_XtWKBE_3.jpeg" alt="部分天气信息"></center>

<p><br></p>
</li>
<li><p>用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;tqtongji2&#39;).find_all(&#39;li&#39;)</code>命令得到该月份每天的部分天气信息并保存在<code>list</code>中，同时用<code>find(&#39;div&#39;, class_=&#39;tqtongji2&#39;).find_all(&#39;a&#39;)</code>命令提取出其中的链接。</p>
<p><br></p>
</li>
<li><p>打开其中一天的链接，右键审查元素，发现该天的其他天气信息都在标签<code>&lt;div class=&#39;history_sh&#39;&gt;</code>下面的<code>&lt;span&gt;</code>标签中。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/爬虫笔记/20170622200503_XNW30r_4.jpeg" alt="其他天气信息"></center>

<p><br></p>
</li>
<li><p>用Beautiful Soup对象的<code>find(&#39;div&#39;, class_=&#39;history_sh&#39;).find_all(&#39;span&#39;)</code>命令提取出该天的其他天气信息并保存在<code>list</code>中。</p>
<p><br></p>
</li>
<li><p>利用numpy数组和pandas的DataFrame结构对提取出的数据进行整合，最后输出为CSV文件。</p>
</li>
</ol>
<p><br></p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">	</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">History_weather</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_url</span><span class="params">(self, url)</span>:</span></div><div class="line">        all_weather_index = self.get_all_weather_index(url)</div><div class="line">        all_weather_index = all_weather_index[:<span class="number">30</span>]</div><div class="line">        print(<span class="string">"Get all weather index!"</span>)</div><div class="line">        result_weather = self.get_all_weather(all_weather_index)</div><div class="line">        result_weather.to_csv(<span class="string">'all_weather.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">        print(<span class="string">'Save all weather success!'</span>)</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_soup</span><span class="params">(self, url)</span>:</span></div><div class="line">        headers = &#123;</div><div class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 \</span></div><div class="line">            (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8'&#125;</div><div class="line">        html = requests.get(url, headers)</div><div class="line">        soup = BeautifulSoup(html.text, <span class="string">'lxml'</span>)</div><div class="line">        <span class="keyword">return</span> soup</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_all_weather_index</span><span class="params">(self, url)</span>:</span></div><div class="line">        all_weather_soup = self.get_soup(url)</div><div class="line">        all_weather_index = all_weather_soup.find(</div><div class="line">            <span class="string">'div'</span>, class_=<span class="string">'tqtongji1'</span>).find_all(<span class="string">'a'</span>)</div><div class="line">        <span class="keyword">return</span> all_weather_index</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_all_weather</span><span class="params">(self, all_weather_index)</span>:</span></div><div class="line">        all_month_weather = list()</div><div class="line">        day_weather = list()</div><div class="line">        	</div><div class="line">        <span class="keyword">for</span> weather <span class="keyword">in</span> all_weather_index:</div><div class="line">            month_url = weather[<span class="string">'href'</span>]</div><div class="line">            month_name = weather.get_text()</div><div class="line">            month_weather_soup = self.get_soup(month_url)</div><div class="line">            month_weather = month_weather_soup.find(</div><div class="line">                <span class="string">'div'</span>, class_=<span class="string">'tqtongji2'</span>).find_all(<span class="string">'li'</span>)</div><div class="line">            day_weather_url = month_weather_soup.find(</div><div class="line">                <span class="string">'div'</span>, class_=<span class="string">'tqtongji2'</span>).find_all(<span class="string">'a'</span>)</div><div class="line">            <span class="keyword">for</span> day <span class="keyword">in</span> day_weather_url:</div><div class="line">                day_url = day[<span class="string">'href'</span>]</div><div class="line">                day_soup = self.get_soup(day_url)</div><div class="line">                day_text = day_soup.find(</div><div class="line">                    <span class="string">'div'</span>, class_=<span class="string">'history_sh'</span>).find_all(<span class="string">'span'</span>)</div><div class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> day_text:</div><div class="line">                    day_weather.append(i.get_text())</div><div class="line">            weather_list = list()</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> month_weather:</div><div class="line">                weather_list.append(i.get_text())</div><div class="line">            weather_list = weather_list[<span class="number">6</span>:]</div><div class="line">            all_month_weather = all_month_weather + weather_list</div><div class="line">            print(<span class="string">"Get weather :"</span> + month_name)</div><div class="line">            	</div><div class="line">        day_weather = np.array(day_weather).reshape(<span class="number">-1</span>, <span class="number">8</span>)</div><div class="line">        day_weather = DataFrame(day_weather, columns=[</div><div class="line">            <span class="string">'Ultraviolet'</span>, <span class="string">'Dress'</span>, <span class="string">'Travel'</span>, <span class="string">'Comfort_level'</span>,</div><div class="line">            <span class="string">'Morning_exercise'</span>, <span class="string">'Car_wash'</span>, <span class="string">'Drying_index'</span>, <span class="string">'Breath_allergy'</span>])</div><div class="line">        all_month_weather = np.array(all_month_weather).reshape(<span class="number">-1</span>, <span class="number">6</span>)</div><div class="line">        all_month_weather = DataFrame(all_month_weather, columns=[</div><div class="line">            <span class="string">'Date'</span>, <span class="string">'Max_temp'</span>, <span class="string">'Min_temp'</span>, </div><div class="line">            <span class="string">'Whether'</span>, <span class="string">'Wind_direction'</span>, <span class="string">'Wind_power'</span>])</div><div class="line">        print(all_month_weather.shape)</div><div class="line">        print(day_weather.shape)</div><div class="line">        result_weather = pd.merge(</div><div class="line">            all_month_weather, day_weather, left_index=<span class="keyword">True</span>, right_index=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> result_weather</div><div class="line">	</div><div class="line">history_weather = History_weather()</div><div class="line">begin_url = <span class="string">'http://lishi.tianqi.com/yangzhong/index.html'</span></div><div class="line">history_weather.begin_url(begin_url)</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>打开<code>all_weather.csv</code>文件：</p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Max_temp</th>
<th>Min_temp</th>
<th>Whether</th>
<th>…</th>
<th>Breath_allergy</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017-04-01</td>
<td>18</td>
<td>8</td>
<td>晴</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>2017-04-02</td>
<td>22</td>
<td>10</td>
<td>晴</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>2017-04-03</td>
<td>24</td>
<td>12</td>
<td>多云</td>
<td>…</td>
<td>极易</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>2014-11-30</td>
<td>10</td>
<td>2</td>
<td>多云转晴</td>
<td>…</td>
<td>极不易发</td>
</tr>
</tbody>
</table>
<p>共得到910天、14项天气特征数据。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>以上只是简单的调用Requests和Beautiful Soup来进行爬取，是一个非常简单的爬虫实现，后面我们会学习更加高阶的爬虫方法。</p>
<p>爬虫源代码可以在我的<a href="https://github.com/zangbo/MachineLearning/tree/master/Web%20Crawler/history%20weather" target="_blank" rel="external">Github</a>上下载。</p>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;本文记录了爬虫的一个简单实战应用，因为前不久做了个竞赛需要用到历史天气，所以就写了个简单的爬虫程序，利用Requests库和Beautiful Soup库爬取某地的历史天气记录。&lt;/p&gt;
&lt;p&gt;中间的数据整合方便起见调用了numpy库和pandas库，关于两者的详细使用方法大家可以去查阅相关资料。&lt;/p&gt;
&lt;p&gt;所有代码基于python3.5版本。&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘" scheme="http://zangbo.me/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="爬虫" scheme="http://zangbo.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫笔记（三）：BeautifulSoup基础</title>
    <link href="http://zangbo.me/2017/06/15/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://zangbo.me/2017/06/15/爬虫学习笔记（三）/</id>
    <published>2017-06-15T13:48:57.000Z</published>
    <updated>2017-06-23T02:17:12.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Beautiful Soup库以及一些基础用法，并主要介绍了<code>find_all()</code>和<code>find()</code>    两个搜索方法。利用Beautiful Soup我们可以很方便的从页面中提取信息，该库兼容python2和3版本。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Beautiful-Soup介绍"><a href="#Beautiful-Soup介绍" class="headerlink" title="Beautiful Soup介绍"></a>Beautiful Soup介绍</h1><p>我们之前已经用Requests获得了需要的页面，接下来需要从页面中提取出我们想要的信息，我们可以选择用正则表达式，但对于很多新手来说正则表达式使用的不熟练会产生很多不必要的麻烦。而对于很多任务来说，有一个工具即高效又便捷，它就是Beautiful Soup。</p>
<p>Beautiful Soup是一个可以从HTML或XML文件中提取数据的Python库，它能够通过你喜欢的转换器实现惯用的文档导航，查找，修改文档的方式。因为BeautifulSoup用法太多，本文仅介绍在爬虫中常用的功能，如果需要了解更多，请查阅官方文档。</p>
<p>文章中代码输出结果用#表示。</p>
<p><br></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>下面一段HTML代码将作为我们的例子反复使用：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">html_doc = """</div><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">title</span>&gt;</span>The Dormouse's story<span class="tag">&lt;/<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span><span class="tag">&lt;<span class="name">b</span>&gt;</span>The Dormouse's story<span class="tag">&lt;/<span class="name">b</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"story"</span>&gt;</span>Once upon a time there were three little sisters; and their names were</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/elsie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link1"</span>&gt;</span>Elsie<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/lacie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link2"</span>&gt;</span>Lacie<span class="tag">&lt;/<span class="name">a</span>&gt;</span> and</div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://example.com/tillie"</span> <span class="attr">class</span>=<span class="string">"sister"</span> <span class="attr">id</span>=<span class="string">"link3"</span>&gt;</span>Tillie<span class="tag">&lt;/<span class="name">a</span>&gt;</span>;</div><div class="line">and they lived at the bottom of a well.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"story"</span>&gt;</span>...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">"""</div></pre></td></tr></table></figure>
<p>使用BeautifulSoup解析这段代码，能够得到一个<code>BeautifulSoup</code>的对象，并能按照标准的缩进格式的结构输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc)</div><div class="line">	</div><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;title&gt;</span></div><div class="line"><span class="comment">#    The Dormouse's story</span></div><div class="line"><span class="comment">#   &lt;/title&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="title"&gt;</span></div><div class="line"><span class="comment">#    &lt;b&gt;</span></div><div class="line"><span class="comment">#     The Dormouse's story</span></div><div class="line"><span class="comment">#    &lt;/b&gt;</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;</span></div><div class="line"><span class="comment">#     Elsie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ,</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Lacie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    and</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/tillie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Tillie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ; and they lived at the bottom of a well.</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    ...</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="安装-Beautiful-Soup"><a href="#安装-Beautiful-Soup" class="headerlink" title="安装 Beautiful Soup"></a>安装 Beautiful Soup</h1><p>Beautiful Soup有3和4两个版本，3版本目前已经停止开发，建议使用4版本。</p>
<p>我们可以很方便的使用pip进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install beautifulsoup4</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="安装解析器"><a href="#安装解析器" class="headerlink" title="安装解析器"></a>安装解析器</h1><p>Beautiful Soup支持Python标准库中的HTML解析器，还支持一些第三方的解析器，下表列出了主要的解析器，以及它们的优缺点：</p>
<table>
<thead>
<tr>
<th>解析器</th>
<th>使用方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python标准库</td>
<td><code>BeautifulSoup(markup, &quot;html.parser&quot;)</code></td>
<td>Python的内置标准库执行速度适中文档容错能力强</td>
<td>Python 2.7.3 or 3.2.2前的版本中文档容错能力差</td>
</tr>
<tr>
<td>lxml HTML 解析器</td>
<td><code>BeautifulSoup(markup, &quot;lxml&quot;)</code></td>
<td>速度快文档容错能力强</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>lxml XML 解析器</td>
<td><code>BeautifulSoup(markup, [&quot;lxml&quot;, &quot;xml&quot;])``BeautifulSoup(markup, &quot;xml&quot;)</code></td>
<td>速度快唯一支持XML的解析器</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>html5lib</td>
<td><code>BeautifulSoup(markup, &quot;html5lib&quot;)</code></td>
<td>最好的容错性以浏览器的方式解析文档生成HTML5格式的文档</td>
<td>速度慢不依赖外部扩展</td>
</tr>
</tbody>
</table>
<p>这里推荐使用lxml作为解析器，因为它的效率更高。</p>
<p>可以用pip来安装lxml：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install lxml</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="使用Beautiful-Soup"><a href="#使用Beautiful-Soup" class="headerlink" title="使用Beautiful Soup"></a>使用Beautiful Soup</h1><p>将一段文档传入BeautifulSoup 的构造方法，就能得到一个文档的对象，我们可以制定解析器。例如，我们将开头的例子传入BeautifulSoup，并使用lxml解析器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure>
<p>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种： <code>Tag</code> , <code>NavigableString</code> , <code>BeautifulSoup</code> , <code>Comment</code> 。</p>
<p><br></p>
<p>我们重点讲两个搜索方法：<code>find()</code>和<code>find_all()</code>，我们用这两种方法可以根据HTML文档的标签快速找到我们需要的内容。</p>
<p>以开头提到的文档作为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">html_doc = <span class="string">"""</span></div><div class="line">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</div><div class="line">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</div><div class="line">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</div><div class="line">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</div><div class="line">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</div><div class="line">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</div><div class="line">and they lived at the bottom of a well.&lt;/p&gt;</div><div class="line">&lt;p class="story"&gt;...&lt;/p&gt;</div><div class="line">"""</div><div class="line">	</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc,<span class="string">'lxml'</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="find-all"><a href="#find-all" class="headerlink" title="find_all()"></a>find_all()</h2><p><strong>find_all(name, attrs, recursive, text, kwargs)</strong></p>
<h3 id="name-参数"><a href="#name-参数" class="headerlink" title="name 参数"></a>name 参数</h3><p><code>name</code>参数可以查找所有名字为 <code>name</code> 的tag：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div></pre></td></tr></table></figure>
<p><code>name</code>参数的值可以是字符串、正则表达式、列表、方法或是 <code>True</code></p>
<p>例如：</p>
<p>在<code>find_all()</code>中传入一个<strong>字符串</strong>参数，Beautiful Soup会查找与字符串完整匹配的内容，下面的例子用于查找文档中所有的<code>&lt;b&gt;</code>标签:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'b'</span>)</div><div class="line"><span class="comment"># [&lt;b&gt;The Dormouse's story&lt;/b&gt;]</span></div></pre></td></tr></table></figure>
<p>如果传入<strong>列表</strong>参数，Beautiful Soup会将与列表中任一元素匹配的内容返回。下面代码找到文档中所有<code>&lt;a&gt;</code>标签和<code>&lt;b&gt;</code>标签:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all([<span class="string">"a"</span>, <span class="string">"b"</span>])</div><div class="line"><span class="comment"># [&lt;b&gt;The Dormouse's story&lt;/b&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;, </span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><code>find_all()</code>方法搜索当前tag的所有tag子节点，并判断是否符合过滤器的条件。</p>
<p><br></p>
<h3 id="keyword-参数"><a href="#keyword-参数" class="headerlink" title="keyword 参数"></a>keyword 参数</h3><ol>
<li><p>如果一个指定名字的参数不是搜索内置的参数名，搜索时会把该参数当作指定名字tag的属性来搜索，如果包含一个名字为 <code>id</code> 的参数，Beautiful Soup会搜索每个tag的”id”属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(id=<span class="string">'link2'</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><br></p>
</li>
<li><p>如果传入 <code>href</code> 参数，Beautiful Soup会搜索每个tag的”href”属性:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(href=re.compile(<span class="string">"elsie"</span>))</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><br></p>
</li>
<li><p>利用<code>true</code>参数在文档树中查找所有包含 <code>id</code> 属性的tag，无论 <code>id</code> 的值是什么:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.find_all(id=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
</li>
</ol>
<p>搜索指定名字的属性时可以使用的参数值包括字符串、正则表达式、列表或是 <code>True</code></p>
<p><br></p>
<h3 id="按CSS搜索"><a href="#按CSS搜索" class="headerlink" title="按CSS搜索"></a>按CSS搜索</h3><p>按照CSS类名搜索tag的功能非常实用，但标识CSS类名的关键字 <code>class</code> 在Python中是保留字，使用 <code>class</code> 做参数会导致语法错误。这里我们可以通过 <code>class_</code> 参数搜索有指定CSS类名的tag:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>, class_=<span class="string">"sister"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure>
<p><code>class_</code> 参数同样接受不同类型的过滤器：字符串、正则表达式、方法或 <code>True</code> 。</p>
<p><br></p>
<h3 id="text-参数"><a href="#text-参数" class="headerlink" title="text 参数"></a>text 参数</h3><p>通过 <code>text</code> 参数可以搜搜文档中的字符串内容。与 <code>name</code> 参数的可选值一样，<code>text</code> 参数接受字符串、正则表达式、方法或 <code>True</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">soup.find_all(text=<span class="string">"Elsie"</span>)</div><div class="line"><span class="comment"># [u'Elsie']</span></div><div class="line">	</div><div class="line">soup.find_all(text=[<span class="string">"Tillie"</span>, <span class="string">"Elsie"</span>, <span class="string">"Lacie"</span>])</div><div class="line"><span class="comment"># [u'Elsie', u'Lacie', u'Tillie']</span></div><div class="line">	</div><div class="line">soup.find_all(text=re.compile(<span class="string">"Dormouse"</span>))</div><div class="line"><span class="comment"># [u"The Dormouse's story", u"The Dormouse's story"]</span></div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_the_only_string_within_a_tag</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="string">""</span>Return <span class="keyword">True</span> <span class="keyword">if</span> this string <span class="keyword">is</span> the only child of its parent tag.<span class="string">""</span></div><div class="line">    <span class="keyword">return</span> (s == s.parent.string)</div><div class="line">	</div><div class="line">soup.find_all(text=is_the_only_string_within_a_tag)</div><div class="line"><span class="comment"># [u"The Dormouse's story", u"The Dormouse's story", u'Elsie', u'Lacie', u'Tillie', u'...']</span></div></pre></td></tr></table></figure>
<p><br></p>
<h3 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h3><p>下面两行代码是等价的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>)</div><div class="line">soup(<span class="string">"a"</span>)</div></pre></td></tr></table></figure>
<p>这两行也是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.title.find_all(text=<span class="keyword">True</span>)</div><div class="line">soup.title(text=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="find"><a href="#find" class="headerlink" title="find()"></a>find()</h2><p><strong>find(name, attrs, recursive, text, kwargs)</strong></p>
<p><code>find_all()</code> 方法将返回文档中符合条件的所有tag，尽管有时候我们只想得到一个结果.比如文档中只有一个<code>&lt;body&gt;</code>标签，那么使用 <code>find_all()</code>方法来查找<code>&lt;body&gt;</code>标签就不太合适， 使用 <code>find_all</code> 方法并设置 <code>limit=1</code> 参数不如直接使用 <code>find()</code> 方法。下面两行代码是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'title'</span>, limit=<span class="number">1</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div><div class="line">	</div><div class="line">soup.find(<span class="string">'title'</span>)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure>
<p>唯一的区别是 <code>find_all()</code> 方法的返回结果是值包含一个元素的列表,而 <code>find()</code> 方法直接返回结果.</p>
<p><code>find_all()</code> 方法没有找到目标是返回空列表， <code>find()</code> 方法找不到目标时返回 <code>None</code>。</p>
<p><br></p>
<h3 id="简写-1"><a href="#简写-1" class="headerlink" title="简写"></a>简写</h3><p><code>soup.head.title</code>是利用tag的名字来简写，这个简写的原理就是多次调用当前tag的<code>find()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.head.title</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div><div class="line">	</div><div class="line">soup.find(<span class="string">"head"</span>).find(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="输出文本"><a href="#输出文本" class="headerlink" title="输出文本"></a>输出文本</h1><p>如果只想得到tag中包含的文本内容，那么可以采用<code>get_text()</code>方法，这个方法获取到tag中包含的所有文版内容，包括子孙tag中的内容，并将结果作为Unicode字符串返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">'&lt;a href="http://example.com/"&gt;\nI linked to &lt;i&gt;example.com&lt;/i&gt;\n&lt;/a&gt;'</span></div><div class="line">soup = BeautifulSoup(markup)</div><div class="line">	</div><div class="line">soup.get_text()</div><div class="line"><span class="comment"># u'\nI linked to example.com\n'</span></div><div class="line">soup.i.get_text()</div><div class="line"><span class="comment"># u'example.com'</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="关于编码"><a href="#关于编码" class="headerlink" title="关于编码"></a>关于编码</h1><p>使用Beautiful Soup解析后，文档都被转换成了Unicode。</p>
<p>Beautiful Soup用了<strong>编码自动检测</strong>子库来识别当前文档编码并转换成Unicode编码。<code>BeautifulSoup</code>对象的<code>.original_encoding</code>属性记录了自动识别编码的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">"&lt;h1&gt;Sacr\xc3\xa9 bleu!&lt;/h1&gt;"</span></div><div class="line">soup = BeautifulSoup(markup,<span class="string">'lxml'</span>)</div><div class="line">soup.original_encoding</div><div class="line"><span class="comment"># 'utf-8'</span></div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p>如果先用<code>r = requests.get(url)</code>指令获得了html页面，建议先用指令<code>r.encoding</code>判断<code>r.text</code>所用的编码方式和源html是否相同（可以通过查看html中的charset= 来判断源html采用的编码方式），如果不同的话要用<code>r.encoding=</code>进行修改，再用<code>soup = BeautifulSoup(r.text,&#39;lxml&#39;)</code>命令。因为<code>r.text</code>会根据当前的编码方式把html转化为Unicode编码输出，如果html包含汉字，而没有采用<code>utf-8</code>进行编码的话，<code>r.text</code>没有办法把汉字转化为相应的Unicode编码，再送进Beautiful Soup中也就无法得到汉字信息了。</p>
<p>也可以直接用<code>soup = BeautifulSoup(r.content,&#39;lxml&#39;)</code>指令，因为<code>r.content</code>返回的是bytes型原始编码信息，而Beautiful Soup可以用<strong>编码自动检测</strong>子库来识别当前文档编码并转换成Unicode编码，但获取文本信息时不建议这样使用，一般来说图像等多媒体内容用content更多一些。</p>
<p><br></p>
<p>而通过Beautiful Soup<strong>输出文档</strong>时，不管输入文档是什么编码方式，输出编码均为UTF-8编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">markup = <span class="string">b'''</span></div><div class="line">&lt;html&gt;</div><div class="line">  &lt;head&gt;</div><div class="line">    &lt;meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" /&gt;</div><div class="line">  &lt;/head&gt;</div><div class="line">  &lt;body&gt;</div><div class="line">    &lt;p&gt;Sacr\xe9 bleu!&lt;/p&gt;</div><div class="line">  &lt;/body&gt;</div><div class="line">&lt;/html&gt;</div><div class="line">'''</div><div class="line">	</div><div class="line">soup = BeautifulSoup(markup)</div><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;meta content="text/html; charset=utf-8" http-equiv="Content-type" /&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p&gt;</span></div><div class="line"><span class="comment">#    Sacré bleu!</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure>
<p>如果不想用UTF-8编码输出，可以将编码方式传入<code>prettify()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(soup.prettify(<span class="string">"latin-1"</span>))</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;meta content="text/html; charset=latin-1" http-equiv="Content-type" /&gt;</span></div><div class="line"><span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>关于Beautiful Soup还有很多其他用法，这里只是重点介绍了运用最多的两个搜索方法<code>find_all()</code>和<code>find()</code>，如果想了解更多的使用方法，请参考官方文档。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>官方文档：<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html#" target="_blank" rel="external">Beautiful Soup Documentation — Beautiful Soup 4.4.0 documentation</a></p>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本文介绍了Beautiful Soup库以及一些基础用法，并主要介绍了&lt;code&gt;find_all()&lt;/code&gt;和&lt;code&gt;find()&lt;/code&gt;    两个搜索方法。利用Beautiful Soup我们可以很方便的从页面中提取信息，该库兼容python2和3版本。
    
    </summary>
    
      <category term="数据挖掘" scheme="http://zangbo.me/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="爬虫" scheme="http://zangbo.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫笔记（二）：Requests基础</title>
    <link href="http://zangbo.me/2017/06/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://zangbo.me/2017/06/14/爬虫学习笔记（二）/</id>
    <published>2017-06-14T02:47:57.000Z</published>
    <updated>2017-06-21T16:54:59.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Requests库以及一些基础用法，基于python3.5版本<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Requests介绍"><a href="#Requests介绍" class="headerlink" title="Requests介绍"></a>Requests介绍</h1><p>Requests是一个基于urllib3的HTTP库，简单优美，支持python2.6–3.5的各版本，具有无比强大的功能，完全满足今日web的需求。在爬虫学习过程中，我们用它向服务器发送请求，以获取我们需要的信息。</p>
<p><br></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>只需要在终端中运行以下代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install requests</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>我们先来看一个具体的实例来感受下Request：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'http://zangbo.me'</span>, auth=(<span class="string">'user'</span>, <span class="string">'pass'</span>))</div><div class="line">r.status_code</div><div class="line">r.headers[<span class="string">'content-type'</span>]</div><div class="line">r.encoding</div><div class="line">r.text</div></pre></td></tr></table></figure>
<p>以下是输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">200</span></div><div class="line"><span class="string">'text/html; charset=utf-8'</span></div><div class="line"><span class="string">'utf-8'</span></div><div class="line"><span class="string">'&lt;!DOCTYPE html&gt;'</span></div></pre></td></tr></table></figure>
<p>以上代码我们请求了本站的地址，然后依次打印出状态码，请求头，编码方式和相应内容。</p>
<p>是不是功能强大，下面，我们正式开始学习Requests。</p>
<p><br></p>
<h1 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h1><p>首先导入Requests模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div></pre></td></tr></table></figure>
<p>Requests包含各种不同的请求方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>)</div><div class="line">r = requests.put(<span class="string">"http://httpbin.org/put"</span>)</div><div class="line">r = requests.delete(<span class="string">"http://httpbin.org/delete"</span>)</div><div class="line">r = requests.head(<span class="string">"http://httpbin.org/get"</span>)</div><div class="line">r = requests.options(<span class="string">"http://httpbin.org/get"</span>)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get请求</h2><p>我们用的最多的是get请求，用来获取某个网页：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</div></pre></td></tr></table></figure>
<p>现在，我们有一个名叫r的Response对象，它包含了所有关于该页面的信息，我们可以从中提取出我们想要的信息。</p>
<p><br></p>
<h2 id="带参数的get请求"><a href="#带参数的get请求" class="headerlink" title="带参数的get请求"></a>带参数的get请求</h2><p>如果你是手工构建 URL，那么数据会以键/值对的形式置于 URL 中，跟在一个问号的后面，我们可以用字典的形式传递URL参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</div><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=payload)</div><div class="line">print(r.url)</div></pre></td></tr></table></figure>
<p>可以得到输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http://httpbin.org/get?key2=value2&amp;key1=value1</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h2><p>如果我们想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。只需简单地传递一个字典给 data 参数，该字典在发出请求时会自动编码为表单形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=payload)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  ...</div><div class="line">  <span class="string">"form"</span>: &#123;</div><div class="line">    <span class="string">"key2"</span>: <span class="string">"value2"</span>,</div><div class="line">    <span class="string">"key1"</span>: <span class="string">"value1"</span></div><div class="line">  &#125;,</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="获取响应内容"><a href="#获取响应内容" class="headerlink" title="获取响应内容"></a>获取响应内容</h1><h2 id="响应状态码"><a href="#响应状态码" class="headerlink" title="响应状态码"></a>响应状态码</h2><p>我们可以检测发送请求是否成功获得想要的数据，通过检测响应状态码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</div><div class="line">r.status_code</div></pre></td></tr></table></figure>
<p>若输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">200</div></pre></td></tr></table></figure>
<p>表明获取成功。</p>
<p><br></p>
<h2 id="文本内容"><a href="#文本内容" class="headerlink" title="文本内容"></a>文本内容</h2><p>以百度首页为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">"https://www.baidu.com"</span>)</div><div class="line">r.text</div></pre></td></tr></table></figure>
<p>Requests会基于HTTP的头部对响应的编码进行推测，让我们访问<code>r.text</code>时，Requests会使用其推测的文本编码，我们可以用<code>r.encoding</code>改变解码方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">r.encoding</div><div class="line">r.encoding = <span class="string">'ISO-8859-1'</span></div><div class="line">r.encoding</div></pre></td></tr></table></figure>
<p>以上代码输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="string">'utf-8'</span></div><div class="line"><span class="string">'ISO-8859-1'</span></div></pre></td></tr></table></figure>
<p>如果你改变了编码，每当你访问 <code>r.text</code> ，Request 都将会使用 <code>r.encoding</code> 的新值。当不确定时，可以使用 <code>r.content</code> 来找到编码，然后设置 <code>r.encoding</code> 为相应的编码。这样就能使用正确的编码解析 <code>r.text</code> 了。关于<code>r.content</code>下面会介绍。</p>
<p><br></p>
<h2 id="二进制内容"><a href="#二进制内容" class="headerlink" title="二进制内容"></a>二进制内容</h2><p>对于非文本请求，例如图片等多媒体内容，可以以字节的方式访问：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r.content</div></pre></td></tr></table></figure>
<p>这里注意两者的区别：</p>
<ul>
<li>r.text返回的是Unicode型的数据。如果是读取文本，可以通过r.text。</li>
<li>r.content返回的是bytes型也就是二进制的数据。如果读取图片，文件，可以通过r.content来实现。</li>
</ul>
<p>例如我们下载并保存一张图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">jpg_url = <span class="string">'http://mm.howkuai.com/wp-content/uploads/2017a/04/13/01.jpg'</span><span class="comment">#一张美女图</span></div><div class="line">content = requests.get(jpg_url).content</div><div class="line"><span class="keyword">with</span> open(<span class="string">'image.jpg'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f: <span class="comment">#写入多媒体文件必须要 b 这个参数！</span></div><div class="line">    f.write(content)</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="JSON内容"><a href="#JSON内容" class="headerlink" title="JSON内容"></a>JSON内容</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'https://github.com/timeline.json'</span>)</div><div class="line">r.json()</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="响应头"><a href="#响应头" class="headerlink" title="响应头"></a>响应头</h2><p>通过命令可以查看服务器响应头，以一个Python字典形式展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">r = requests.get(<span class="string">'http://zangbo.me'</span>)</div><div class="line">r.headers</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">	<span class="string">'Server'</span>: <span class="string">'GitHub.com'</span>, </div><div class="line">	<span class="string">'Date'</span>: <span class="string">'Thu, 15 Jun 2017 04:04:47 GMT'</span>, </div><div class="line">	<span class="string">'Content-Type'</span>: <span class="string">'text/html; charset=utf-8'</span>, </div><div class="line">	<span class="string">'Transfer-Encoding'</span>: <span class="string">'chunked'</span>, </div><div class="line">	<span class="string">'Last-Modified'</span>: <span class="string">'Wed, 14 Jun 2017 15:41:23 GMT'</span>, </div><div class="line">	<span class="string">'Access-Control-Allow-Origin'</span>: <span class="string">'*'</span>, </div><div class="line">	<span class="string">'Expires'</span>: <span class="string">'Thu, 15 Jun 2017 04:14:47 GMT'</span>, </div><div class="line">	<span class="string">'Cache-Control'</span>: <span class="string">'max-age=600'</span>, </div><div class="line">	<span class="string">'Content-Encoding'</span>: <span class="string">'gzip'</span>, </div><div class="line">	<span class="string">'X-GitHub-Request-Id'</span>: <span class="string">'E1D6:0B0F:15941BE:1E637E3:5942075F'</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><br></p>
<h2 id="获取响应内容总结"><a href="#获取响应内容总结" class="headerlink" title="获取响应内容总结"></a>获取响应内容总结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">r.status_code<span class="comment">#获取状态码</span></div><div class="line">r.text<span class="comment">#文本内容</span></div><div class="line">r.encoding<span class="comment">#查看编码</span></div><div class="line">r.encoding = <span class="string">'ISO-8859-1'</span><span class="comment">#改变编码</span></div><div class="line">r.content<span class="comment">#二进制内容</span></div><div class="line">r.json()<span class="comment">#JSON内容</span></div><div class="line">r.raw<span class="comment">#原始内容</span></div><div class="line">r.headers<span class="comment">#响应头</span></div></pre></td></tr></table></figure>
<p><br></p>
<h1 id="定制请求头"><a href="#定制请求头" class="headerlink" title="定制请求头"></a>定制请求头</h1><p>有些网站具有反爬虫机制，我们有时候需要把爬虫伪装成浏览器来访问网页，因此我们需要为请求添加HTTP头部，只需要简单地传递一个<code>dict</code>给<code>headers</code>参数即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">url = <span class="string">"https://www.baidu.com"</span></div><div class="line">headers = &#123;<span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8'</span>&#125;</div><div class="line">r = requests.get(url,headers=headers)</div></pre></td></tr></table></figure>
<p>注：如何查看自己浏览器的userAgent</p>
<p>打开浏览器的控制台，输入：javascript:alert(navigator.userAgent)</p>
<p><br></p>
<h1 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h1><p>我们可以告诉 requests 在经过以 <code>timeout</code> 参数设定的秒数时间之后停止等待响应：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">requests.get(<span class="string">'http://github.com'</span>, timeout=<span class="number">0.001</span>)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<p><code>timeout</code> 仅对连接过程有效，与响应体的下载无关。 <code>timeout</code> 并不是整个下载响应的时间限制，而是如果服务器在 <code>timeout</code> 秒内没有应答，将会引发一个异常（更精确地说，是在 <code>timeout</code> 秒内没有从基础套接字上接收到任何字节的数据时）</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>本文基于Requests官方文档：</p>
<ul>
<li>英文版：<a href="http://docs.python-requests.org/en/master/" target="_blank" rel="external">Requests: HTTP for Humans</a></li>
<li>中文版：<a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">Requests: 让HTTP服务人类</a></li>
</ul>
<p>本文仅介绍基础用法，更高阶以及更详细的内容（Cookie、重定向与请求历史、错误与异常等）请参考官方文档。</p>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本文介绍了Requests库以及一些基础用法，基于python3.5版本
    
    </summary>
    
      <category term="数据挖掘" scheme="http://zangbo.me/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="爬虫" scheme="http://zangbo.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫笔记（一）：爬虫基础知识</title>
    <link href="http://zangbo.me/2017/06/13/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://zangbo.me/2017/06/13/爬虫学习笔记（一）/</id>
    <published>2017-06-13T04:43:52.000Z</published>
    <updated>2017-06-21T16:53:05.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本系列记录了爬虫学习的历程，本文整理了学习爬虫需要掌握的基础背景知识，包括HTML、URL在内的基础概念以及爬虫的工作原理。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h1><p>HTML中文全称“超文本标记语言”。超文本指页面内可以包含图片、链接，甚至音乐、程序等非文字元素。</p>
<p>超文本标记语言包括Head部分和Body部分，其中Head部分提供关于网页的信息，Body部分提供网页的具体内容。</p>
<ul>
<li><code>&lt;head&gt;&lt;/head&gt;；</code>这2个标记符分别表示头部信息的开始和结尾。头部中包含的标记是页面的标题、序言、说明等内容，它本身不作为内容来显示，但影响网页显示的效果。</li>
<li><code>&lt;body&gt;&lt;/body&gt;；</code>网页中显示的实际内容均包含在这2个正文标记符之间。正文标记符又称为实体标记。</li>
</ul>
<p><br></p>
<h1 id="URI"><a href="#URI" class="headerlink" title="URI"></a>URI</h1><p>统一资源标识符(Uniform Resource Identifier)，是一个用于标识某一互联网资源名称的字符串。Web上可用的每种资源：HTML文档、图像、视频片段、程序等，由一个URI进行定位。</p>
<p>URI由三部分组成：</p>
<ol>
<li>访问资源的命名机制</li>
<li>存放资源的主机名</li>
<li>资源自身的名称，由路径表示，着重强调于资源。</li>
</ol>
<p><br></p>
<h1 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h1><p>统一资源定位符(Uniform Resource Location)，是URI的一个子集，URI确定一个资源，URL不仅确定一个资源，还告诉你它在哪里。互联网上每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。</p>
<p>URL一般由三部组成：</p>
<ol>
<li>协议(或称为服务方式)</li>
<li>存有该资源的服务器名称或者主机IP地址(有时也包括端口号)</li>
<li>主机资源的具体地址。如目录和文件名等。</li>
</ol>
<p>以下是一个例子：</p>
<p><code>https://tianchi.aliyun.com/competition/information.htm?raceId=231602</code></p>
<p>协议://授权/路径?查询</p>
<ul>
<li>第一部分和第二部分用“://”符号隔开，</li>
<li>第二部分和第三部分用“/”符号隔开。</li>
<li>第一部分和第二部分是不可缺少的，第三部分有时可以省略。</li>
</ul>
<p>URI表示请求服务器的路径，定义这么一个资源，而URL同时说明要如何访问这个资源（通过http://）</p>
<p><br></p>
<h1 id="浏览网页的过程和爬虫原理"><a href="#浏览网页的过程和爬虫原理" class="headerlink" title="浏览网页的过程和爬虫原理"></a>浏览网页的过程和爬虫原理</h1><p>浏览器作为一个客户端，输入地址后，会向服务器端发送一次请求(Request)，服务器正常的话会返回一个响应(Response)，响应的内容是请求打开的页面内容。浏览器收到相应后，对信息进行相应处理，页面就显示在我们面前了。</p>
<p>爬虫的流程相似，我们通过http库向目标站点发送一个Request(里面包含URL)，如果服务器响应，会得到一个Response，我们在利用正则表达式等工具对它进行解析，最后保存数据。</p>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本系列记录了爬虫学习的历程，本文整理了学习爬虫需要掌握的基础背景知识，包括HTML、URL在内的基础概念以及爬虫的工作原理。
    
    </summary>
    
      <category term="数据挖掘" scheme="http://zangbo.me/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="爬虫" scheme="http://zangbo.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>编码相关知识</title>
    <link href="http://zangbo.me/2017/06/13/%E7%BC%96%E7%A0%81%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/"/>
    <id>http://zangbo.me/2017/06/13/编码相关知识/</id>
    <published>2017-06-13T03:40:30.000Z</published>
    <updated>2017-06-22T13:17:53.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文整理了一些常见的编码，它们的起源、区分以及在Python 3中的使用，因为平时被各种不同的编码所困扰，索性就整理出来。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="编码的起源"><a href="#编码的起源" class="headerlink" title="编码的起源"></a>编码的起源</h1><p>在计算机中，所有的数据在存储和运算时都要使用二进制数表示（因为计算机用高电平和低电平分别表示1和0），例如像a、b、c、d这样的52个字母（包括大写）、以及0、1等数字还有一些常用的符号（例如*、#、@等）在计算机中存储时也要使用二进制数来表示，而具体用哪些二进制数字表示哪个符号，当然每个人都可以约定自己的一套（这就叫编码），而大家如果要想互相通信而不造成混乱，那么大家就必须使用相同的编码规则。</p>
<p><br></p>
<h1 id="ASCII"><a href="#ASCII" class="headerlink" title="ASCII"></a>ASCII</h1><p>为了有一套统一的编码规则，美国国家标准协会就出台了ASCII编码，统一规定了上述常用符号用哪些二进制数来表示，后来被国际标准化组织（ISO）定位了国际标准。</p>
<p>标准ASCII码使用指定的7位二进制数组合来表示128种可能的字符，而由于电脑开发初期就建立了8位元位元组，即1byte=8bits，因此如果使用一个位元组来保存字元，则可以存储256个码，于是就诞生了扩展ASCII码。</p>
<p><br></p>
<h1 id="ISO8859-1"><a href="#ISO8859-1" class="headerlink" title="ISO8859-1"></a>ISO8859-1</h1><p>之前说过，标准ASCII是针对英语设计的，当处理带有音调标号（形如汉语的拼音）的亚洲文字时就会出现问题。因此，创建出了一些包括255个字符的由ASCII扩展的字符集。</p>
<p>其中一种扩展的8位字符集是ISO 8859-1 Latin 1，也简称为ISOLatin-1。它把位于128-255之间的字符用于拉丁字母表中特殊语言字符的编码，也因此而得名。ISOLatin-1向下兼容ASCII。</p>
<p><br></p>
<h1 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h1><p>然而欧洲语言不是地球上的唯一语言，亚洲和非洲语言并不能被8位字符集所支持。仅汉语字母表就有80000以上个字符。但是把汉语、日语和越南语的一些相似的字符结合起来，在不同的语言里，使不同的字符代表不同的字，这样只用2个字节就可以编码地球上几乎所有地区的文字。因此，创建了Unicode编码，它通过增加一个高字节对ISO Latin-1字符集进行扩展，当这些高字节位为0时，低字节就是ISO Latin-1字符。最初的Unicode标准UCS-2是使用两个字节表示一个字符，后来有人嫌少使用4个字节表示一个字符，即UCS-4，不过我们用的最多的还是UCS-2。</p>
<p>总的来说，Unicode是计算机科学领域里的一项业界标准，包括字符集、编码方案等。Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。</p>
<p><br></p>
<h1 id="UTF"><a href="#UTF" class="headerlink" title="UTF"></a>UTF</h1><p>对可以用ASCII表示的字符使用Unicode并不高效，因为Unicode比ASCII占用大一倍的空间，而对ASCII来说高字节的0对他毫无用处。为了解决这个问题，就出现了通用转换格式，即UTF（Unicode Transformation Format）。</p>
<center><img src="http://i1.buimg.com/597140/576550baa99295f6.png" alt="Markdown"></center>

<p>换句话说，在计算机<strong>内存</strong>中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，是由UTF负责的。最简单的一种形式是直接用UCS的码位来保存，这就是UTF-16，比如，”汉”直接使用\x6C\x49保存(UTF-16-BE)，或是倒过来使用\x49\x6C保存(UTF-16-LE)。但用着用着美国人觉得自己吃了大亏，以前英文字母只需要一个字节就能保存了，现在大锅饭一吃变成了两个字节，空间消耗大了一倍……于是UTF-8横空出世。</p>
<p><strong>UTF-8</strong>是一种可变长度字符编码。它用1到6个字节编码Unicode字符。用在网页上可以统一页面显示中文简体繁体及其它语言（如英文，日文，韩文）。</p>
<p>此外，常见的UTF格式还有：UTF-7、UTF-7.5、UTF-8、UTF-16、UTF-32</p>
<p><br></p>
<h2 id="Python-3中的编码"><a href="#Python-3中的编码" class="headerlink" title="Python 3中的编码"></a>Python 3中的编码</h2><p>在python 3版本中，字符串是以Unicode编码的，也就是说，python 3字符串支持中英或其他各种语言。此外，我们可以利用<code>.encode()</code>函数来把Unicode字符串进行编码，编码后的字符串前面会用b表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">u = <span class="string">'汉'</span></div><div class="line">print(u)</div><div class="line">s = u.encode(<span class="string">'UTF-8'</span>)</div><div class="line">print(s)</div><div class="line">d = s.decode(<span class="string">'UTF-8'</span>)</div><div class="line">print(d)</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">'汉'</span></div><div class="line">b<span class="string">'\xe6\xb1\x89'</span></div><div class="line"><span class="string">'汉'</span></div></pre></td></tr></table></figure>
<p>关于python 3中读取文件：</p>
<blockquote>
<p>文件总是存储为编码好的数据，因此，为了使用文件中读取的文本数据，必须首先将其解码为一个Unicode字符串。python 3中，文本正常情况下会自动为你解码，所以打开或读取文件会得到一个Unicode字符串。使用的解码方式取决系统，在Mac OS 或者 大多数Linux系统中，首选编码是UTF-8，但Windows不一定。可以使用locale.getpreferredencoding()方法得到系统的默认解码方式。</p>
</blockquote>
<p>注意：python 2和python 3在编码方面具有较大差别，具体可以查阅相关资料，因为我平时只用python 3，所以这里就不提供了。</p>
<p><br></p>
<h1 id="GB2312"><a href="#GB2312" class="headerlink" title="GB2312"></a>GB2312</h1><p>汉字编码字符集，GB2312编码适用于汉字处理、汉字通信等系统之间的信息交换，通行于中国大陆；新加坡等地也采用此编码。中国大陆几乎所有的中文系统和国际化的软件都支持GB 2312。</p>
<p><br></p>
<h1 id="GBK"><a href="#GBK" class="headerlink" title="GBK"></a>GBK</h1><p>GBK全称《汉字内码扩展规范》，向下与 GB 2312 编码兼容，使用了双字节编码方案，共23940个码位，共收录了21003个汉字。</p>
<p><br></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在计算机<strong>内存</strong>中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，是由UTF负责的，所以在很多网页的头文件可以看到编码方式utf-8，这表明该网页使用utf-8编码的。</p>
<p>另外在编程语言的使用过程中，切记关注编码方式是很重要的一件事。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://www.cnblogs.com/284628487a/p/5584714.html" target="_blank" rel="external">Python3 字符编码 - Coder25 - 博客园</a></li>
<li>百度百科</li>
</ul>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本文整理了一些常见的编码，它们的起源、区分以及在Python 3中的使用，因为平时被各种不同的编码所困扰，索性就整理出来。
    
    </summary>
    
      <category term="计算机基础" scheme="http://zangbo.me/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>配置GithubPage二级域名</title>
    <link href="http://zangbo.me/2017/06/12/%E9%85%8D%E7%BD%AEGithubPage%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D/"/>
    <id>http://zangbo.me/2017/06/12/配置GithubPage二级域名/</id>
    <published>2017-06-12T14:10:55.000Z</published>
    <updated>2017-06-22T14:53:19.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>因为女友生日做了两个网站，就思考该如何把它们放在GithubPage下，并且设置成二级域名，查阅了一堆资料终于明白GithubPage二级域名该如何配置，于是整理在下面。</excerpt></p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="Github-Page种类"><a href="#Github-Page种类" class="headerlink" title="Github Page种类"></a>Github Page种类</h1><ol>
<li><p><code>UserPage</code>: 用户的整个站点，这个是最初github支持的类型，创建一个形如username.github.com的项目就可以，用户想要在github上托管网站，这个是必须要建的，而且命名方式固定。</p>
</li>
<li><p><code>ProjectPage</code>: 用户创建出来的其他网站项目也可以托管在github上，首先新建一个项目，然后建立一个名叫<code>gh-pages</code>的branch，这个branch里的文件就是page的站点文件。</p>
<p><br></p>
</li>
</ol>
<h1 id="UserPage默认域名"><a href="#UserPage默认域名" class="headerlink" title="UserPage默认域名"></a>UserPage默认域名</h1><p>用户站点的默认域名是<code>username.github.io</code>，比如我的站点就是<code>zangbo.github.io</code></p>
<p><br></p>
<h1 id="ProjectPage默认域名"><a href="#ProjectPage默认域名" class="headerlink" title="ProjectPage默认域名"></a>ProjectPage默认域名</h1><p>其他网站项目的默认域名，是使用UserPage域名加上二级目录实现的，比如我有个项目叫<code>mylove</code>，那么该项目的站点就是访问 <code>zangbo.github.io/mylove</code></p>
<p><br></p>
<h1 id="UserPage自定义域名"><a href="#UserPage自定义域名" class="headerlink" title="UserPage自定义域名"></a>UserPage自定义域名</h1><p>我有自己的域名，如何绑定到UserPage? 比如用<code>www.zangbo.me</code>替代<code>zangbo.github.io</code>，它是使用CNAME技术来实现的。</p>
<p>具体步骤可以参考我前些日子写的文章：《Mac环境利用GitHub和Hexo搭建个人博客》，除了添加<code>CNAME</code>指向还应添加<code>A</code>指向。</p>
<blockquote>
<p>CNAME指向之后，当浏览器访问<code>www.zangbo.me</code>的时候浏览器就知道<code>实际上</code>是访问<code>zangbo.github.io</code><br>添加CNAME 文件之后，当GithubPage服务器接收到访问<code>www.zangbo.me</code>的http请求，就知道，对应的是这个工程了。</p>
</blockquote>
<p><br></p>
<h1 id="ProjectPage自定义域名"><a href="#ProjectPage自定义域名" class="headerlink" title="ProjectPage自定义域名"></a>ProjectPage自定义域名</h1><p>比如用<code>mylove.zangbo.me</code>替代<code>zangbo.github.io/mylove</code></p>
<ol>
<li><p>同样的，去域名注册商那里，做一个<code>CNAME</code>指向，将<code>mylove.zangbo.me</code> 指向 <code>zangbo.github.io</code>，如果以后会有很多二级域名都指过来，操作方式相同。</p>
</li>
<li><p>在<code>zangbo/mylove</code>这个项目(也就是page项目)根目录下建一个<code>CNAME</code>文件，里面填写<code>mylove.zangbo.me</code>，然后提交到仓库;</p>
</li>
<li><p>等几分钟。</p>
<p><br></p>
</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.zhaoxiaodan.com/%E5%85%B6%E4%BB%96/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEGithubPage%E7%9A%84%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D.html" target="_blank" rel="external">如何配置GithubPage的二级域名</a></p>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;因为女友生日做了两个网站，就思考该如何把它们放在GithubPage下，并且设置成二级域名，查阅了一堆资料终于明白GithubPage二级域名该如何配置，于是整理在下面。&lt;/excerpt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="http://zangbo.me/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="个人网站" scheme="http://zangbo.me/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>送给女友的生日礼物</title>
    <link href="http://zangbo.me/2017/06/12/%E9%80%81%E5%A5%B3%E5%8F%8B%E7%9A%84%E7%94%9F%E6%97%A5%E7%A4%BC%E7%89%A9/"/>
    <id>http://zangbo.me/2017/06/12/送女友的生日礼物/</id>
    <published>2017-06-12T13:36:59.000Z</published>
    <updated>2017-06-23T12:05:23.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><excerpt in="" index="" |="" 首页摘要=""> 

<p>女朋友生日来临之际，自己在别人模版基础上修改着做了两个网站当作礼物，并没有兼容手机端，最好在电脑端登录观看效果。</p>
<a id="more"></a>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<div id="aplayer0" class="aplayer" style="margin-bottom: 20px;"></div>
		<script>
			new APlayer({
				element: document.getElementById("aplayer0"),
				narrow: false,
				autoplay: false,
				showlrc: 0,
				music: {
					title: "童话镇",
					author: "陈一发儿",
					url: "http://mp3.haoduoge.com/s/2016-12-24/1482568978.mp3",
					pic: "http://p4.music.126.net/tfa811GLreJI_S0h9epqRA==/3394192426154346.jpg?param=130y130",
				}
			});
		</script>
<p><br></p>
<h1 id="我的礼物"><a href="#我的礼物" class="headerlink" title="我的礼物"></a>我的礼物</h1><p>以下是我的两个网站：</p>
<ul>
<li>博莎小站：<a href="http://lisa.zangbo.me" target="_blank" rel="external">http://lisa.zangbo.me</a></li>
<li>送给Lisa的一封信：<a href="http://mylove.zangbo.me" target="_blank" rel="external">http://mylove.zangbo.me</a></li>
</ul>
<p>注：手机端打开会出现排版问题，建议电脑浏览器浏览。</p>
<p><br></p>
<h1 id="参考网站"><a href="#参考网站" class="headerlink" title="参考网站"></a>参考网站</h1><ul>
<li><p>第一个网站参考：<a href="http://huyuqian.chengdazhi.com" target="_blank" rel="external">治芊小站</a></p>
<p>作者主页：<a href="http://chengdazhi.com" target="_blank" rel="external">程大治</a></p>
<p>发现这个网站是在知乎问题 <a href="https://www.zhihu.com/question/37804443" target="_blank" rel="external">程序员能给女朋友做什么特别浪漫的礼物？ - 知乎</a> 下，其中不乏很多很好的创意，通过评论得知作者也是在网上找的模版，很容易就搜到了：<a href="http://www.cssmoban.com/cssthemes/6077.shtml" target="_blank" rel="external">简洁瀑布流布局动物世界博客模板</a>，于是直接进行二次加工。</p>
</li>
</ul>
<ul>
<li><p>第二个网站参考：<a href="http://hackerzhou.me/ex_love/" target="_blank" rel="external">Our Love Story</a></p>
<p>作者GitHub：<a href="https://github.com/hackerzhou" target="_blank" rel="external">hackerzhou</a></p>
<p>这个网站在圈子里可以说是很有名了，作者开发于2011年，当年送的人如今已经变成了前女友，不免一阵唏嘘。作者把代码放在GitHub上允许大家自由的进行二次开发，我便拿来进行修改，因为女朋友并非程序员，所以原本的网页中代码生成部分改为了一封信。</p>
</li>
</ul>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>因为妹子是文科生，送之前我也不知道妹子会不会喜欢这种理工男的浪漫，所以这两个网页只能算是礼物的一部分，自然还是要准备其他实质性的东西的。</p>
<p><br></p>
</the></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;女朋友生日来临之际，自己在别人模版基础上修改着做了两个网站当作礼物，并没有兼容手机端，最好在电脑端登录观看效果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="个人网站" scheme="http://zangbo.me/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/"/>
    
      <category term="前端" scheme="http://zangbo.me/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec笔记：Negative Sample</title>
    <link href="http://zangbo.me/2017/06/06/Word2Vec%E4%B9%8BNegative-Sample/"/>
    <id>http://zangbo.me/2017/06/06/Word2Vec之Negative-Sample/</id>
    <published>2017-06-06T09:08:43.000Z</published>
    <updated>2017-06-21T16:52:33.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Word2Vec中对Skip Gram模型的改进——Negative Sample方法。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在skip-gram模型中，我们发现这是个十分庞大的神经网络，如果我们把词向量设置成300，那么在一个具有10000个单词的字典中，隐层和输出层分别具有3百万个参数。执行梯度下降算法时将会十分缓慢，更糟糕的是，我们需要事先在一个数量庞大的数据集上训练，以便于fine-tune参数和避免过拟合。因此对模型的训练将十分困难。</p>
<p>为了解决这个问题，作者提出了三个修改方案：</p>
<ol>
<li>把常见的短语或单词对当成一个单词来对待</li>
<li>降采样高频单词来见少训练样本数量</li>
<li>用一种他们称之为“负采样”的方式来修改优化目标，这将使得每个训练样本只更新一小部分的模型参数。</li>
</ol>
<p>它们均可以使得训练变的可行同时也改善了结果词向量的质量。</p>
<p><br></p>
<h1 id="单词对和短语"><a href="#单词对和短语" class="headerlink" title="单词对和短语"></a>单词对和短语</h1><p>作者指出一个单词对像“Boston Globe”(一个报纸名)，它的含义和“Boston”以及“Globe”都有着很大的区别，因此把“Boston Globe”当作一个单词来对待是很有意义的，它将会得到一个词向量来表示。在Google News数据集上，这个改变可以把模型字典尺寸从一千亿降低为三百万。Google开源了它们训练好得到的词向量，长度是300，共计三百万个。至于如何从单词中区分短语并且缩短字典尺寸，一个方法是可以参考维基百科的词条目录。</p>
<p><br></p>
<h1 id="降采样高频单词"><a href="#降采样高频单词" class="headerlink" title="降采样高频单词"></a>降采样高频单词</h1><p>在之前的例子中，针对一些通用单词例如“the”存在一些问题：</p>
<ol>
<li>当我们看一些单词对的时候，(“fox”，“the”)没有告诉我们过多的关于“fox”的含义的信息，“the”出现在几乎每个单词的前面。</li>
<li>我们拥有太多的关于(“the”，…)的采样，而我们学习一个好的关于“the”的向量不需要这么多的采样。</li>
</ol>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_S9yXV9_1.jpeg" alt=""></center>

<p>Word2Vec采用了一个叫做“降采样”的框架来解决这个问题。对于每个出现在我们训练文件中的单词，它们将会有一定的概率被删除相关的训练数据，概率的高低取决于它们出现的频率。</p>
<p>如果我们有一个尺寸为10的窗口，那么我们将移除一个特定的关于“the”的训练样本：</p>
<ol>
<li>当我们训练其他单词，“the”将不会在任何窗口中出现。</li>
<li>当我们训练“the”作为输入时，窗口尺寸会小于10。</li>
</ol>
<p><br></p>
<h1 id="采样频率"><a href="#采样频率" class="headerlink" title="采样频率"></a>采样频率</h1><p>关于如何计算字典中一个单词被保留下来的概率，有一套C语言的代码实现的公式。</p>
<p>Wi代表单词，z(wi)表示单词在语料库中出现的频率，例如“peanut”在一个拥有10亿单词语料库中出现了1000次，那么z(‘peanut’)=1E-6。</p>
<p>代码中还有一个参数称之为’sample’，它将控制采样发生的力度，默认是0.001，该数值越小则单词越不可能被保存。</p>
<p>P(wi)为保留单词的概率：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_BJPh3y_5.jpeg" alt=""></center>

<p>我们可以画出该公式的图：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_f7DguJ_6.jpeg" alt=""></center>

<p>如果我们使用默认采样值为0.001，那么有以下有趣的点：</p>
<ul>
<li>P=1，当z&lt;=0.0026时，这意味着当单词数量小于0.26%时将会被全部保留下来。</li>
<li>P=0.5，当z=0.00746时，这意味着有50%的概率被保留下来</li>
<li>P=0.033，当z=1时，这意味着如果语料库中全部是同一个单词时，只有3.3%的概率会被保存下来，这种情况当然很荒谬。</li>
</ul>
<p>论文中定义的公式和C语言代码中有所区别，我觉得C语言代码的实现更加权威。</p>
<p><br></p>
<h1 id="负采样-Negative-Sampling"><a href="#负采样-Negative-Sampling" class="headerlink" title="负采样(Negative Sampling)"></a>负采样(Negative Sampling)</h1><p>训练一个神经网络意味着每个样本都要更新所有参数，使用skip-gram模型将会每次都要更新数量庞大的模型，负采样采用的策略是更新每个样本时之更新很小比例的参数，而不是更新所有参数。</p>
<p>我们训练一个单词对时，例如(“fox”，“quick”)，当输入是“fox”时标签是“quick”的one-hot向量，那么也就是说输出标签神经元中只有一个是1，其他的成千上万个都是0。</p>
<p>因此当我们使用负采样时，我们将随机选择很小数量的“负”单词(例如选择5个)来更新权值，在我们的语料库中，一个“负”单词指的是我们希望让输出的标签的神经元为0，我们依然会更新“正”单词的神经元参数(也就是标签中为1的神经元)，论文中提到对于一个小规模的数据集，可以选择5-20个单词，如果是庞大的数据集，可以选择2-5个。</p>
<p>之前我们的输出层是300<em>10000的参数矩阵，但如今我们更新权值时只需要更新(“quick”)所代表的神经元的参数(输出为1)，以及再加上5个随机的输出为0的神经元，总共只需要更新6个神经元的参数，也就是300</em>6=1800个参数，这仅仅是原来输出层参数的0.06%！</p>
<p>注意在隐藏层中，不管是否使用负采样策略，都只有输入单词的权值被更新。</p>
<p><br></p>
<h1 id="选择负采样"><a href="#选择负采样" class="headerlink" title="选择负采样"></a>选择负采样</h1><p>选择一个单词作为负采样和它出现的频率有关，出现频率越高的单词越容易被选为负样本。在word2vec的C语言实现中，我们可以看到一个计算概率的公式：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_OWB2Gx_7.jpeg" alt=""></center>

<p>这种在C语言中实现的方式很有趣。他们有一个100M大小的元素数组（他们称之为unigram表），他们用词表中每个单词的索引多次填充这个表。然后，要实际选择一个负样本，只需生成0到100M之间的随机整数，并选择表中该索引处的单词。由于较高概率单词在表中出现次数较多，因此更有可能选择这些单词。</p>
<p><br></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这里有一些关于Word2Vec的学习资源：</p>
<p><a href="http://mccormickml.com/2016/04/27/word2vec-resources/" target="_blank" rel="external">Word2Vec Resources · Chris McCormick</a></p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="external">Word2Vec Tutorial Part 2 - Negative Sampling · Chris McCormick</a></p>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本文介绍了Word2Vec中对Skip Gram模型的改进——Negative Sample方法。
    
    </summary>
    
      <category term="自然语言处理" scheme="http://zangbo.me/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="Word2Vec" scheme="http://zangbo.me/tags/Word2Vec/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec笔记：Skip-Gram模型</title>
    <link href="http://zangbo.me/2017/06/05/Word2Vec%E4%B9%8Bskip-gram%E6%A8%A1%E5%9E%8B/"/>
    <id>http://zangbo.me/2017/06/05/Word2Vec之skip-gram模型/</id>
    <published>2017-06-05T08:23:12.000Z</published>
    <updated>2017-06-21T16:52:01.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文介绍了Word2Vec的基本概念，同时对Word2Vec的经典模型——The skip gram neural network model进行详细的学习和整理。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<p><br></p>
<h1 id="模型任务"><a href="#模型任务" class="headerlink" title="模型任务"></a>模型任务</h1><p>我们将要训练一个简单的神经网络来执行某个确定的任务，它只有一个隐层，但是我们却不去使用该训练好的网络，我们只是来学习隐层的权值，该权值就是我们的词向量(word vectors)。</p>
<p>具体任务如下：在一个句子中间部分选择一个单词作为输入单词，然后网络会告诉我们字典中每个单词在该单词附近的概率，这里的附近范围指的是该输入单词前面5个和后面5个(共10个)单词。比如我们输入单词是“苏联”，那么字典中“联邦”和“俄罗斯”的概率会高于“西瓜”和“袋鼠”</p>
<p>我们训练文件中存在很多成对的单词，我们不停的把一对单词送入神经网络进行训练，神经网络会统计单词出现的频率，比如（“苏联”，“联邦”）出现的次数更多，那么当我们输入“苏联”这个单词时，“联邦”的概率将会更大。如下是一个窗口为2的例子，前面2个后面2个，蓝色的为输入单词。</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_S9yXV9_1.jpeg" alt=""></center>

<p><br></p>
<h1 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h1><p>我们不可能直接把一个单词送入神经网络中训练，因此我们需要找到一个方式来代替单词。为了实现这点，我们先从训练文件中建立一个单词字典，假设我们有10000个独立单词。</p>
<p>我们讲吧每个输入单词例如“ants”作为一个one-hot向量，这个向量有10000维，我们将某一维设置为1，其他位置设置为0来表示单词“蚂蚁”。在训练网络时，输入是10000维向量(表示输入单词)，输出标签是与它相关单词对中的另外一个单词的one-hot向量，也是10000维。但是当我们使用该网络时，输出层使用softmax层，输出将变成一个10000维的概率向量(因为字典中有10000个单词)，其中的每一个维度都表示字典中的一个单词在该输入单词附近的概率。下图为使用时的模型：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_EBQYfm_2.jpeg" alt=""></center>

<h1 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h1><p>如果我们用300个单元来表示隐藏层，那么我们学到的词向量就是300维的，因此隐层的参数矩阵是[10000,300]的，300是Google在发行的数据库中使用的长度，这是个超参数，可以根据自己的需求更改。</p>
<p>因此我们的目标实际上是学习这样一个参数矩阵，输出层在我们训练完网络就丢弃不用。那么输入的one-hot向量什么作用呢，因为该向量大部分都是0，只有一个维度是1，因此可以起到选择的作用，如下简单的例子：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_HQ30yx_3.jpeg" alt=""></center>

<p>我们可以看出隐层的输出是输入one-hot向量的一个词向量表示。</p>
<p><br></p>
<h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><p>1*300的词向量”ants”将会被送到输出层，输出层是一个softmax回归分类器，分类器会输出字典中出现过的每一个单词在它附近的概率，这些概率在0-1之间，而且加起来的和为1。特别的，每个输出单元都有个权值向量和词向量相乘，然后用exp()函数得出结果，最后为了使得所有的向量概率加起来为1，还要除以10000个词向量的exp()结果的和。</p>
<p>以下是计算输出神经元输出“car”这个单词的图示：</p>
<center><img src="http://orwbuystz.bkt.clouddn.com/word2vec/20170621204146_XjsDH7_4.jpeg" alt=""></center>

<p>注意即使训练时某个单词在另一个单词附近的概率是100%，但使用时输出改单词的概率也不是100%，这和softmax函数有关。</p>
<p><br></p>
<h1 id="一些直觉"><a href="#一些直觉" class="headerlink" title="一些直觉"></a>一些直觉</h1><p>如果两个单词又非常相似的语义，我们的模型将会对这两个单词的输入来输出非常相似的结果。其中的一个方法是这两个单词的词向量是相似的。因此如果两个单词有相似的语义，那么他们有相似的词向量。</p>
<p>那么什么叫相似的语义呢？比如“intelligent”和“smart”这种有相似意思的单词，或者“engine”和“transmission”这种相关的单词，或者“ant”和“ants”这种具有相同意思的不同词性的单词。</p>
<p><br></p>
<h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>这种模型包含了数量庞大的参数，对于300特征的隐层和10000单词的字典，需要隐层和输出层各3M大小的权值数量，训练一个庞大的数据库是很困难的，因此我们做了一些改进来便于训练，这就是下篇文章提到的Negative Sample。</p>
<p><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="external">Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick</a></p>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本文介绍了Word2Vec的基本概念，同时对Word2Vec的经典模型——The skip gram neural network model进行详细的学习和整理。
    
    </summary>
    
      <category term="自然语言处理" scheme="http://zangbo.me/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="Word2Vec" scheme="http://zangbo.me/tags/Word2Vec/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中几种不同的梯度下降算法</title>
    <link href="http://zangbo.me/2017/06/04/%E5%85%B3%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://zangbo.me/2017/06/04/关于随机梯度下降/</id>
    <published>2017-06-04T07:32:01.000Z</published>
    <updated>2017-06-11T08:18:17.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><excerpt in="" index="" |="" 首页摘要=""><br>本文整理了关于梯度下降算法几个容易混淆的概念，它们都属于最速下降法。<a id="more"></a></excerpt></p>
<the rest="" of="" contents="" |="" 余下全文="">

<hr>
<p>之前关于Tensorflow的笔记有一个地方提到了随机梯度下降，官方文档提到说“这种随机抓取一小部分数据训练的方法称为随机梯度下降(SGD)”，但后来我又查了资料，这种方法准确来说应该称为Mini-batch梯度下降，因为标准的SGD是每次只随机取一个样本进行训练的，关于梯度下降算法几个容易混淆的概念我整理了下，它们都属于最速下降法：</p>
<p><br></p>
<ol>
<li><p><strong>批量梯度下降(Batch Gradient Descent，BGD)</strong></p>
<p>每次迭代的梯度方向计算由所有训练样本共同投票决定，即每次的参数更新需要把所有的训练样本都迭代一遍，可以采用线性搜索来确定最优步长，但缺点是参数更新速度太慢，而且如果训练样本数量特别庞大，对显存要求会很高。“Batch”的含义是训练集中所有样本参与每一轮迭代。</p>
</li>
<li><p><strong>随机梯度下降(Stochastic Gradient Descent，SGD)</strong></p>
<p>和BGD是两个极端，SGD每次迭代只选择一个训练样本，计算梯度然后进行参数的更新，直到收敛，这种方法也可用作online learning。这种方法的优点是参数更新速度很快，不需要占用较大显存。但缺点也很明显，因为不同训练样本的差距和噪声的影响，每次参数更新方向未必是正确的优化方向，但实践证明总体趋势一般是朝着最优点方向的。</p>
</li>
<li><p><strong>Mini-batch Gradient Descent</strong></p>
<p>这种方法在前两种方法里取一个折中，每次在训练集中取一部分数据进行迭代，然后更新参数。这是实践中使用最多的一种方法，具体选取的batch大小取决于数据集和显卡的质量，实验证明太大和太小都不好，应该选择一个适中的尺寸来达到训练效果最优。具体怎么选那就是一门玄学了，不过普通研究生平时科研的情况下选择自己显存能承受的最大值就可以了吧。</p>
</li>
</ol>
<p><br></p>
<p>平时也有些地方把SGD和Mini-batch GD统称为SGD，前者是后者的一个特例，TensorFlow的官方文档大概就是这样。这两者虽然也叫最速下降法，但步长一般不采用线搜索来获得，因为参数更新频率太快，线搜索计算比较复杂会消耗大量时间，所以平时都是凭经验指定步长(即学习率)。因为只要下降方向正确，步长在一个可以接受的范围内依然可以迭代到最优值，但学习率如果选的太大则无法下降，选的太小又使得训练过程漫长很难达到最优值，因此关于学习率的选择又是一门玄学。</p>
<p><br></p>
</the>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本文整理了关于梯度下降算法几个容易混淆的概念，它们都属于最速下降法。
    
    </summary>
    
      <category term="深度学习" scheme="http://zangbo.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="梯度下降算法" scheme="http://zangbo.me/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
